{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "llMVcMfQPVEj"
   },
   "source": [
    "# **Assignment Instructions:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZI55eu--dbD"
   },
   "source": [
    "**CNN on CIFR Assignment:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LX6XbdMS-P4W"
   },
   "source": [
    "1.  Please visit this link to access the state-of-art DenseNet code for reference - DenseNet - cifar10 notebook link\n",
    "2.  You need to create a copy of this and \"retrain\" this model to achieve 90+ test accuracy. \n",
    "3.  You cannot use DropOut layers.\n",
    "4.  You MUST use Image Augmentation Techniques.\n",
    "5.  You cannot use an already trained model as a beginning points, you have to initilize as your own\n",
    "6.  You cannot run the program for more than 300 Epochs, and it should be clear from your log, that you have only used 300 Epochs\n",
    "7.  You cannot use test images for training the model.\n",
    "8.  You cannot change the general architecture of DenseNet (which means you must use Dense Block, Transition and Output blocks as mentioned in the code)\n",
    "9.  You are free to change Convolution types (e.g. from 3x3 normal convolution to Depthwise Separable, etc)\n",
    "10. You cannot have more than 1 Million parameters in total\n",
    "11. You are free to move the code from Keras to Tensorflow, Pytorch, MXNET etc. \n",
    "12. You can use any optimization algorithm you need. \n",
    "13. You can checkpoint your model and retrain the model from that checkpoint so that no need of training the model from first if you lost at any epoch while training. You can directly load that model and Train from that epoch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N0qCY60zPh65"
   },
   "source": [
    "# **Importing Libraries & Callbacks:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "31zmu_9ZoXQS",
    "outputId": "1b54fb2a-c56a-4c8c-faa5-fb7afbb52d6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wVIx_KIigxPV"
   },
   "outputs": [],
   "source": [
    "# import keras\n",
    "# from keras.datasets import cifar10\n",
    "# from keras.models import Model, Sequential\n",
    "# from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge, Activation\n",
    "# from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "# from keras.layers import Concatenate\n",
    "# from keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.models import Model, load_model, save_model\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, Flatten, Conv2D\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rvTLi6LXy3UE"
   },
   "outputs": [],
   "source": [
    "# this part will prevent tensorflow to allocate all the avaliable GPU Memory\n",
    "# backend\n",
    "import tensorflow as tf\n",
    "from keras import backend as k\n",
    "\n",
    "# Don't pre-allocate memory; allocate as-needed\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "# Create a session with the above options specified.\n",
    "session = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cQ7FRVBhubpR"
   },
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/39779710/setting-up-a-learningratescheduler-in-keras\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, ModelCheckpoint, EarlyStopping,ReduceLROnPlateau\n",
    "def scheduler(epoch, lr):\n",
    "  if epoch % 10 ==0:\n",
    "    return lr*0.95\n",
    "  else:\n",
    "    return lr\n",
    "lr_scheduler = LearningRateScheduler(scheduler, verbose=1)\n",
    "\n",
    "\n",
    "#earlystop = EarlyStopping(monitor='accuracy', patience=50, verbose=1)\n",
    "decay_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.9, patience=10, \n",
    "                                                verbose=0, mode='auto', min_delta=0.001, \n",
    "                                                cooldown=0, min_lr=1e-6)\n",
    "\n",
    "\n",
    "#callback_list = [lr_scheduler, decay_lr, checkpoint, tensorboard_callback]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DzWPK9LCXJbT"
   },
   "outputs": [],
   "source": [
    "#Reference:https://machinelearningmastery.com/how-to-develop-a-cnn-from-scratch-for-cifar-10-photo-classification/\n",
    "# plot diagnostic learning curves\n",
    "import matplotlib.pyplot as plt\n",
    "def summarize_diagnostics(history):\n",
    "  #plot Loss\n",
    "  plt.figure(figsize=(10,10))\n",
    "  plt.subplot(211)\n",
    "  plt.title('Cross Entropy Loss')\n",
    "  plt.plot(history.history['loss'], color='blue', label='train')\n",
    "  plt.plot(history.history['val_loss'], color='orange', label='test')\n",
    "  plt.xlabel(\"Epochs\")\n",
    "\n",
    "  # plot accuracy\n",
    "  plt.subplot(212)\n",
    "  plt.title('Classification Accuracy')\n",
    "  plt.plot(history.history['accuracy'], color='blue', label='train')\n",
    "  plt.plot(history.history['val_accuracy'], color='orange', label='test')\n",
    "  plt.xlabel(\"Epochs\")\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rs9Bztpr1FPn"
   },
   "source": [
    "**Importing various optimizers for experiment:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H-eLp9xV1FZY"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "optimizer_SGD = tf.keras.optimizers.SGD(\n",
    "    learning_rate=0.01, momentum=0.7, nesterov=True, \n",
    "    name='SGD')\n",
    "\n",
    "optimizer_aadam = tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,\n",
    "    name='Adam')\n",
    "\n",
    "optimizer_adamax = tf.keras.optimizers.Adamax(\n",
    "    learning_rate=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-07,\n",
    "    name='Adamax')\n",
    "\n",
    "optimizer_RMSprop = tf.keras.optimizers.RMSprop(\n",
    "    learning_rate=0.001, rho=0.9, momentum=0.8, epsilon=1e-06, centered=False,\n",
    "    name='RMSprop')\n",
    "\n",
    "optimizer_adagrad = tf.keras.optimizers.Adagrad(\n",
    "    learning_rate=0.1, initial_accumulator_value=0.1, epsilon=1e-07,\n",
    "    name='Adagrad')\n",
    "\n",
    "optimizer_adadelta = tf.keras.optimizers.Adadelta(\n",
    "    learning_rate=0.1, rho=0.95, epsilon=1e-07, name='Adadelta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DIHqBKxem37J"
   },
   "source": [
    "**Importing Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mB7o3zu1g6eT",
    "outputId": "74969d12-759f-47dd-f0e0-f051e05303b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 2s 0us/step\n",
      "170508288/170498071 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "num_classes=10\n",
    "# Load CIFAR10 Data\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "img_height, img_width, channel = X_train.shape[1],X_train.shape[2],X_train.shape[3]\n",
    "\n",
    "# convert to one hot encoing \n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a1RnV1qvPorf"
   },
   "source": [
    "# **Given Model:** Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dsO_yGxcg5D8"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 10\n",
    "l = 40\n",
    "num_filter = 12\n",
    "compression = 0.5\n",
    "dropout_rate = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3lAk_Mw_5-rn",
    "outputId": "5ed4896a-652a-4842-bb64-9b6bffa0a025"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DVkpgHsc5-rp",
    "outputId": "16861d16-c9dd-49f5-c9b5-22d568d7dd4b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 32, 32, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ee-sge5Kg7vr"
   },
   "outputs": [],
   "source": [
    "# Dense Block\n",
    "def denseblock(input, num_filter = 12, dropout_rate = 0.2):\n",
    "    global compression\n",
    "    temp = input\n",
    "    for _ in range(l): \n",
    "        BatchNorm = layers.BatchNormalization()(temp)\n",
    "        relu = layers.Activation('relu')(BatchNorm)\n",
    "        Conv2D_3_3 = layers.Conv2D(int(num_filter*compression), (3,3), use_bias=False ,padding='same')(relu)\n",
    "        if dropout_rate>0:\n",
    "            Conv2D_3_3 = layers.Dropout(dropout_rate)(Conv2D_3_3)\n",
    "        concat = layers.Concatenate(axis=-1)([temp,Conv2D_3_3])\n",
    "        \n",
    "        temp = concat\n",
    "        \n",
    "    return temp\n",
    "\n",
    "## transition Blosck\n",
    "def transition(input, num_filter = 12, dropout_rate = 0.2):\n",
    "    global compression\n",
    "    BatchNorm = layers.BatchNormalization()(input)\n",
    "    relu = layers.Activation('relu')(BatchNorm)\n",
    "    Conv2D_BottleNeck = layers.Conv2D(int(num_filter*compression), (1,1), use_bias=False ,padding='same')(relu)\n",
    "    if dropout_rate>0:\n",
    "         Conv2D_BottleNeck = layers.Dropout(dropout_rate)(Conv2D_BottleNeck)\n",
    "    avg = layers.AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n",
    "    return avg\n",
    "\n",
    "#output layer\n",
    "def output_layer(input):\n",
    "    global compression\n",
    "    BatchNorm = layers.BatchNormalization()(input)\n",
    "    relu = layers.Activation('relu')(BatchNorm)\n",
    "    AvgPooling = layers.AveragePooling2D(pool_size=(2,2))(relu)\n",
    "    flat = layers.Flatten()(AvgPooling)\n",
    "    output = layers.Dense(num_classes, activation='softmax')(flat)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "anPCpQWhhGb7"
   },
   "outputs": [],
   "source": [
    "num_filter = 32\n",
    "dropout_rate = 0.2\n",
    "l = 12\n",
    "input = layers.Input(shape=(img_height, img_width, channel,))\n",
    "First_Conv2D = layers.Conv2D(num_filter, (3,3), use_bias=False ,padding='same')(input)\n",
    "\n",
    "First_Block = denseblock(First_Conv2D, num_filter, dropout_rate)\n",
    "First_Transition = transition(First_Block, num_filter, dropout_rate)\n",
    "\n",
    "Second_Block = denseblock(First_Transition, num_filter, dropout_rate)\n",
    "Second_Transition = transition(Second_Block, num_filter, dropout_rate)\n",
    "\n",
    "Third_Block = denseblock(Second_Transition, num_filter, dropout_rate)\n",
    "Third_Transition = transition(Third_Block, num_filter, dropout_rate)\n",
    "\n",
    "Last_Block = denseblock(Third_Transition,  num_filter, dropout_rate)\n",
    "output = output_layer(Last_Block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 322
    },
    "id": "c5Wksy8z5-rw",
    "outputId": "bb88b086-1451-429a-8f4a-da772a12f7b2"
   },
   "outputs": [
    {
     "data": {
      "image/jpeg": "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\n",
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"600\"\n",
       "            height=\"300\"\n",
       "            src=\"https://www.youtube.com/embed/-W6y8xnd--U\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x7f6454e3d4d0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://arxiv.org/pdf/1608.06993.pdf\n",
    "from IPython.display import IFrame, YouTubeVideo\n",
    "YouTubeVideo(id='-W6y8xnd--U', width=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1kFh7pdxhNtT",
    "outputId": "2e7fa8b2-3292-43df-fcd6-c58d1fb25d11",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 32, 32, 32)   864         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 32, 32, 32)  128         ['conv2d[0][0]']                 \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 32, 32, 32)   0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 32, 32, 16)   4608        ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 32, 32, 16)   0           ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 32, 32, 48)   0           ['conv2d[0][0]',                 \n",
      "                                                                  'dropout[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 32, 32, 48)  192         ['concatenate[0][0]']            \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 32, 32, 48)   0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 32, 32, 16)   6912        ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 32, 32, 16)   0           ['conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 32, 32, 64)   0           ['concatenate[0][0]',            \n",
      "                                                                  'dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 32, 32, 64)  256         ['concatenate_1[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 32, 32, 64)   0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 32, 32, 16)   9216        ['activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 32, 32, 16)   0           ['conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 32, 32, 80)   0           ['concatenate_1[0][0]',          \n",
      "                                                                  'dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 32, 32, 80)  320         ['concatenate_2[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 32, 32, 80)   0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 32, 32, 16)   11520       ['activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 32, 32, 16)   0           ['conv2d_4[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 32, 32, 96)   0           ['concatenate_2[0][0]',          \n",
      "                                                                  'dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 32, 32, 96)  384         ['concatenate_3[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 32, 32, 96)   0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 32, 32, 16)   13824       ['activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 32, 32, 16)   0           ['conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate)    (None, 32, 32, 112)  0           ['concatenate_3[0][0]',          \n",
      "                                                                  'dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 32, 32, 112)  448        ['concatenate_4[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 32, 32, 112)  0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 32, 32, 16)   16128       ['activation_5[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 32, 32, 16)   0           ['conv2d_6[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_5 (Concatenate)    (None, 32, 32, 128)  0           ['concatenate_4[0][0]',          \n",
      "                                                                  'dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 32, 32, 128)  512        ['concatenate_5[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 32, 32, 128)  0           ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 32, 32, 16)   18432       ['activation_6[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 32, 32, 16)   0           ['conv2d_7[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_6 (Concatenate)    (None, 32, 32, 144)  0           ['concatenate_5[0][0]',          \n",
      "                                                                  'dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 32, 32, 144)  576        ['concatenate_6[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 32, 32, 144)  0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 32, 32, 16)   20736       ['activation_7[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 32, 32, 16)   0           ['conv2d_8[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_7 (Concatenate)    (None, 32, 32, 160)  0           ['concatenate_6[0][0]',          \n",
      "                                                                  'dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 32, 32, 160)  640        ['concatenate_7[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (None, 32, 32, 160)  0           ['batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 32, 32, 16)   23040       ['activation_8[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 32, 32, 16)   0           ['conv2d_9[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_8 (Concatenate)    (None, 32, 32, 176)  0           ['concatenate_7[0][0]',          \n",
      "                                                                  'dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 32, 32, 176)  704        ['concatenate_8[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_9 (Activation)      (None, 32, 32, 176)  0           ['batch_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 32, 32, 16)   25344       ['activation_9[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 32, 32, 16)   0           ['conv2d_10[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_9 (Concatenate)    (None, 32, 32, 192)  0           ['concatenate_8[0][0]',          \n",
      "                                                                  'dropout_9[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 32, 32, 192)  768        ['concatenate_9[0][0]']          \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_10 (Activation)     (None, 32, 32, 192)  0           ['batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 32, 32, 16)   27648       ['activation_10[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 32, 32, 16)   0           ['conv2d_11[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_10 (Concatenate)   (None, 32, 32, 208)  0           ['concatenate_9[0][0]',          \n",
      "                                                                  'dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 32, 32, 208)  832        ['concatenate_10[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_11 (Activation)     (None, 32, 32, 208)  0           ['batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 32, 32, 16)   29952       ['activation_11[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, 32, 32, 16)   0           ['conv2d_12[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_11 (Concatenate)   (None, 32, 32, 224)  0           ['concatenate_10[0][0]',         \n",
      "                                                                  'dropout_11[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 32, 32, 224)  896        ['concatenate_11[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_12 (Activation)     (None, 32, 32, 224)  0           ['batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 32, 32, 16)   3584        ['activation_12[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)           (None, 32, 32, 16)   0           ['conv2d_13[0][0]']              \n",
      "                                                                                                  \n",
      " average_pooling2d (AveragePool  (None, 16, 16, 16)  0           ['dropout_12[0][0]']             \n",
      " ing2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 16, 16, 16)  64          ['average_pooling2d[0][0]']      \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_13 (Activation)     (None, 16, 16, 16)   0           ['batch_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 16, 16, 16)   2304        ['activation_13[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)           (None, 16, 16, 16)   0           ['conv2d_14[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_12 (Concatenate)   (None, 16, 16, 32)   0           ['average_pooling2d[0][0]',      \n",
      "                                                                  'dropout_13[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 16, 16, 32)  128         ['concatenate_12[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_14 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 16, 16, 16)   4608        ['activation_14[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)           (None, 16, 16, 16)   0           ['conv2d_15[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_13 (Concatenate)   (None, 16, 16, 48)   0           ['concatenate_12[0][0]',         \n",
      "                                                                  'dropout_14[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 16, 16, 48)  192         ['concatenate_13[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_15 (Activation)     (None, 16, 16, 48)   0           ['batch_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 16, 16, 16)   6912        ['activation_15[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)           (None, 16, 16, 16)   0           ['conv2d_16[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_14 (Concatenate)   (None, 16, 16, 64)   0           ['concatenate_13[0][0]',         \n",
      "                                                                  'dropout_15[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 16, 16, 64)  256         ['concatenate_14[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_16 (Activation)     (None, 16, 16, 64)   0           ['batch_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 16, 16, 16)   9216        ['activation_16[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_16 (Dropout)           (None, 16, 16, 16)   0           ['conv2d_17[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_15 (Concatenate)   (None, 16, 16, 80)   0           ['concatenate_14[0][0]',         \n",
      "                                                                  'dropout_16[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 16, 16, 80)  320         ['concatenate_15[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_17 (Activation)     (None, 16, 16, 80)   0           ['batch_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 16, 16, 16)   11520       ['activation_17[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_17 (Dropout)           (None, 16, 16, 16)   0           ['conv2d_18[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_16 (Concatenate)   (None, 16, 16, 96)   0           ['concatenate_15[0][0]',         \n",
      "                                                                  'dropout_17[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_18 (BatchN  (None, 16, 16, 96)  384         ['concatenate_16[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_18 (Activation)     (None, 16, 16, 96)   0           ['batch_normalization_18[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 16, 16, 16)   13824       ['activation_18[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_18 (Dropout)           (None, 16, 16, 16)   0           ['conv2d_19[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_17 (Concatenate)   (None, 16, 16, 112)  0           ['concatenate_16[0][0]',         \n",
      "                                                                  'dropout_18[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_19 (BatchN  (None, 16, 16, 112)  448        ['concatenate_17[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_19 (Activation)     (None, 16, 16, 112)  0           ['batch_normalization_19[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)             (None, 16, 16, 16)   16128       ['activation_19[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_19 (Dropout)           (None, 16, 16, 16)   0           ['conv2d_20[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_18 (Concatenate)   (None, 16, 16, 128)  0           ['concatenate_17[0][0]',         \n",
      "                                                                  'dropout_19[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_20 (BatchN  (None, 16, 16, 128)  512        ['concatenate_18[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_20 (Activation)     (None, 16, 16, 128)  0           ['batch_normalization_20[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)             (None, 16, 16, 16)   18432       ['activation_20[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_20 (Dropout)           (None, 16, 16, 16)   0           ['conv2d_21[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_19 (Concatenate)   (None, 16, 16, 144)  0           ['concatenate_18[0][0]',         \n",
      "                                                                  'dropout_20[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_21 (BatchN  (None, 16, 16, 144)  576        ['concatenate_19[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_21 (Activation)     (None, 16, 16, 144)  0           ['batch_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)             (None, 16, 16, 16)   20736       ['activation_21[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_21 (Dropout)           (None, 16, 16, 16)   0           ['conv2d_22[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_20 (Concatenate)   (None, 16, 16, 160)  0           ['concatenate_19[0][0]',         \n",
      "                                                                  'dropout_21[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_22 (BatchN  (None, 16, 16, 160)  640        ['concatenate_20[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_22 (Activation)     (None, 16, 16, 160)  0           ['batch_normalization_22[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)             (None, 16, 16, 16)   23040       ['activation_22[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_22 (Dropout)           (None, 16, 16, 16)   0           ['conv2d_23[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_21 (Concatenate)   (None, 16, 16, 176)  0           ['concatenate_20[0][0]',         \n",
      "                                                                  'dropout_22[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_23 (BatchN  (None, 16, 16, 176)  704        ['concatenate_21[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_23 (Activation)     (None, 16, 16, 176)  0           ['batch_normalization_23[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_24 (Conv2D)             (None, 16, 16, 16)   25344       ['activation_23[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_23 (Dropout)           (None, 16, 16, 16)   0           ['conv2d_24[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_22 (Concatenate)   (None, 16, 16, 192)  0           ['concatenate_21[0][0]',         \n",
      "                                                                  'dropout_23[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_24 (BatchN  (None, 16, 16, 192)  768        ['concatenate_22[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_24 (Activation)     (None, 16, 16, 192)  0           ['batch_normalization_24[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_25 (Conv2D)             (None, 16, 16, 16)   27648       ['activation_24[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_24 (Dropout)           (None, 16, 16, 16)   0           ['conv2d_25[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_23 (Concatenate)   (None, 16, 16, 208)  0           ['concatenate_22[0][0]',         \n",
      "                                                                  'dropout_24[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_25 (BatchN  (None, 16, 16, 208)  832        ['concatenate_23[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_25 (Activation)     (None, 16, 16, 208)  0           ['batch_normalization_25[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_26 (Conv2D)             (None, 16, 16, 16)   3328        ['activation_25[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_25 (Dropout)           (None, 16, 16, 16)   0           ['conv2d_26[0][0]']              \n",
      "                                                                                                  \n",
      " average_pooling2d_1 (AveragePo  (None, 8, 8, 16)    0           ['dropout_25[0][0]']             \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " batch_normalization_26 (BatchN  (None, 8, 8, 16)    64          ['average_pooling2d_1[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_26 (Activation)     (None, 8, 8, 16)     0           ['batch_normalization_26[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_27 (Conv2D)             (None, 8, 8, 16)     2304        ['activation_26[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_26 (Dropout)           (None, 8, 8, 16)     0           ['conv2d_27[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_24 (Concatenate)   (None, 8, 8, 32)     0           ['average_pooling2d_1[0][0]',    \n",
      "                                                                  'dropout_26[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_27 (BatchN  (None, 8, 8, 32)    128         ['concatenate_24[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_27 (Activation)     (None, 8, 8, 32)     0           ['batch_normalization_27[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_28 (Conv2D)             (None, 8, 8, 16)     4608        ['activation_27[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_27 (Dropout)           (None, 8, 8, 16)     0           ['conv2d_28[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_25 (Concatenate)   (None, 8, 8, 48)     0           ['concatenate_24[0][0]',         \n",
      "                                                                  'dropout_27[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_28 (BatchN  (None, 8, 8, 48)    192         ['concatenate_25[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_28 (Activation)     (None, 8, 8, 48)     0           ['batch_normalization_28[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_29 (Conv2D)             (None, 8, 8, 16)     6912        ['activation_28[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_28 (Dropout)           (None, 8, 8, 16)     0           ['conv2d_29[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_26 (Concatenate)   (None, 8, 8, 64)     0           ['concatenate_25[0][0]',         \n",
      "                                                                  'dropout_28[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_29 (BatchN  (None, 8, 8, 64)    256         ['concatenate_26[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_29 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_29[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_30 (Conv2D)             (None, 8, 8, 16)     9216        ['activation_29[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_29 (Dropout)           (None, 8, 8, 16)     0           ['conv2d_30[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_27 (Concatenate)   (None, 8, 8, 80)     0           ['concatenate_26[0][0]',         \n",
      "                                                                  'dropout_29[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_30 (BatchN  (None, 8, 8, 80)    320         ['concatenate_27[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_30 (Activation)     (None, 8, 8, 80)     0           ['batch_normalization_30[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_31 (Conv2D)             (None, 8, 8, 16)     11520       ['activation_30[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_30 (Dropout)           (None, 8, 8, 16)     0           ['conv2d_31[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_28 (Concatenate)   (None, 8, 8, 96)     0           ['concatenate_27[0][0]',         \n",
      "                                                                  'dropout_30[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_31 (BatchN  (None, 8, 8, 96)    384         ['concatenate_28[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_31 (Activation)     (None, 8, 8, 96)     0           ['batch_normalization_31[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_32 (Conv2D)             (None, 8, 8, 16)     13824       ['activation_31[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_31 (Dropout)           (None, 8, 8, 16)     0           ['conv2d_32[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_29 (Concatenate)   (None, 8, 8, 112)    0           ['concatenate_28[0][0]',         \n",
      "                                                                  'dropout_31[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_32 (BatchN  (None, 8, 8, 112)   448         ['concatenate_29[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_32 (Activation)     (None, 8, 8, 112)    0           ['batch_normalization_32[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_33 (Conv2D)             (None, 8, 8, 16)     16128       ['activation_32[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_32 (Dropout)           (None, 8, 8, 16)     0           ['conv2d_33[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_30 (Concatenate)   (None, 8, 8, 128)    0           ['concatenate_29[0][0]',         \n",
      "                                                                  'dropout_32[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_33 (BatchN  (None, 8, 8, 128)   512         ['concatenate_30[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_33 (Activation)     (None, 8, 8, 128)    0           ['batch_normalization_33[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_34 (Conv2D)             (None, 8, 8, 16)     18432       ['activation_33[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_33 (Dropout)           (None, 8, 8, 16)     0           ['conv2d_34[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_31 (Concatenate)   (None, 8, 8, 144)    0           ['concatenate_30[0][0]',         \n",
      "                                                                  'dropout_33[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_34 (BatchN  (None, 8, 8, 144)   576         ['concatenate_31[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_34 (Activation)     (None, 8, 8, 144)    0           ['batch_normalization_34[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_35 (Conv2D)             (None, 8, 8, 16)     20736       ['activation_34[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_34 (Dropout)           (None, 8, 8, 16)     0           ['conv2d_35[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_32 (Concatenate)   (None, 8, 8, 160)    0           ['concatenate_31[0][0]',         \n",
      "                                                                  'dropout_34[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_35 (BatchN  (None, 8, 8, 160)   640         ['concatenate_32[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_35 (Activation)     (None, 8, 8, 160)    0           ['batch_normalization_35[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_36 (Conv2D)             (None, 8, 8, 16)     23040       ['activation_35[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_35 (Dropout)           (None, 8, 8, 16)     0           ['conv2d_36[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_33 (Concatenate)   (None, 8, 8, 176)    0           ['concatenate_32[0][0]',         \n",
      "                                                                  'dropout_35[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_36 (BatchN  (None, 8, 8, 176)   704         ['concatenate_33[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_36 (Activation)     (None, 8, 8, 176)    0           ['batch_normalization_36[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_37 (Conv2D)             (None, 8, 8, 16)     25344       ['activation_36[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_36 (Dropout)           (None, 8, 8, 16)     0           ['conv2d_37[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_34 (Concatenate)   (None, 8, 8, 192)    0           ['concatenate_33[0][0]',         \n",
      "                                                                  'dropout_36[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_37 (BatchN  (None, 8, 8, 192)   768         ['concatenate_34[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_37 (Activation)     (None, 8, 8, 192)    0           ['batch_normalization_37[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_38 (Conv2D)             (None, 8, 8, 16)     27648       ['activation_37[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_37 (Dropout)           (None, 8, 8, 16)     0           ['conv2d_38[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_35 (Concatenate)   (None, 8, 8, 208)    0           ['concatenate_34[0][0]',         \n",
      "                                                                  'dropout_37[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_38 (BatchN  (None, 8, 8, 208)   832         ['concatenate_35[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_38 (Activation)     (None, 8, 8, 208)    0           ['batch_normalization_38[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_39 (Conv2D)             (None, 8, 8, 16)     3328        ['activation_38[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_38 (Dropout)           (None, 8, 8, 16)     0           ['conv2d_39[0][0]']              \n",
      "                                                                                                  \n",
      " average_pooling2d_2 (AveragePo  (None, 4, 4, 16)    0           ['dropout_38[0][0]']             \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " batch_normalization_39 (BatchN  (None, 4, 4, 16)    64          ['average_pooling2d_2[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_39 (Activation)     (None, 4, 4, 16)     0           ['batch_normalization_39[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_40 (Conv2D)             (None, 4, 4, 16)     2304        ['activation_39[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_39 (Dropout)           (None, 4, 4, 16)     0           ['conv2d_40[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_36 (Concatenate)   (None, 4, 4, 32)     0           ['average_pooling2d_2[0][0]',    \n",
      "                                                                  'dropout_39[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_40 (BatchN  (None, 4, 4, 32)    128         ['concatenate_36[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_40 (Activation)     (None, 4, 4, 32)     0           ['batch_normalization_40[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_41 (Conv2D)             (None, 4, 4, 16)     4608        ['activation_40[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_40 (Dropout)           (None, 4, 4, 16)     0           ['conv2d_41[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_37 (Concatenate)   (None, 4, 4, 48)     0           ['concatenate_36[0][0]',         \n",
      "                                                                  'dropout_40[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_41 (BatchN  (None, 4, 4, 48)    192         ['concatenate_37[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_41 (Activation)     (None, 4, 4, 48)     0           ['batch_normalization_41[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_42 (Conv2D)             (None, 4, 4, 16)     6912        ['activation_41[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_41 (Dropout)           (None, 4, 4, 16)     0           ['conv2d_42[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_38 (Concatenate)   (None, 4, 4, 64)     0           ['concatenate_37[0][0]',         \n",
      "                                                                  'dropout_41[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_42 (BatchN  (None, 4, 4, 64)    256         ['concatenate_38[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_42 (Activation)     (None, 4, 4, 64)     0           ['batch_normalization_42[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_43 (Conv2D)             (None, 4, 4, 16)     9216        ['activation_42[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_42 (Dropout)           (None, 4, 4, 16)     0           ['conv2d_43[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_39 (Concatenate)   (None, 4, 4, 80)     0           ['concatenate_38[0][0]',         \n",
      "                                                                  'dropout_42[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_43 (BatchN  (None, 4, 4, 80)    320         ['concatenate_39[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_43 (Activation)     (None, 4, 4, 80)     0           ['batch_normalization_43[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_44 (Conv2D)             (None, 4, 4, 16)     11520       ['activation_43[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_43 (Dropout)           (None, 4, 4, 16)     0           ['conv2d_44[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_40 (Concatenate)   (None, 4, 4, 96)     0           ['concatenate_39[0][0]',         \n",
      "                                                                  'dropout_43[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_44 (BatchN  (None, 4, 4, 96)    384         ['concatenate_40[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_44 (Activation)     (None, 4, 4, 96)     0           ['batch_normalization_44[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_45 (Conv2D)             (None, 4, 4, 16)     13824       ['activation_44[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_44 (Dropout)           (None, 4, 4, 16)     0           ['conv2d_45[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_41 (Concatenate)   (None, 4, 4, 112)    0           ['concatenate_40[0][0]',         \n",
      "                                                                  'dropout_44[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_45 (BatchN  (None, 4, 4, 112)   448         ['concatenate_41[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_45 (Activation)     (None, 4, 4, 112)    0           ['batch_normalization_45[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_46 (Conv2D)             (None, 4, 4, 16)     16128       ['activation_45[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_45 (Dropout)           (None, 4, 4, 16)     0           ['conv2d_46[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_42 (Concatenate)   (None, 4, 4, 128)    0           ['concatenate_41[0][0]',         \n",
      "                                                                  'dropout_45[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_46 (BatchN  (None, 4, 4, 128)   512         ['concatenate_42[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_46 (Activation)     (None, 4, 4, 128)    0           ['batch_normalization_46[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_47 (Conv2D)             (None, 4, 4, 16)     18432       ['activation_46[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_46 (Dropout)           (None, 4, 4, 16)     0           ['conv2d_47[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_43 (Concatenate)   (None, 4, 4, 144)    0           ['concatenate_42[0][0]',         \n",
      "                                                                  'dropout_46[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_47 (BatchN  (None, 4, 4, 144)   576         ['concatenate_43[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_47 (Activation)     (None, 4, 4, 144)    0           ['batch_normalization_47[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_48 (Conv2D)             (None, 4, 4, 16)     20736       ['activation_47[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_47 (Dropout)           (None, 4, 4, 16)     0           ['conv2d_48[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_44 (Concatenate)   (None, 4, 4, 160)    0           ['concatenate_43[0][0]',         \n",
      "                                                                  'dropout_47[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_48 (BatchN  (None, 4, 4, 160)   640         ['concatenate_44[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_48 (Activation)     (None, 4, 4, 160)    0           ['batch_normalization_48[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_49 (Conv2D)             (None, 4, 4, 16)     23040       ['activation_48[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_48 (Dropout)           (None, 4, 4, 16)     0           ['conv2d_49[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_45 (Concatenate)   (None, 4, 4, 176)    0           ['concatenate_44[0][0]',         \n",
      "                                                                  'dropout_48[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_49 (BatchN  (None, 4, 4, 176)   704         ['concatenate_45[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_49 (Activation)     (None, 4, 4, 176)    0           ['batch_normalization_49[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_50 (Conv2D)             (None, 4, 4, 16)     25344       ['activation_49[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_49 (Dropout)           (None, 4, 4, 16)     0           ['conv2d_50[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_46 (Concatenate)   (None, 4, 4, 192)    0           ['concatenate_45[0][0]',         \n",
      "                                                                  'dropout_49[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_50 (BatchN  (None, 4, 4, 192)   768         ['concatenate_46[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_50 (Activation)     (None, 4, 4, 192)    0           ['batch_normalization_50[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_51 (Conv2D)             (None, 4, 4, 16)     27648       ['activation_50[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_50 (Dropout)           (None, 4, 4, 16)     0           ['conv2d_51[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_47 (Concatenate)   (None, 4, 4, 208)    0           ['concatenate_46[0][0]',         \n",
      "                                                                  'dropout_50[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_51 (BatchN  (None, 4, 4, 208)   832         ['concatenate_47[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_51 (Activation)     (None, 4, 4, 208)    0           ['batch_normalization_51[0][0]'] \n",
      "                                                                                                  \n",
      " average_pooling2d_3 (AveragePo  (None, 2, 2, 208)   0           ['activation_51[0][0]']          \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 832)          0           ['average_pooling2d_3[0][0]']    \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 10)           8330        ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 790,058\n",
      "Trainable params: 777,994\n",
      "Non-trainable params: 12,064\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[input], outputs=[output])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Aqzk9AFXb1y",
    "outputId": "c9b86373-fd04-4334-94e0-d9c8746746d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262\n"
     ]
    }
   ],
   "source": [
    "print(len(model.layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b4XOsW3ahSkL"
   },
   "outputs": [],
   "source": [
    "# determine Loss function and Optimizer\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "crhGk7kEhXAz",
    "outputId": "1c69efc4-015b-4bfe-cb02-199ca61f7ce2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "391/391 [==============================] - 63s 123ms/step - loss: 1.5028 - accuracy: 0.4407 - val_loss: 1.3083 - val_accuracy: 0.5462\n",
      "Epoch 2/15\n",
      "391/391 [==============================] - 46s 118ms/step - loss: 1.0619 - accuracy: 0.6156 - val_loss: 1.0897 - val_accuracy: 0.6273\n",
      "Epoch 3/15\n",
      "391/391 [==============================] - 46s 118ms/step - loss: 0.8823 - accuracy: 0.6860 - val_loss: 1.2462 - val_accuracy: 0.6174\n",
      "Epoch 4/15\n",
      "391/391 [==============================] - 46s 118ms/step - loss: 0.7728 - accuracy: 0.7267 - val_loss: 1.2038 - val_accuracy: 0.6291\n",
      "Epoch 5/15\n",
      "391/391 [==============================] - 46s 118ms/step - loss: 0.6914 - accuracy: 0.7566 - val_loss: 1.0449 - val_accuracy: 0.6720\n",
      "Epoch 6/15\n",
      "391/391 [==============================] - 46s 118ms/step - loss: 0.6316 - accuracy: 0.7786 - val_loss: 1.2943 - val_accuracy: 0.6589\n",
      "Epoch 7/15\n",
      "391/391 [==============================] - 46s 118ms/step - loss: 0.5857 - accuracy: 0.7967 - val_loss: 0.9262 - val_accuracy: 0.7239\n",
      "Epoch 8/15\n",
      "391/391 [==============================] - 46s 118ms/step - loss: 0.5460 - accuracy: 0.8090 - val_loss: 0.7367 - val_accuracy: 0.7643\n",
      "Epoch 9/15\n",
      "391/391 [==============================] - 46s 118ms/step - loss: 0.5157 - accuracy: 0.8189 - val_loss: 0.9940 - val_accuracy: 0.7287\n",
      "Epoch 10/15\n",
      "391/391 [==============================] - 46s 118ms/step - loss: 0.4793 - accuracy: 0.8329 - val_loss: 0.7871 - val_accuracy: 0.7622\n",
      "Epoch 11/15\n",
      "391/391 [==============================] - 46s 118ms/step - loss: 0.4572 - accuracy: 0.8414 - val_loss: 0.7744 - val_accuracy: 0.7724\n",
      "Epoch 12/15\n",
      "391/391 [==============================] - 46s 118ms/step - loss: 0.4300 - accuracy: 0.8498 - val_loss: 0.7201 - val_accuracy: 0.7918\n",
      "Epoch 13/15\n",
      "391/391 [==============================] - 46s 117ms/step - loss: 0.4131 - accuracy: 0.8578 - val_loss: 0.9406 - val_accuracy: 0.7511\n",
      "Epoch 14/15\n",
      "391/391 [==============================] - 46s 117ms/step - loss: 0.3953 - accuracy: 0.8626 - val_loss: 0.7291 - val_accuracy: 0.7854\n",
      "Epoch 15/15\n",
      "391/391 [==============================] - 46s 118ms/step - loss: 0.3818 - accuracy: 0.8665 - val_loss: 0.9087 - val_accuracy: 0.7570\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=128,\n",
    "                    epochs=15,\n",
    "                    verbose=1, \n",
    "                    validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 694
    },
    "id": "ZcWydmIVhZGr",
    "outputId": "826dc89c-d991-4b90-b983-81ae845921ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 4s 11ms/step - loss: 0.9087 - accuracy: 0.7570\n",
      "Test loss: 0.9087499976158142\n",
      "Test accuracy: 0.7570000290870667\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Testing the model\n",
    "score = model.evaluate(X_test, y_test, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "print(\"--\"*25)\n",
    "\n",
    "\n",
    "summarize_diagnostics(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UE3lF6EH1r_L",
    "outputId": "d22fb91c-cf41-4c84-ab08-aabe2ecba28a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# Save the trained weights in to .h5 format\n",
    "model.save_weights(\"DNST_model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P8QtXfFdwdpa"
   },
   "source": [
    "_________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ngFDkXxhOg1d"
   },
   "source": [
    "# **Model 1:** Depthwise Seperable Conv2D + without Dropout + Without Dense Layer + Image Augmentation + SGD + Weight Regularizer(L1+L2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OOUGij-bOg1f"
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "if 'model' in locals():\n",
    "  del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "612xGRKhOg1f"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs =100\n",
    "l = 12\n",
    "num_filter = 36\n",
    "compression = 0.5\n",
    "dropout_rate = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_xE583AHOg1g"
   },
   "outputs": [],
   "source": [
    "# Dense Block\n",
    "def denseblock(input, num_filter = num_filter, dropout_rate = dropout_rate):\n",
    "    global compression\n",
    "    temp = input\n",
    "    for _ in range(l): \n",
    "        BatchNorm = layers.BatchNormalization()(temp)\n",
    "        relu = layers.Activation('relu')(BatchNorm)\n",
    "        Conv2D_3_3 = layers.SeparableConv2D(int(num_filter*compression), (3,3), use_bias=False ,padding='same', \n",
    "                                   kernel_initializer='he_normal',kernel_regularizer=regularizers.L1L2(l1=0.000001, l2=0.00001))(relu)\n",
    "        #Conv2D_3_3 = layers.Conv2D(int(num_filter*compression), (3,3), use_bias=False ,padding='same')(relu)\n",
    "        if dropout_rate>0:\n",
    "            Conv2D_3_3 = layers.Dropout(dropout_rate)(Conv2D_3_3)\n",
    "        concat = layers.Concatenate(axis=-1)([temp,Conv2D_3_3])\n",
    "        \n",
    "        temp = concat\n",
    "        \n",
    "    return temp\n",
    "\n",
    "## transition Blosck\n",
    "def transition(input, num_filter = num_filter, dropout_rate = dropout_rate):\n",
    "    global compression\n",
    "    BatchNorm = layers.BatchNormalization()(input)\n",
    "    relu = layers.Activation('relu')(BatchNorm)\n",
    "    Conv2D_BottleNeck = layers.SeparableConv2D(int(num_filter*compression), (1,1), use_bias=False ,padding='same',  \n",
    "                                               depthwise_initializer='he_normal', depthwise_regularizer=regularizers.L1L2(l1=0.0001, l2=0.0001),\n",
    "                                               kernel_regularizer=regularizers.L2(l2=0.001))(relu)\n",
    "    #Conv2D_BottleNeck =  Conv2D(int(num_filter*compression), (1,1), use_bias=False, kernel_regularizer = regularizers.l1() ,padding='same')(relu)\n",
    "    if dropout_rate>0:\n",
    "         Conv2D_BottleNeck = layers.Dropout(dropout_rate)(Conv2D_BottleNeck)\n",
    "    avg = layers.AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n",
    "    return avg\n",
    "\n",
    "#output layer\n",
    "def output_layer(input):\n",
    "    global compression\n",
    "    BatchNorm = layers.BatchNormalization()(input)\n",
    "    relu = layers.Activation('relu')(BatchNorm)\n",
    "    AvgPooling = layers.AveragePooling2D(pool_size=(2,2))(relu)\n",
    "    #flat = layers.Flatten()(AvgPooling)\n",
    "    #output = layers.Dense(num_classes, activation='softmax')(flat)\n",
    "    out_conv = layers.Conv2D(num_classes, kernel_size = (2,2), activation='softmax')(AvgPooling)\n",
    "    output = layers.Flatten()(out_conv)\n",
    "\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9VGV8MamOg1i"
   },
   "outputs": [],
   "source": [
    "#Model Architecture\n",
    "input = layers.Input(shape=(img_height, img_width, channel,))\n",
    "First_Conv2D = layers.Conv2D(num_filter, (3,3), use_bias=False ,padding='same')(input)\n",
    "#First_Conv2D = layers.SeparableConv2D(int(num_filter), (3,3), use_bias=True ,padding='same', \n",
    "                                      #kernel_regularizer=regularizers.L1L2(l1=0.0001, l2=0.0001))(input)\n",
    "\n",
    "First_Block = denseblock(First_Conv2D, num_filter, dropout_rate)\n",
    "First_Transition = transition(First_Block, num_filter, dropout_rate)\n",
    "\n",
    "Second_Block = denseblock(First_Transition, num_filter, dropout_rate)\n",
    "Second_Transition = transition(Second_Block, num_filter, dropout_rate)\n",
    "\n",
    "Third_Block = denseblock(Second_Transition, num_filter, dropout_rate)\n",
    "Third_Transition = transition(Third_Block, num_filter, dropout_rate)\n",
    "\n",
    "Last_Block = denseblock(Third_Transition,  num_filter, dropout_rate)\n",
    "output = output_layer(Last_Block)\n",
    "\n",
    "model = Model(inputs=[input], outputs=[output])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x0KbImBWOg1k"
   },
   "outputs": [],
   "source": [
    "# mormalize data\n",
    "#X_train = X_train.astype('float32') / 255\n",
    "#X_test = X_test.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dRirEinLOg1l"
   },
   "outputs": [],
   "source": [
    "###Image data Generator class\n",
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(featurewise_center=False,  \n",
    "                                                          samplewise_center=False, \n",
    "                                                          featurewise_std_normalization=False, \n",
    "                                                          samplewise_std_normalization=False,  \n",
    "                                                          zca_whitening=False,  \n",
    "                                                          rotation_range=15, \n",
    "                                                          width_shift_range=0.1, \n",
    "                                                          height_shift_range=0.1,  \n",
    "                                                          horizontal_flip=True,  \n",
    "                                                          vertical_flip=False , zoom_range=0.2, shear_range=15)\n",
    "##We are fitting the data to Image data generator.\n",
    "#ImageGenerator = ImageFlow.flow(X_train,seed=10,batch_size=32)\n",
    "datagen.fit(X_train, augment=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3KrXUpzHOg1m"
   },
   "outputs": [],
   "source": [
    "#Saving Best Model and Representation of results\n",
    "filepath = \"/content/drive/MyDrive/27 CNN on CIFR/Model Output/model_7_Rev01_depthwise.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath= filepath, save_weights_only=True,\n",
    "                              monitor='val_accuracy', verbose=1,\n",
    "                              save_best_only=True, mode='max') \n",
    "\n",
    "log_dir = \"logs/model_7_Rev01_depthwise\"\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir = log_dir, histogram_freq=1)\n",
    "#callback_list = [checkpoint, tensorboard_callback, decay_lr]\n",
    "\n",
    "#Model Compilation\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.SGD(0.01, momentum = 0.7),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xjcd3IOKOg1o",
    "outputId": "fbac0a19-94b9-42b1-afcc-91e57524b27e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 32, 32, 36)   972         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 32, 32, 36)  144         ['conv2d[0][0]']                 \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 32, 32, 36)   0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " separable_conv2d (SeparableCon  (None, 32, 32, 18)  972         ['activation[0][0]']             \n",
      " v2D)                                                                                             \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 32, 32, 54)   0           ['conv2d[0][0]',                 \n",
      "                                                                  'separable_conv2d[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 32, 32, 54)  216         ['concatenate[0][0]']            \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 32, 32, 54)   0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " separable_conv2d_1 (SeparableC  (None, 32, 32, 18)  1458        ['activation_1[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 32, 32, 72)   0           ['concatenate[0][0]',            \n",
      "                                                                  'separable_conv2d_1[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 32, 32, 72)  288         ['concatenate_1[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 32, 32, 72)   0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " separable_conv2d_2 (SeparableC  (None, 32, 32, 18)  1944        ['activation_2[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 32, 32, 90)   0           ['concatenate_1[0][0]',          \n",
      "                                                                  'separable_conv2d_2[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 32, 32, 90)  360         ['concatenate_2[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 32, 32, 90)   0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " separable_conv2d_3 (SeparableC  (None, 32, 32, 18)  2430        ['activation_3[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 32, 32, 108)  0           ['concatenate_2[0][0]',          \n",
      "                                                                  'separable_conv2d_3[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 32, 32, 108)  432        ['concatenate_3[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 32, 32, 108)  0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " separable_conv2d_4 (SeparableC  (None, 32, 32, 18)  2916        ['activation_4[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate)    (None, 32, 32, 126)  0           ['concatenate_3[0][0]',          \n",
      "                                                                  'separable_conv2d_4[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 32, 32, 126)  504        ['concatenate_4[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 32, 32, 126)  0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " separable_conv2d_5 (SeparableC  (None, 32, 32, 18)  3402        ['activation_5[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " concatenate_5 (Concatenate)    (None, 32, 32, 144)  0           ['concatenate_4[0][0]',          \n",
      "                                                                  'separable_conv2d_5[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 32, 32, 144)  576        ['concatenate_5[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 32, 32, 144)  0           ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " separable_conv2d_6 (SeparableC  (None, 32, 32, 18)  3888        ['activation_6[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " concatenate_6 (Concatenate)    (None, 32, 32, 162)  0           ['concatenate_5[0][0]',          \n",
      "                                                                  'separable_conv2d_6[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 32, 32, 162)  648        ['concatenate_6[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 32, 32, 162)  0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " separable_conv2d_7 (SeparableC  (None, 32, 32, 18)  4374        ['activation_7[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " concatenate_7 (Concatenate)    (None, 32, 32, 180)  0           ['concatenate_6[0][0]',          \n",
      "                                                                  'separable_conv2d_7[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 32, 32, 180)  720        ['concatenate_7[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (None, 32, 32, 180)  0           ['batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " separable_conv2d_8 (SeparableC  (None, 32, 32, 18)  4860        ['activation_8[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " concatenate_8 (Concatenate)    (None, 32, 32, 198)  0           ['concatenate_7[0][0]',          \n",
      "                                                                  'separable_conv2d_8[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 32, 32, 198)  792        ['concatenate_8[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_9 (Activation)      (None, 32, 32, 198)  0           ['batch_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " separable_conv2d_9 (SeparableC  (None, 32, 32, 18)  5346        ['activation_9[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " concatenate_9 (Concatenate)    (None, 32, 32, 216)  0           ['concatenate_8[0][0]',          \n",
      "                                                                  'separable_conv2d_9[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 32, 32, 216)  864        ['concatenate_9[0][0]']          \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_10 (Activation)     (None, 32, 32, 216)  0           ['batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_10 (Separable  (None, 32, 32, 18)  5832        ['activation_10[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_10 (Concatenate)   (None, 32, 32, 234)  0           ['concatenate_9[0][0]',          \n",
      "                                                                  'separable_conv2d_10[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 32, 32, 234)  936        ['concatenate_10[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_11 (Activation)     (None, 32, 32, 234)  0           ['batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_11 (Separable  (None, 32, 32, 18)  6318        ['activation_11[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_11 (Concatenate)   (None, 32, 32, 252)  0           ['concatenate_10[0][0]',         \n",
      "                                                                  'separable_conv2d_11[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 32, 32, 252)  1008       ['concatenate_11[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_12 (Activation)     (None, 32, 32, 252)  0           ['batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_12 (Separable  (None, 32, 32, 18)  4788        ['activation_12[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " average_pooling2d (AveragePool  (None, 16, 16, 18)  0           ['separable_conv2d_12[0][0]']    \n",
      " ing2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 16, 16, 18)  72          ['average_pooling2d[0][0]']      \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_13 (Activation)     (None, 16, 16, 18)   0           ['batch_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_13 (Separable  (None, 16, 16, 18)  486         ['activation_13[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_12 (Concatenate)   (None, 16, 16, 36)   0           ['average_pooling2d[0][0]',      \n",
      "                                                                  'separable_conv2d_13[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 16, 16, 36)  144         ['concatenate_12[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_14 (Activation)     (None, 16, 16, 36)   0           ['batch_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_14 (Separable  (None, 16, 16, 18)  972         ['activation_14[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_13 (Concatenate)   (None, 16, 16, 54)   0           ['concatenate_12[0][0]',         \n",
      "                                                                  'separable_conv2d_14[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 16, 16, 54)  216         ['concatenate_13[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_15 (Activation)     (None, 16, 16, 54)   0           ['batch_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_15 (Separable  (None, 16, 16, 18)  1458        ['activation_15[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_14 (Concatenate)   (None, 16, 16, 72)   0           ['concatenate_13[0][0]',         \n",
      "                                                                  'separable_conv2d_15[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 16, 16, 72)  288         ['concatenate_14[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_16 (Activation)     (None, 16, 16, 72)   0           ['batch_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_16 (Separable  (None, 16, 16, 18)  1944        ['activation_16[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_15 (Concatenate)   (None, 16, 16, 90)   0           ['concatenate_14[0][0]',         \n",
      "                                                                  'separable_conv2d_16[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 16, 16, 90)  360         ['concatenate_15[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_17 (Activation)     (None, 16, 16, 90)   0           ['batch_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_17 (Separable  (None, 16, 16, 18)  2430        ['activation_17[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_16 (Concatenate)   (None, 16, 16, 108)  0           ['concatenate_15[0][0]',         \n",
      "                                                                  'separable_conv2d_17[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_18 (BatchN  (None, 16, 16, 108)  432        ['concatenate_16[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_18 (Activation)     (None, 16, 16, 108)  0           ['batch_normalization_18[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_18 (Separable  (None, 16, 16, 18)  2916        ['activation_18[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_17 (Concatenate)   (None, 16, 16, 126)  0           ['concatenate_16[0][0]',         \n",
      "                                                                  'separable_conv2d_18[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_19 (BatchN  (None, 16, 16, 126)  504        ['concatenate_17[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_19 (Activation)     (None, 16, 16, 126)  0           ['batch_normalization_19[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_19 (Separable  (None, 16, 16, 18)  3402        ['activation_19[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_18 (Concatenate)   (None, 16, 16, 144)  0           ['concatenate_17[0][0]',         \n",
      "                                                                  'separable_conv2d_19[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_20 (BatchN  (None, 16, 16, 144)  576        ['concatenate_18[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_20 (Activation)     (None, 16, 16, 144)  0           ['batch_normalization_20[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_20 (Separable  (None, 16, 16, 18)  3888        ['activation_20[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_19 (Concatenate)   (None, 16, 16, 162)  0           ['concatenate_18[0][0]',         \n",
      "                                                                  'separable_conv2d_20[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_21 (BatchN  (None, 16, 16, 162)  648        ['concatenate_19[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_21 (Activation)     (None, 16, 16, 162)  0           ['batch_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_21 (Separable  (None, 16, 16, 18)  4374        ['activation_21[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_20 (Concatenate)   (None, 16, 16, 180)  0           ['concatenate_19[0][0]',         \n",
      "                                                                  'separable_conv2d_21[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_22 (BatchN  (None, 16, 16, 180)  720        ['concatenate_20[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_22 (Activation)     (None, 16, 16, 180)  0           ['batch_normalization_22[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_22 (Separable  (None, 16, 16, 18)  4860        ['activation_22[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_21 (Concatenate)   (None, 16, 16, 198)  0           ['concatenate_20[0][0]',         \n",
      "                                                                  'separable_conv2d_22[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_23 (BatchN  (None, 16, 16, 198)  792        ['concatenate_21[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_23 (Activation)     (None, 16, 16, 198)  0           ['batch_normalization_23[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_23 (Separable  (None, 16, 16, 18)  5346        ['activation_23[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_22 (Concatenate)   (None, 16, 16, 216)  0           ['concatenate_21[0][0]',         \n",
      "                                                                  'separable_conv2d_23[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_24 (BatchN  (None, 16, 16, 216)  864        ['concatenate_22[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_24 (Activation)     (None, 16, 16, 216)  0           ['batch_normalization_24[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_24 (Separable  (None, 16, 16, 18)  5832        ['activation_24[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_23 (Concatenate)   (None, 16, 16, 234)  0           ['concatenate_22[0][0]',         \n",
      "                                                                  'separable_conv2d_24[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_25 (BatchN  (None, 16, 16, 234)  936        ['concatenate_23[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_25 (Activation)     (None, 16, 16, 234)  0           ['batch_normalization_25[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_25 (Separable  (None, 16, 16, 18)  4446        ['activation_25[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " average_pooling2d_1 (AveragePo  (None, 8, 8, 18)    0           ['separable_conv2d_25[0][0]']    \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " batch_normalization_26 (BatchN  (None, 8, 8, 18)    72          ['average_pooling2d_1[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_26 (Activation)     (None, 8, 8, 18)     0           ['batch_normalization_26[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_26 (Separable  (None, 8, 8, 18)    486         ['activation_26[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_24 (Concatenate)   (None, 8, 8, 36)     0           ['average_pooling2d_1[0][0]',    \n",
      "                                                                  'separable_conv2d_26[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_27 (BatchN  (None, 8, 8, 36)    144         ['concatenate_24[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_27 (Activation)     (None, 8, 8, 36)     0           ['batch_normalization_27[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_27 (Separable  (None, 8, 8, 18)    972         ['activation_27[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_25 (Concatenate)   (None, 8, 8, 54)     0           ['concatenate_24[0][0]',         \n",
      "                                                                  'separable_conv2d_27[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_28 (BatchN  (None, 8, 8, 54)    216         ['concatenate_25[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_28 (Activation)     (None, 8, 8, 54)     0           ['batch_normalization_28[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_28 (Separable  (None, 8, 8, 18)    1458        ['activation_28[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_26 (Concatenate)   (None, 8, 8, 72)     0           ['concatenate_25[0][0]',         \n",
      "                                                                  'separable_conv2d_28[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_29 (BatchN  (None, 8, 8, 72)    288         ['concatenate_26[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_29 (Activation)     (None, 8, 8, 72)     0           ['batch_normalization_29[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_29 (Separable  (None, 8, 8, 18)    1944        ['activation_29[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_27 (Concatenate)   (None, 8, 8, 90)     0           ['concatenate_26[0][0]',         \n",
      "                                                                  'separable_conv2d_29[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_30 (BatchN  (None, 8, 8, 90)    360         ['concatenate_27[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_30 (Activation)     (None, 8, 8, 90)     0           ['batch_normalization_30[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_30 (Separable  (None, 8, 8, 18)    2430        ['activation_30[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_28 (Concatenate)   (None, 8, 8, 108)    0           ['concatenate_27[0][0]',         \n",
      "                                                                  'separable_conv2d_30[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_31 (BatchN  (None, 8, 8, 108)   432         ['concatenate_28[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_31 (Activation)     (None, 8, 8, 108)    0           ['batch_normalization_31[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_31 (Separable  (None, 8, 8, 18)    2916        ['activation_31[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_29 (Concatenate)   (None, 8, 8, 126)    0           ['concatenate_28[0][0]',         \n",
      "                                                                  'separable_conv2d_31[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_32 (BatchN  (None, 8, 8, 126)   504         ['concatenate_29[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_32 (Activation)     (None, 8, 8, 126)    0           ['batch_normalization_32[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_32 (Separable  (None, 8, 8, 18)    3402        ['activation_32[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_30 (Concatenate)   (None, 8, 8, 144)    0           ['concatenate_29[0][0]',         \n",
      "                                                                  'separable_conv2d_32[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_33 (BatchN  (None, 8, 8, 144)   576         ['concatenate_30[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_33 (Activation)     (None, 8, 8, 144)    0           ['batch_normalization_33[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_33 (Separable  (None, 8, 8, 18)    3888        ['activation_33[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_31 (Concatenate)   (None, 8, 8, 162)    0           ['concatenate_30[0][0]',         \n",
      "                                                                  'separable_conv2d_33[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_34 (BatchN  (None, 8, 8, 162)   648         ['concatenate_31[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_34 (Activation)     (None, 8, 8, 162)    0           ['batch_normalization_34[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_34 (Separable  (None, 8, 8, 18)    4374        ['activation_34[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_32 (Concatenate)   (None, 8, 8, 180)    0           ['concatenate_31[0][0]',         \n",
      "                                                                  'separable_conv2d_34[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_35 (BatchN  (None, 8, 8, 180)   720         ['concatenate_32[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_35 (Activation)     (None, 8, 8, 180)    0           ['batch_normalization_35[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_35 (Separable  (None, 8, 8, 18)    4860        ['activation_35[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_33 (Concatenate)   (None, 8, 8, 198)    0           ['concatenate_32[0][0]',         \n",
      "                                                                  'separable_conv2d_35[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_36 (BatchN  (None, 8, 8, 198)   792         ['concatenate_33[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_36 (Activation)     (None, 8, 8, 198)    0           ['batch_normalization_36[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_36 (Separable  (None, 8, 8, 18)    5346        ['activation_36[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_34 (Concatenate)   (None, 8, 8, 216)    0           ['concatenate_33[0][0]',         \n",
      "                                                                  'separable_conv2d_36[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_37 (BatchN  (None, 8, 8, 216)   864         ['concatenate_34[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_37 (Activation)     (None, 8, 8, 216)    0           ['batch_normalization_37[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_37 (Separable  (None, 8, 8, 18)    5832        ['activation_37[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_35 (Concatenate)   (None, 8, 8, 234)    0           ['concatenate_34[0][0]',         \n",
      "                                                                  'separable_conv2d_37[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_38 (BatchN  (None, 8, 8, 234)   936         ['concatenate_35[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_38 (Activation)     (None, 8, 8, 234)    0           ['batch_normalization_38[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_38 (Separable  (None, 8, 8, 18)    4446        ['activation_38[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " average_pooling2d_2 (AveragePo  (None, 4, 4, 18)    0           ['separable_conv2d_38[0][0]']    \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " batch_normalization_39 (BatchN  (None, 4, 4, 18)    72          ['average_pooling2d_2[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_39 (Activation)     (None, 4, 4, 18)     0           ['batch_normalization_39[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_39 (Separable  (None, 4, 4, 18)    486         ['activation_39[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_36 (Concatenate)   (None, 4, 4, 36)     0           ['average_pooling2d_2[0][0]',    \n",
      "                                                                  'separable_conv2d_39[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_40 (BatchN  (None, 4, 4, 36)    144         ['concatenate_36[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_40 (Activation)     (None, 4, 4, 36)     0           ['batch_normalization_40[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_40 (Separable  (None, 4, 4, 18)    972         ['activation_40[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_37 (Concatenate)   (None, 4, 4, 54)     0           ['concatenate_36[0][0]',         \n",
      "                                                                  'separable_conv2d_40[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_41 (BatchN  (None, 4, 4, 54)    216         ['concatenate_37[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_41 (Activation)     (None, 4, 4, 54)     0           ['batch_normalization_41[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_41 (Separable  (None, 4, 4, 18)    1458        ['activation_41[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_38 (Concatenate)   (None, 4, 4, 72)     0           ['concatenate_37[0][0]',         \n",
      "                                                                  'separable_conv2d_41[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_42 (BatchN  (None, 4, 4, 72)    288         ['concatenate_38[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_42 (Activation)     (None, 4, 4, 72)     0           ['batch_normalization_42[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_42 (Separable  (None, 4, 4, 18)    1944        ['activation_42[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_39 (Concatenate)   (None, 4, 4, 90)     0           ['concatenate_38[0][0]',         \n",
      "                                                                  'separable_conv2d_42[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_43 (BatchN  (None, 4, 4, 90)    360         ['concatenate_39[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_43 (Activation)     (None, 4, 4, 90)     0           ['batch_normalization_43[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_43 (Separable  (None, 4, 4, 18)    2430        ['activation_43[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_40 (Concatenate)   (None, 4, 4, 108)    0           ['concatenate_39[0][0]',         \n",
      "                                                                  'separable_conv2d_43[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_44 (BatchN  (None, 4, 4, 108)   432         ['concatenate_40[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_44 (Activation)     (None, 4, 4, 108)    0           ['batch_normalization_44[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_44 (Separable  (None, 4, 4, 18)    2916        ['activation_44[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_41 (Concatenate)   (None, 4, 4, 126)    0           ['concatenate_40[0][0]',         \n",
      "                                                                  'separable_conv2d_44[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_45 (BatchN  (None, 4, 4, 126)   504         ['concatenate_41[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_45 (Activation)     (None, 4, 4, 126)    0           ['batch_normalization_45[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_45 (Separable  (None, 4, 4, 18)    3402        ['activation_45[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_42 (Concatenate)   (None, 4, 4, 144)    0           ['concatenate_41[0][0]',         \n",
      "                                                                  'separable_conv2d_45[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_46 (BatchN  (None, 4, 4, 144)   576         ['concatenate_42[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_46 (Activation)     (None, 4, 4, 144)    0           ['batch_normalization_46[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_46 (Separable  (None, 4, 4, 18)    3888        ['activation_46[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_43 (Concatenate)   (None, 4, 4, 162)    0           ['concatenate_42[0][0]',         \n",
      "                                                                  'separable_conv2d_46[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_47 (BatchN  (None, 4, 4, 162)   648         ['concatenate_43[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_47 (Activation)     (None, 4, 4, 162)    0           ['batch_normalization_47[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_47 (Separable  (None, 4, 4, 18)    4374        ['activation_47[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_44 (Concatenate)   (None, 4, 4, 180)    0           ['concatenate_43[0][0]',         \n",
      "                                                                  'separable_conv2d_47[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_48 (BatchN  (None, 4, 4, 180)   720         ['concatenate_44[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_48 (Activation)     (None, 4, 4, 180)    0           ['batch_normalization_48[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_48 (Separable  (None, 4, 4, 18)    4860        ['activation_48[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_45 (Concatenate)   (None, 4, 4, 198)    0           ['concatenate_44[0][0]',         \n",
      "                                                                  'separable_conv2d_48[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_49 (BatchN  (None, 4, 4, 198)   792         ['concatenate_45[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_49 (Activation)     (None, 4, 4, 198)    0           ['batch_normalization_49[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_49 (Separable  (None, 4, 4, 18)    5346        ['activation_49[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_46 (Concatenate)   (None, 4, 4, 216)    0           ['concatenate_45[0][0]',         \n",
      "                                                                  'separable_conv2d_49[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_50 (BatchN  (None, 4, 4, 216)   864         ['concatenate_46[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_50 (Activation)     (None, 4, 4, 216)    0           ['batch_normalization_50[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_50 (Separable  (None, 4, 4, 18)    5832        ['activation_50[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_47 (Concatenate)   (None, 4, 4, 234)    0           ['concatenate_46[0][0]',         \n",
      "                                                                  'separable_conv2d_50[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_51 (BatchN  (None, 4, 4, 234)   936         ['concatenate_47[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_51 (Activation)     (None, 4, 4, 234)    0           ['batch_normalization_51[0][0]'] \n",
      "                                                                                                  \n",
      " average_pooling2d_3 (AveragePo  (None, 2, 2, 234)   0           ['activation_51[0][0]']          \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 1, 1, 10)     9370        ['average_pooling2d_3[0][0]']    \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 10)           0           ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 208,630\n",
      "Trainable params: 195,058\n",
      "Non-trainable params: 13,572\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pcY2etQoOg1p"
   },
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    lr = 0.01\n",
    "    if epoch > 180:\n",
    "        lr *= 0.00001\n",
    "    elif epoch > 180:\n",
    "        lr *= 0.0001\n",
    "    elif epoch > 120:\n",
    "        lr *= 0.001\n",
    "    print('\\nLearning rate: ', lr)\n",
    "    return lr\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WwG58PdlOg1q",
    "outputId": "745bd9b8-ce8c-4665-bb3e-253f9233b3d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Learning rate:  0.01\n",
      "Epoch 1/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 1.6673 - accuracy: 0.3875\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.12760, saving model to /content/drive/MyDrive/27 CNN on CIFR/Model Output/model_7_Rev01.h5\n",
      "390/390 [==============================] - 66s 155ms/step - loss: 1.6673 - accuracy: 0.3875 - val_loss: 2.3530 - val_accuracy: 0.1276 - lr: 0.0100\n",
      "\n",
      "Learning rate:  0.01\n",
      "Epoch 2/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 1.3880 - accuracy: 0.5001\n",
      "Epoch 00002: val_accuracy improved from 0.12760 to 0.46130, saving model to /content/drive/MyDrive/27 CNN on CIFR/Model Output/model_7_Rev01.h5\n",
      "390/390 [==============================] - 59s 152ms/step - loss: 1.3880 - accuracy: 0.5001 - val_loss: 1.5488 - val_accuracy: 0.4613 - lr: 0.0100\n",
      "\n",
      "Learning rate:  0.01\n",
      "Epoch 3/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 1.2415 - accuracy: 0.5559\n",
      "Epoch 00003: val_accuracy improved from 0.46130 to 0.49820, saving model to /content/drive/MyDrive/27 CNN on CIFR/Model Output/model_7_Rev01.h5\n",
      "390/390 [==============================] - 59s 151ms/step - loss: 1.2415 - accuracy: 0.5559 - val_loss: 1.5952 - val_accuracy: 0.4982 - lr: 0.0100\n",
      "\n",
      "Learning rate:  0.01\n",
      "Epoch 4/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 1.1398 - accuracy: 0.5923\n",
      "Epoch 00004: val_accuracy improved from 0.49820 to 0.53900, saving model to /content/drive/MyDrive/27 CNN on CIFR/Model Output/model_7_Rev01.h5\n",
      "390/390 [==============================] - 59s 152ms/step - loss: 1.1398 - accuracy: 0.5923 - val_loss: 1.3950 - val_accuracy: 0.5390 - lr: 0.0100\n",
      "\n",
      "Learning rate:  0.01\n",
      "Epoch 5/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 1.0685 - accuracy: 0.6192\n",
      "Epoch 00005: val_accuracy improved from 0.53900 to 0.59340, saving model to /content/drive/MyDrive/27 CNN on CIFR/Model Output/model_7_Rev01.h5\n",
      "390/390 [==============================] - 59s 151ms/step - loss: 1.0685 - accuracy: 0.6192 - val_loss: 1.1667 - val_accuracy: 0.5934 - lr: 0.0100\n",
      "\n",
      "Learning rate:  0.01\n",
      "Epoch 6/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 1.0012 - accuracy: 0.6434\n",
      "Epoch 00006: val_accuracy improved from 0.59340 to 0.63320, saving model to /content/drive/MyDrive/27 CNN on CIFR/Model Output/model_7_Rev01.h5\n",
      "390/390 [==============================] - 59s 151ms/step - loss: 1.0012 - accuracy: 0.6434 - val_loss: 1.0445 - val_accuracy: 0.6332 - lr: 0.0100\n",
      "\n",
      "Learning rate:  0.01\n",
      "Epoch 7/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.9542 - accuracy: 0.6631\n",
      "Epoch 00007: val_accuracy did not improve from 0.63320\n",
      "390/390 [==============================] - 58s 150ms/step - loss: 0.9542 - accuracy: 0.6631 - val_loss: 1.1392 - val_accuracy: 0.6128 - lr: 0.0100\n",
      "\n",
      "Learning rate:  0.01\n",
      "Epoch 8/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.9127 - accuracy: 0.6787\n",
      "Epoch 00008: val_accuracy improved from 0.63320 to 0.67580, saving model to /content/drive/MyDrive/27 CNN on CIFR/Model Output/model_7_Rev01.h5\n",
      "390/390 [==============================] - 59s 151ms/step - loss: 0.9127 - accuracy: 0.6787 - val_loss: 0.9296 - val_accuracy: 0.6758 - lr: 0.0100\n",
      "\n",
      "Learning rate:  0.01\n",
      "Epoch 9/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.8754 - accuracy: 0.6938\n",
      "Epoch 00009: val_accuracy did not improve from 0.67580\n",
      "390/390 [==============================] - 59s 150ms/step - loss: 0.8754 - accuracy: 0.6938 - val_loss: 1.2504 - val_accuracy: 0.6082 - lr: 0.0100\n",
      "\n",
      "Learning rate:  0.01\n",
      "Epoch 10/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.8376 - accuracy: 0.7044\n",
      "Epoch 00010: val_accuracy did not improve from 0.67580\n",
      "390/390 [==============================] - 58s 150ms/step - loss: 0.8376 - accuracy: 0.7044 - val_loss: 1.1419 - val_accuracy: 0.6225 - lr: 0.0100\n",
      "\n",
      "Learning rate:  0.01\n",
      "Epoch 11/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.8156 - accuracy: 0.7124\n",
      "Epoch 00011: val_accuracy did not improve from 0.67580\n",
      "390/390 [==============================] - 58s 150ms/step - loss: 0.8156 - accuracy: 0.7124 - val_loss: 1.0569 - val_accuracy: 0.6417 - lr: 0.0100\n",
      "\n",
      "Learning rate:  0.01\n",
      "Epoch 12/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.7899 - accuracy: 0.7233\n",
      "Epoch 00012: val_accuracy improved from 0.67580 to 0.71180, saving model to /content/drive/MyDrive/27 CNN on CIFR/Model Output/model_7_Rev01.h5\n",
      "390/390 [==============================] - 59s 151ms/step - loss: 0.7899 - accuracy: 0.7233 - val_loss: 0.8401 - val_accuracy: 0.7118 - lr: 0.0100\n",
      "\n",
      "Learning rate:  0.01\n",
      "Epoch 13/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.7743 - accuracy: 0.7289\n",
      "Epoch 00013: val_accuracy did not improve from 0.71180\n",
      "390/390 [==============================] - 58s 149ms/step - loss: 0.7743 - accuracy: 0.7289 - val_loss: 0.9355 - val_accuracy: 0.6992 - lr: 0.0100\n",
      "\n",
      "Learning rate:  0.01\n",
      "Epoch 14/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.7542 - accuracy: 0.7372\n",
      "Epoch 00014: val_accuracy did not improve from 0.71180\n",
      "390/390 [==============================] - 58s 150ms/step - loss: 0.7542 - accuracy: 0.7372 - val_loss: 1.1367 - val_accuracy: 0.6460 - lr: 0.0100\n",
      "\n",
      "Learning rate:  0.01\n",
      "Epoch 15/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.7266 - accuracy: 0.7467\n",
      "Epoch 00015: val_accuracy did not improve from 0.71180\n",
      "390/390 [==============================] - 58s 149ms/step - loss: 0.7266 - accuracy: 0.7467 - val_loss: 0.8953 - val_accuracy: 0.6999 - lr: 0.0100\n",
      "\n",
      "Learning rate:  0.01\n",
      "Epoch 16/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.7118 - accuracy: 0.7512\n",
      "Epoch 00016: val_accuracy improved from 0.71180 to 0.71250, saving model to /content/drive/MyDrive/27 CNN on CIFR/Model Output/model_7_Rev01.h5\n",
      "390/390 [==============================] - 59s 151ms/step - loss: 0.7118 - accuracy: 0.7512 - val_loss: 0.8671 - val_accuracy: 0.7125 - lr: 0.0100\n",
      "\n",
      "Learning rate:  0.01\n",
      "Epoch 17/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.6953 - accuracy: 0.7593\n",
      "Epoch 00017: val_accuracy improved from 0.71250 to 0.72360, saving model to /content/drive/MyDrive/27 CNN on CIFR/Model Output/model_7_Rev01.h5\n",
      "390/390 [==============================] - 59s 151ms/step - loss: 0.6953 - accuracy: 0.7593 - val_loss: 0.8412 - val_accuracy: 0.7236 - lr: 0.0100\n",
      "\n",
      "Learning rate:  0.01\n",
      "Epoch 18/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.6794 - accuracy: 0.7638\n",
      "Epoch 00018: val_accuracy improved from 0.72360 to 0.77050, saving model to /content/drive/MyDrive/27 CNN on CIFR/Model Output/model_7_Rev01.h5\n",
      "390/390 [==============================] - 59s 151ms/step - loss: 0.6794 - accuracy: 0.7638 - val_loss: 0.6945 - val_accuracy: 0.7705 - lr: 0.0100\n",
      "\n",
      "Learning rate:  0.01\n",
      "Epoch 19/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.6660 - accuracy: 0.7700\n",
      "Epoch 00019: val_accuracy did not improve from 0.77050\n",
      "390/390 [==============================] - 59s 150ms/step - loss: 0.6660 - accuracy: 0.7700 - val_loss: 0.7951 - val_accuracy: 0.7422 - lr: 0.0100\n",
      "\n",
      "Learning rate:  0.01\n",
      "Epoch 20/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.6565 - accuracy: 0.7739\n",
      "Epoch 00020: val_accuracy did not improve from 0.77050\n",
      "390/390 [==============================] - 59s 150ms/step - loss: 0.6565 - accuracy: 0.7739 - val_loss: 0.7460 - val_accuracy: 0.7463 - lr: 0.0100\n",
      "\n",
      "Learning rate:  0.01\n",
      "Epoch 21/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.6446 - accuracy: 0.7765\n",
      "Epoch 00021: val_accuracy did not improve from 0.77050\n",
      "390/390 [==============================] - 58s 149ms/step - loss: 0.6446 - accuracy: 0.7765 - val_loss: 0.9485 - val_accuracy: 0.7015 - lr: 0.0100\n",
      "\n",
      "Learning rate:  0.01\n",
      "Epoch 22/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.6281 - accuracy: 0.7829\n",
      "Epoch 00022: val_accuracy did not improve from 0.77050\n",
      "390/390 [==============================] - 58s 149ms/step - loss: 0.6281 - accuracy: 0.7829 - val_loss: 0.7182 - val_accuracy: 0.7654 - lr: 0.0100\n",
      "\n",
      "Learning rate:  0.01\n",
      "Epoch 23/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.6190 - accuracy: 0.7859\n",
      "Epoch 00023: val_accuracy did not improve from 0.77050\n",
      "390/390 [==============================] - 58s 149ms/step - loss: 0.6190 - accuracy: 0.7859 - val_loss: 0.8807 - val_accuracy: 0.7232 - lr: 0.0100\n",
      "\n",
      "Learning rate:  0.01\n",
      "Epoch 24/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.6121 - accuracy: 0.7881\n",
      "Epoch 00024: val_accuracy did not improve from 0.77050\n",
      "390/390 [==============================] - 58s 149ms/step - loss: 0.6121 - accuracy: 0.7881 - val_loss: 0.8589 - val_accuracy: 0.7256 - lr: 0.0100\n",
      "\n",
      "Learning rate:  0.01\n",
      "Epoch 25/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.5928 - accuracy: 0.7960\n",
      "Epoch 00025: val_accuracy did not improve from 0.77050\n",
      "390/390 [==============================] - 58s 149ms/step - loss: 0.5928 - accuracy: 0.7960 - val_loss: 0.8485 - val_accuracy: 0.7223 - lr: 0.0100\n",
      "\n",
      "Learning rate:  0.01\n",
      "Epoch 26/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.5952 - accuracy: 0.7952\n",
      "Epoch 00026: val_accuracy did not improve from 0.77050\n",
      "390/390 [==============================] - 59s 151ms/step - loss: 0.5952 - accuracy: 0.7952 - val_loss: 0.7680 - val_accuracy: 0.7538 - lr: 0.0100\n",
      "\n",
      "Learning rate:  0.01\n",
      "Epoch 27/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.5806 - accuracy: 0.7991\n",
      "Epoch 00027: val_accuracy improved from 0.77050 to 0.78570, saving model to /content/drive/MyDrive/27 CNN on CIFR/Model Output/model_7_Rev01.h5\n",
      "390/390 [==============================] - 59s 152ms/step - loss: 0.5806 - accuracy: 0.7991 - val_loss: 0.6540 - val_accuracy: 0.7857 - lr: 0.0100\n",
      "\n",
      "Learning rate:  0.01\n",
      "Epoch 28/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.5749 - accuracy: 0.8014\n",
      "Epoch 00028: val_accuracy did not improve from 0.78570\n",
      "390/390 [==============================] - 59s 150ms/step - loss: 0.5749 - accuracy: 0.8014 - val_loss: 0.7764 - val_accuracy: 0.7543 - lr: 0.0100\n",
      "\n",
      "Learning rate:  0.01\n",
      "Epoch 29/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.5693 - accuracy: 0.8043\n",
      "Epoch 00029: val_accuracy improved from 0.78570 to 0.80440, saving model to /content/drive/MyDrive/27 CNN on CIFR/Model Output/model_7_Rev01.h5\n",
      "390/390 [==============================] - 59s 152ms/step - loss: 0.5693 - accuracy: 0.8043 - val_loss: 0.5894 - val_accuracy: 0.8044 - lr: 0.0100\n",
      "\n",
      "Learning rate:  0.01\n",
      "Epoch 30/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.5555 - accuracy: 0.8074\n",
      "Epoch 00030: val_accuracy did not improve from 0.80440\n",
      "390/390 [==============================] - 59s 150ms/step - loss: 0.5555 - accuracy: 0.8074 - val_loss: 0.8337 - val_accuracy: 0.7377 - lr: 0.0100\n"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "\n",
    "decay_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.95, patience=5, \n",
    "                                                verbose=1, mode='auto', min_delta=0.001, \n",
    "                                                cooldown=0, min_lr=0.000001)\n",
    "\n",
    "callback_list = [checkpoint, lr_scheduler]\n",
    "\n",
    "history = model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),\\\n",
    "                    steps_per_epoch=X_train.shape[0] // batch_size, epochs=epochs,\\\n",
    "                    verbose=1,validation_data=(X_test,y_test),callbacks=callback_list)\n",
    "\n",
    "model.save_weights('/content/drive/MyDrive/27 CNN on CIFR/Model Output/model_7_30Epoch_Rev01_depthwise.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 694
    },
    "id": "iEtCd_iFOg1q",
    "outputId": "9e1277df-27f3-4927-ea0e-4c5881a37daf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 4s 12ms/step - loss: 0.5894 - accuracy: 0.8044\n",
      "Test loss: 0.5894258618354797\n",
      "Test accuracy: 0.8044000267982483\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlkAAAJcCAYAAADKNbH0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdd3iV5f3H8fc37JmwN4JsJMwwpW4QEFe1rVrRtiraOqu1amtFa636c89a6561tQ6cgBOUGYbsPWQJgbBH5v374z4xh5BxkpyV5PO6rnOd5DnPeZ77HG3z8R7f25xziIiIiEh4JcS6ASIiIiKVkUKWiIiISAQoZImIiIhEgEKWiIiISAQoZImIiIhEgEKWiIiISAQoZImIiIhEgEKWiBzBzC4ys1Qz229mW83sEzMbHsP2rDezQ4H25D2eDPG9X5nZ5ZFuYyjM7Fdm9k2s2yEi0VM91g0QkfhhZjcCtwJXAZOATGAUcDZwVEAws+rOuewoNO1M59xn4b5oFNsvIlWQerJEBAAzSwT+ClztnHvHOXfAOZflnPvAOXdz4Jw7zextM3vNzPYCvzKz1mY20czSzWy1mV0RdM1BgV6xvWa2zcweDhyvHbjGTjPbbWZzzKxFGdr8KzP7xsweNLNdZrbOzEYHXrsH+AnwZHDvl5k5M7vazFYBqwLHrgi0PT3wWVoH3cOZ2XVmttbMdpjZA2aWYGY1A+cnB53b3MwOmlmzUn6OYYHvYE/geViBz7jWzPYFPt8vA8c7m9nXgffsMLO3Svv9iUhkKWSJSJ6hQG3g3RLOOxt4G0gCXgf+DWwCWgPnA383s1MC5z4GPOacawh0Av4TOH4pkAi0A5rge84OlbHdg4EVQFPg/4Dnzcycc38GpgHXOOfqO+euCXrPOYH39Qy09V7g50ArYEPgMwU7F0gB+gc+/2+cc5mB8y4OOu9C4HPnXFqojTezxsBHwOP47+Jh4CMza2Jm9QLHRzvnGgDDgAWBt94NTAYaAW2BJ0K9p4hEh0KWiORpAuwIYfhshnPuPedcLj7YHA/c4pw77JxbADwHXBI4NwvobGZNnXP7nXMzg443ATo753Kcc3Odc3uLued7gR6vvMcVQa9tcM79yzmXA7yMD0ol9Yrd65xLd84dAn4JvOCcm+ecywBuA4aaWYeg8+8PnP898Cg+TBG434VmZoHfxwGvlnDvgs4AVjnnXnXOZTvn3gSWA2cGXs8FeplZHefcVufcksDxLOAYoHXgu9d8L5E4o5AlInl2Ak3NrKS5mhuDfm4NpDvn9gUd2wC0Cfx8GdAVWB4YBhsbOP4qfs7Xv81si5n9n5nVKOae5zjnkoIe/wp67Ye8H5xzBwM/1i/lZ9gQdI39+O+iTRHnbwi8B+fcLOAgcJKZdQc6AxNLuHdBR9w/6B5tnHMHgF/ge/q2mtlHgfsA/BEwYLaZLTGz35TyviISYQpZIpJnBpCBH0orjgv6eQvQ2MwaBB1rD2wGcM6tcs5dCDQH7gfeNrN6gbledznneuKHwMaS3/sVTi6E41vwPUIABIbomuR9hoB2QT+3D7wnz8v4IcNxwNvOucOlbOMR9w+6R953OMk5NwLfQ7cc+Ffg+A/OuSucc62BK4GnzaxzKe8tIhGkkCUiADjn9gB3AE+Z2TlmVtfMapjZaDP7vyLesxGYDtwbmMzeG9979RqAmV1sZs0CQ4u7A2/LNbOTzSzZzKoBe/FDX7kR+FjbgGNLOOdN4Ndm1tfMagF/B2Y559YHnXOzmTUys3bA9UDwJPPX8HO2LgZeKeFeFviefnwAHwNdzZfOqG5mvwB6Ah+aWQszOzsQ/DKA/QS+JzP7mZm1DVx3Fz44RuI7FJEyUsgSkR855x4CbgRuB9Lww2TXAO8V87YLgQ74Hpl3gQlB5RZGAUvMbD9+EvwFgXlQLfGT5/cCy4CvKX4u0wd2ZJ2skibn53kMOD+w8vDxwk4ItPUvwP+ArfgJ+hcUOO19YC5+0vlHwPNB798IzMOHnGkltGcYfoJ/8GMPvifvJvww5R+Bsc65Hfj/j74R/92mAycCvw1cayAwK/DdTgSud86tLeH+IhJF5lxRvekiImJmDujinFtdzDkvAFucc7dHr2UiEu9UjFREpBwCqxB/CvSLbUtEJN5ouFBEpIzM7G5gMfCAc25drNsjIvFFw4UiIiIiEaCeLBEREZEIiMs5WU2bNnUdOnSIdTNERERESjR37twdzrmj9iyNy5DVoUMHUlNTY90MERERkRKZWcFdGwANF4qIiIhEhEKWiIiISAQoZImIiIhEgEKWiIiISAQoZImIiIhEgEKWiIiISAQoZImIiIhEgEKWiIiISARUzZC1fSps/ybWrRAREZFKLC4rvkdc6nVQpxU0/yTWLREREZFKqmr2ZCX1gj2LY90KERERqcSqaMhKhoObIHNXrFsiIiIilVTVDFmJvfzz7iWxbYeIiIhUWlUzZCUl++c9i2LbDhEREam0qmbIqtsOajSE3ZqXJSIiIpFRNUOWmR8y3K2eLBEREYmMqhmyIH+FoXOxbomIiIhUQlU3ZCUm+9WFh7bEuiUiIiJSCVXdkJU3+V1DhiIiIhIBJYYsM2tnZl+a2VIzW2Jm1xdyzi/NbKGZLTKz6WbWJ+i19YHjC8wsNdwfoMySAmUcVJRUREREIiCUbXWygZucc/PMrAEw18ymOOeWBp2zDjjRObfLzEYDzwKDg14/2Tm3I3zNDoNaTfzWOurJEhERkQgoMWQ557YCWwM/7zOzZUAbYGnQOdOD3jITaBvmdkZGYi+VcRAREZGIKNWcLDPrAPQDZhVz2mVA8M7LDphsZnPNbHwx1x5vZqlmlpqWllaaZpVdUjLsXQq5OdG5n4iIiFQZIYcsM6sP/A+4wTm3t4hzTsaHrFuCDg93zvUHRgNXm9kJhb3XOfescy7FOZfSrFmzkD9AuST2gpzDsH9NdO4nIiIiVUZIIcvMauAD1uvOuXeKOKc38BxwtnNuZ95x59zmwPN24F1gUHkbHTZaYSgiIiIREsrqQgOeB5Y55x4u4pz2wDvAOOfcyqDj9QKT5TGzesBIIH4mQSX2BEwrDEVERCTsQlldeDwwDlhkZgsCx/4EtAdwzj0D3AE0AZ72mYxs51wK0AJ4N3CsOvCGc+7TsH6C8qheF+p3Uk+WiIiIhF0oqwu/AayEcy4HLi/k+Fqgz9HviCNJyerJEhERkbCruhXf8yT1gn2rIPtQrFsiIiIilYhCVlIyuFzYuyzWLREREZFKRCErMbC9joqSioiISBgpZDXoAgk1YY8mv4uIiEj4KGQlVIeGPdSTJSIiImGlkAV+XpbKOIiIiEgYKWSBD1mHNkPmrli3RERERCoJhSzQ5HcREREJO4UsyN/DUEVJRUREJEwUsgDqtoUaiZqXJSIiImGjkAVg5iu/K2SJiIhImChk5Uns5edkORfrloiIiEgloJCVJykZsnb7VYYiIiIi5aSQlUcrDEVERCSMFLLy5K0w1LwsERERCQOFrDy1GkOd1irjICIiImGhkBUsUSsMRUREJDwUsoIlJcOepZCbE+uWiIiISAWnkBUsqRfkZsD+1bFuiYiIiFRwClnBfpz8rnlZIiIiUj4KWcEa9gBM87JERESk3BSyglWvCw06wx6FLBERESkfhayC8rbXERERESmHEkOWmbUzsy/NbKmZLTGz6ws5x8zscTNbbWYLzax/0GuXmtmqwOPScH+AsEtK9hPfsw/FuiUiIiJSgYXSk5UN3OSc6wkMAa42s54FzhkNdAk8xgP/ADCzxsAEYDAwCJhgZo3C1PbISEoGlwt7l8W6JSIiIlKBlRiynHNbnXPzAj/vA5YBbQqcdjbwivNmAklm1go4HZjinEt3zu0CpgCjwvoJwu3HPQw1L0tERETKrlRzssysA9APmFXgpTbAxqDfNwWOFXW8sGuPN7NUM0tNS0srTbPCq0FnSKil7XVERESkXEIOWWZWH/gfcINzbm+4G+Kce9Y5l+KcS2nWrFm4Lx+6hOqQ2EM9WSIiIlIuIYUsM6uBD1ivO+feKeSUzUC7oN/bBo4VdTy+JSZrhaGIiIiUSyirCw14HljmnHu4iNMmApcEVhkOAfY457YCk4CRZtYoMOF9ZOBYfEvqBYc2Q+auWLdEREREKqjqIZxzPDAOWGRmCwLH/gS0B3DOPQN8DIwBVgMHgV8HXks3s7uBOYH3/dU5lx6+5kfIj9vrLILmJ8S2LSIiIlIhlRiynHPfAFbCOQ64uojXXgBeKFPrYiV4D0OFLBERESkDVXwvTJ02UCNRk99FRESkzBSyCmPme7NUxkFERETKSCGrKIm9fE+Wc7FuiYiIiFRACllFSUqGrD1+laGIiIhIKSlkFSVJ2+uIiIhI2SlkFeXHPQw1L0tERERKTyGrKLUaQ53W6skSERGRMlHIKk5SMuxRyBIREZHSU8gqTlIy7FkGudmxbomIiIhUMApZxUnsBbkZsG91rFsiIiIiFYxCVnHyttdRUVIREREpJYWs4jTsAZagye8iIiJSagpZxaleB+p3Vk+WiIiIlJpCVkmSeqknS0REREpNIaskicl+4nv2oVi3RERERCoQhaySJPUCHOxdGuuWiIiISAWikFWSvBWG2l5HRERESkEhqyT1O0NCLc3LEhERkVJRyCpJQjVI7KmQJSIiIqWikBWKpGSVcRAREZFSUcgKRWIvOLQFMtJj3RIRERGpIBSyQqHtdURERKSUSgxZZvaCmW03s0IThpndbGYLAo/FZpZjZo0Dr603s0WB11LD3fioSerlnzUvS0REREIUSk/WS8Cool50zj3gnOvrnOsL3AZ87ZwLHlc7OfB6SvmaGkN12kCNJJVxEBERkZCVGLKcc1OBUCcjXQi8Wa4WxSOzwOR39WSJiIhIaMI2J8vM6uJ7vP4XdNgBk81srpmNL+H9480s1cxS09LSwtWs8Enq5XuynIt1S0RERKQCCOfE9zOBbwsMFQ53zvUHRgNXm9kJRb3ZOfescy7FOZfSrFmzMDYrTJKSIWsPHNwU65aIiIhIBRDOkHUBBYYKnXObA8/bgXeBQWG8X3QlavK7iIiIhC4sIcvMEoETgfeDjtUzswZ5PwMjgYo7czxvhaHKOIiIiEgIqpd0gpm9CZwENDWzTcAEoAaAc+6ZwGnnApOdcweC3toCeNfM8u7zhnPu0/A1PcpqNvKrDNWTJSIiIiEoMWQ55y4M4ZyX8KUego+tBfqUtWFxSdvriIiISIhU8b00knrBnmWQmx3rloiIiEicU8gqjcRkyM2Afatj3RIRERGJcwpZpfHjHoaalyUiIiLFq3IhKzcXJkyAJ54ow5sbdgdL0PY6IiIiUqIqF7ISEiA1Fe66C/btK+Wbq9eBBl20wlBERERKVOVCFvierJ074amnyvDmxF5aYSgiIiIlqpIha9AgGDMGHnigDL1ZScl+4nv2wYi0TURERCqHKhmywPdmpafDk0+W8o2JvQAHe5ZGolkiIiJSSVTZkDVoEJxxBjz4YCl7s35cYaghQxERESlalQ1ZkN+bVaqVhvU7QbXamvwuIiIixarSIWvgQN+b9dBDsHdviG9KqAYNe6qMg4iIiBSrSocsgDvvLMPcrKRkFSQVERGRYlX5kJWSAmPH+rlZIfdmJfWCQ1shY2fZb+wcpM+F726HTR+U/ToiIiISl6p8yAI/N2vXrlLMzUoMTH4v7ZChc7AzFebfAhM7wacpsOQe+PYC2LuydNcSERGRuKaQhe/NOvPMUszNSurln0NZYegc7JwD8/8IE4+FSQNh+cPQsBsMfh7GLIJqtWD6xZCbVa7PISIiIvFDISugVL1ZdVpDzUZFrzB0DnbMgnl/gIkdYdIgWPEoNOwBg1+An26Dkz+BTr/xgW3QPyF9Diz+W1g/k4iIiMRO9Vg3IF4MGJDfm3XNNZCYWMzJZr4oaXDIcg52zoLv/wvfvw0Hv4eEGtByJCTfBW3P8sGsMO1/Bh3G+aHD1qOh6ZCwfjYRERGJPvVkBbnzzlL0ZiUl++HCtBkw90Z4/xiYPBRWPglJvWHIy/DT7XDSh3DspUUHrDwpT0DdtjB9HGTtD8fHERERkRhSyArSvz+cdRY8/DDs2VPCyUnJkLUXpgyDVU9Bo74w9JVAsPoAjr0EaiaFfvOaif79+9fA/JvK9TlEREQk9hSyCsibm/X44yWc2O6n0PlKGPqqD1YnToSO43xYKqvmJ0CPm2H1syrrICIiUsGZcy7WbThKSkqKS01Njdn9zzkHvv4a1q8vYW5WJORkwKTBcHirX3lYu3mUGyAiIiKlYWZznXMpBY+rJ6sQEybA7t0h9GZFQrVaMOx1yNwDsy73E+pFRESkwikxZJnZC2a23cwKLQplZieZ2R4zWxB43BH02igzW2Fmq83s1nA2PJL69YOzz/Zzs3bvjkEDko6DvvfB5g9gzXMxaICIiIiUVyg9WS8Bo0o4Z5pzrm/g8VcAM6sGPAWMBnoCF5pZz/I0Nppi2psF0O06aHEqzPs97Fsdo0aIiIhIWZUYspxzU4H0Mlx7ELDaObfWOZcJ/Bs4uwzXiYl+/fzcrEceiVFvliXA0JfAagSqwWfHoBEiIiJSVuGakzXUzL4zs0/M7LjAsTbAxqBzNgWOFcrMxptZqpmlpqWlhalZ5XPHHT5gPfZYjBpQty0M/Icvcrrk7zFqhIiIiJRFOELWPOAY51wf4AngvbJcxDn3rHMuxTmX0qxZszA0q/xi3psF0OECOOYiWPxX2DE7Ro0QERGR0ip3yHLO7XXO7Q/8/DFQw8yaApuBdkGntg0cq1AmTPCFSWPWmwUw8Cm/X+KMiyH7QAwbIiIiIqEqd8gys5ZmZoGfBwWuuROYA3Qxs45mVhO4AJhY3vtFW9++cO65Me7NqpkEQ1/2E+Dn/SFGjRAREZHSCKWEw5vADKCbmW0ys8vM7CozuypwyvnAYjP7DngcuMB52cA1wCRgGfAf59ySyHyMyMrrzXr00Rg2osXJ0P1GWP0MbP4ohg0RERGRUKjie4jOOw8+/9xXgU8qxZaEYZWTAZMGwuHtgWrw8TF3TUREpCpTxfdyuuOOOOjN+rEa/C6YPV7V4EVEROKYQlaI+vSBn/7Uz83atSuGDUlKhj5/h03vwdoXY9gQERERKY5CVilMmAB798a4Nwug++/9HK2518P+tTFujIiIiBRGIasUevf2c7MefTTGvVmWAENeBqsG08epGryIiEgcUsgqpTvu8L1ZjzwS44bUawcDn4Yd02Hp/TFujIiIiBSkkFVKeb1Zjz0G6WXZ0TGcOlwEx1wAi+6EnfG1GlNERKSqU8gqg7iZmwW+N6tOy0A1+EOxbo2IiIgEKGSVQXIynH++D1mbNsW4MTUbwZAXYe8KbSItIiISRxSyyujuu/3zqafCtm2xbQstT4MOF8Oy+33YEhERkZhTyCqj7t3h4499T9aIEXEwP6vfg1CtHsz5nYqUioiIxAGFrHIYPhzefx9WrIBRo/w8rZip0wL6/h22fQEb3oxhQ0RERAQUssrttNPg7bdh/nwYOxYOHIhhYzqNh8YDYd6NkLk7hg0RERERhawwOPNMeO01+PZbOPdcyMiIUUMSqsGgZyAjDb67PUaNEBEREVDICptf/AKefx6mTIGf/xyysmLUkMb9ocvVsOpp1c4SERGJIYWsMPrVr+DJJ2HiRBg3DnJyYtSQ3ndD7RYw5yrIjVUjREREqjaFrDC7+mq4/3546y244grIzY1BI2omQv9HIH0urH4mBg0QERERhawI+OMf/R6HL74I118fo4oKx/zC18/67k9w6IcYNEBERKRqU8iKkDvvhBtv9MOHt90Wg6BlBilPQc5hmHdTlG8uIiIiClkRYgYPPghXXeWHD++5JwaNaNgVet4KG96AHz6PQQNERESqLoWsCDKDp57yk+D/8hd45JEYNOK426B+J18JPidWtSVERESqHoWsCEtIgBde8BtK33gjPPtslBtQrbYfNty3EpY9EOWbi4iIVF0KWVFQvTq8/jqMGeOHD197LcoNaH06tP8ZLLkH9q+N8s1FRESqphJDlpm9YGbbzWxxEa//0swWmtkiM5tuZn2CXlsfOL7AzKp0ZcyaNf32Oyef7OtpvfNOlBvQ/xGw6jDnGm0gLSIiEgWh9GS9BIwq5vV1wInOuWTgbqDggNjJzrm+zrmUsjWx8qhTx28oPWgQXHABfPxxFG9et40vUrr1E9gY7YQnIiJS9ZQYspxzU4H0Yl6f7pzbFfh1JtA2TG2rlOrX9+EqORnOOw++/DKKN+96DTTqC3Ovh6x9UbxxBbfpA9j+TaxbISIiFUy452RdBnwS9LsDJpvZXDMbX9wbzWy8maWaWWpaWlqYmxVfkpJg0iQ49li/ufQ30fr7nVAdBv4DDm2BRXdG6aYV3KGt8M158NkJsPgecLEo4S8iIhVR2EKWmZ2MD1m3BB0e7pzrD4wGrjazE4p6v3PuWedcinMupVmzZuFqVtxq2hQ++wxat/bztB54IEpb8DQdAp2vgBWPwa6FUbhhBbfqH5CbDa3PgIW3w9RzIHNXye8TEZEqLywhy8x6A88BZzvnduYdd85tDjxvB94FBoXjfpVFq1YwcyacdZbfimfUKPghGjvg9LkXajb2G0irZ6ZoOYdh1TM+YJ04EQY8AVs/hU9TYNeCWLdORETiXLlDlpm1B94BxjnnVgYdr2dmDfJ+BkYCha5QrMoaN/arDp95BqZNg9694ZNPSn5fudRqDP0egB0zYM0LEb5ZBbb+TchIg+43+Mqy3a6BU7/2RV0nD4W1L8W6hSIiEsdCKeHwJjAD6GZmm8zsMjO7ysyuCpxyB9AEeLpAqYYWwDdm9h0wG/jIOfdpBD5DhWcGV14JqanQooWvp3XTTZARyQLtHS+B5ifAglvg8I4I3qiCcg5WPAqJvaDFKfnHmw2F0fOg6TCY+WuYNd73eImIiBRgLg5rJqWkpLjU1KpZVuvQIbj5Zr8dT//+8Oab0LVrhG62ewl80hc6joMh6tE6wrav4POTYfBz0Omyo1/PzYFFd8CSv0PjATD8bajfIdqtFBGROGBmcwsrVaWK73GmTh148kl47z1Yv94HrZdfjlD90KTjoMdNsPZFlSgoaMWjUKsJHHNR4a8nVIM+98AJ78O+1fBpf9gS6XFeERGpSBSy4tTZZ8N330FKiq8Qf/HFsHdvBG7U6y9Q7xiY81vIzYrADSqgfWtg00TofBVUr1P8uW3PglFzoW57+OoMWDjB93KJiEiVp5AVx9q2hc8/h7vvhrfegr59YdasMN+kej0Y8DjsWQzLHw3zxSuolU+CVYMuvwvt/AadYOR0P89t8V/h6zMgY2fJ7xMRkUpNISvOVasGt98OX38NOTkwfDjcd1+Ya2q1PQvanOULlB74PowXroCy9sKa56H9z6Fu69DfV70uDHkRBj0L276ET/rDzjmRa6eIiMQ9hawK4vjjYcECOPdcuO02GDkStm4N4w1SHvfP3/wMDlfuivvFWvMiZO+DbteX/r1mvtDriG/9z1OGw6p/akNuEZEqSiGrAmnUyA8b/utfMH26r6kVtk2m6x0Dw16H3Qt9Dai9q8J04QokNwdWPu7LMzQtR93cJil+nlaLU3zB15m/guyDYWumiIhUDApZFYwZXH45zJ3rt+Q54wy44YYw1dRqdw6c8gVk7YEpQyFtehguWoFs+Qj2ry1bL1ZBtZrASR9B8p2w7lUfXPetLv91RUSkwlDIqqB69PCT4K+9Fh57DAYPhi++CMOFmw2FkTOgRiP4/BT4/n9huGgp5WTA0vth6+To3nfFo1C3HbT7aXiuZwmQPAFO+hgOboJPB8CKJ7X6MFhOpvbQFJFKSyGrAqtdGx5/HCZOhB074NRT/WbT06aV88INOvug1bi/n6O1/JHozStKn+vDyIJb/b0PbIzOfXct9BPWu14DCdXDe+3Wo/zwYZPBMPdamDQIdoR7mWgFNety+KSP/+5FRCoZhaxK4MwzYfVqH7iWL4cTToARI2DGjHJctHZTOOVz36sz70aYe31ke2ByMn2NqUmDITPdr9LLzYbZV0Qn4K14DKrVhU6XR+b69TvAyZNg+H/g8A9++HD2lVW71MP3/4X1r4JVh7k3+H/eIiKViEJWJVG7th86XLsWHnrIFzIdNszvg1jmHYqq1/GhoPtNsPIJ+Oa8yEzg3r0IJg/xNaaOuRDGLPar9PreD1snwdoIb/lzeDusf93XuarVOHL3MYP2P4Oxy6H7jb5UxIfd/LMLZ02OCuDgFph9FTQZBENf8Qsu1jwX61aJiISVQlYlU6cO3HijD1v33efnbQ0c6CvIL1hQhgtaAvR/0Bcs3TTR7+d3eHt4GpubDUvu9cODhzbDT96BYa/mB52uv4PmJ/metEgOG676J+RmQLfrInePYDUa+O909Hxo2MMPmU0ZDrvK8g+oAnIOZv0Gcg7B0FfhmAug+Ymw8HbI3BXr1omIhI1CViVVvz7ccgusW+crxk+dCv36wfnnw5IlZbhgt2vhhHd9r9OkIbB3RfkauGc5TDkevvsTtDnb9161O/fIcywBhjwPLidyw4Y5mbDqaWg1ChJ7hP/6xUlKhtOmwpCXA/sfDvDDZlmR2D8pjqz6h++h7P8QNOzqe/gGPOYD1sI7Y906EZGwUciq5Bo29BXj162DO+6AyZMhORkuvBBWlDYntT0bTv0KsvfD5GFl21Q6NweWPQyf9vPBYtibfkiydrPCz69/bP6w4ZrnS3+/knwfmCMVjrINZWEGx14CZ67weyWueBw+6Abr36ycRUz3roD5f/ChtvNV+ccb9YFO42HVU7C7LP8VICISfxSyqoikJLjrLh+2br0VPvgAevaESy/1k+ZD1nQQnD7Th6IvToMN/wn9vfvWwOcnwfyboOUIOGMJdLjAB43idPlt0LBhGLf9cc6XbWjYHVqNDN91y6JmIxj4FJw+25eRmH4RfHEq7FkW23aFU24WTB8H1erAkBeO/ufe+26o3gDm/b5yBkwRqXIUsqqYJk3g73/3c7Z+/3v4z3+ge3df4HT9+hAvUv9YGDEdmgyEb38BSx8o/o+iy4WVT8PHvf1w4/DV4vIAACAASURBVJCX4IT3oU7L0O5nCf6PMrl+/lK4/gDvmO5LRnS73t8jHjRJ8eUzBv4D0uf78gYLboXsA7FuWfktvgfS5/iVo3VaHf167abQ+y74YQps/iD67RMRCbM4+csi0da8OTz4oA9b11wDr70GXbvC+PGwZk0IF6jVGE6ZAu1/AQv+CKnXFL4E/8D38MVISL0amg2HMxbDsZeW3HtVUP2O0Pf//B/gcK1CW/4o1EiCjuPCc71wSagGXa7yQ4gdfukLs37YEza+V3F7eHbMgiV/8ys4259X9HldfguJPX2vZU44tjEQEYkdhawqrlUrePRRP2R4xRXwyis+bF18cQgT5KvVhuPfgB5/9JPHp56b3+PinJ9D9VEv2DkTBj4DJ38KdduWvbFdroIWJ8O8m8o/bHhgA2x6BzqPh+r1ynetSKndHIa8CKdNg5qJMO1c+HpseIdMoyH7AEy/GOq08atUi5NQA/o/CvvX+KFcEZEKTCFLAGjbFp56ys/Z+v3v4b33oFcv+OlP/T6JRbIE6Hc/DHwatn4Mn50E6fN8GJh1OTQeAGMWQZcrS997Vdi9Bj9PWIYNVz4FGHS9unxtiobmw2HUPOj/MGyfClPP9vObKop5f/ChaegrPiyWpNUIaHMWLP4bHNoa+faJiESIQpYcoVUrP4y4YQP85S/w5ZeQkgKjRpWwXU+X3/p5VnuW+lIE2770y/JP/dwP9YVL/Y7Q74HAsOG/ynaNrP2w+l/Q7jyo1z58bYukhOrQ/fcw9GVfT2vZQ7FuUWg2fwyrn4EeN0GLE0N/X/+HIDcTFtwWubaJiESYQpYUqkkT+Otffdi6916YN89v13PCCTBpUhGdSG3Gwoipfmua0Qt8cc9ITCjvfCW0OCUwbLih9O9f9wpk7Y5d2YbyaPdTHw4X3Vn+WmWRdniHLzqalAy9/1a69zbo7EPlupdhx+zItE9EJMIUsqRYDRv6kg/r18Njj/nhxFGjfBX5d9+F3IK7wTQeAIP/5YtMRsqPw4aUftjQ5cLKx6HxQGg6NDLti7SUJ30ZhFlXxO92PM7B7PG+wOjQ16BardJf47g/Q+2WMPe6+P2cIiLFUMiSkNStC9dd51ce/utfsHu3n6/Vuze8/jpkR3tv3/odAsOGn8HqZ0N/39ZJvgeo+w3lnyMWK3Va+uG0tGml++zRtO4V2PSu78Fq1Lts16jRAPreBztn+b0lRUQqmJBClpm9YGbbzWxxEa+bmT1uZqvNbKGZ9Q967VIzWxV4XBquhkts1Kzpa2otX+7DFfiViN26+fCVEc1V952vhBan+grioQ4bLn/U12hqd35k2xZpx/468Nn/CAc3xbo1R9q/HlKvheYn+I2wy6PjON/ruOAWyNoXluaJiERLqD1ZLwGjinl9NNAl8BgP/APAzBoDE4DBwCBggpk1KmtjJX5Urw4XXQQLF/phw8aNfY2tTp3gkUdge5j2kC6Wmd/bEEIbNtyzFH6YDF2uhmo1I9++SDKDwc+Cy4bZv42f+lm5OTDjEv/zkJd9za/ysARIedyvMlxyb/nbJyISRSGFLOfcVCC9mFPOBl5x3kwgycxaAacDU5xz6c65XcAUig9rUsEkJMA558Ds2X5CfKdOcOONfpXiaafBs89CWloEG1DvGOj3YGjDhise97W9Oo+PYIOiqP6xfjhuy4ew4a1Yt8Zb/rAfxkx5wg/phkPTIdBhHCx/yG/NJCJSQYRrTlYbYGPQ75sCx4o6fhQzG29mqWaWmhbRv8oSCWYwciR8/bXv3frTn+D77+HKK33gGjkSnnsOdu6MwM07j4eWp/lhw/3rCz8nI93PE+pwcdGbUVdE3a73w2lzr4OMSHy5pbDrO1j4Z78CsuMl4b123/t8odL5fwjvdUVEIihuJr475551zqU451KaNatEfwSroORkuPtuWLECFiyAW27x2/dccQW0aOFXJ77wAqQX1zdaGmYwOLDVTlHDhmv+BTmHfFmJyiShmv/smbtg7u9j146cwzBjHNRsDAP/Gf5FBXVb+9WGm97zvZYiIhVAuELWZqBd0O9tA8eKOi5VgBn06QP33AOrVvlaWzff7H++7DIfuMaMgZdegl27ynmzvGHDbZ/D6n8e+VpuFqx80k8UT0ou543iUKPecNxtsP5V2PJpbNqw8C9+8+/BL/iNniOh+++hXkeYe0Ph+2SKiMSZcIWsicAlgVWGQ4A9zrmtwCRgpJk1Ckx4Hxk4JlWMGfTr5wubrl4Nqal+7tayZfDrX/vANXas3ztx9+4y3uTHYcObjxw23PiOX4HX/YZwfJT4dNyfoWEPmH1l9FfhbfvKV6DvfBW0GRO5+1Sr7UtX7FkCq56J3H1ERMLEXAirkszsTeAkoCmwDb9isAaAc+4ZMzPgSfyk9oPAr51zqYH3/gb4U+BS9zjnXizpfikpKS41NbXUH0YqHud84HrrLfjPf2DjRl8mYuRIX4drzBgfwEJ2YAN8lAxNBsIpU/zqtMnD4HAanLkiMhXo40XadJgyHLpe41fkRUPmHvi4NyTUhDELIr/ZtnPwxQjYNQ/OXAW1mkT2fiIiITCzuc65lKOOhxKyok0hq2pyDmbN8mHrv/+FTZt8D9igQXDGGb6nq2/fEKb7rH7W9+gMfBoa9YfJQ2DA49Dt2qh8jphKvdZvfj3iG2g2LPL3m34JbHgDRnwLTQdH/n4AuxfDJ319nbSBT0XnnlI+OZkw5yq/KKLN2Fi3RiTsFLKkQnHOT5r/8EP/mB3Yvq5Nm/zAdeqpvhJ9oW/+8nTYMR2aDIL0uXDOJl9BvLLL2gcf9fI9SqPnl207m1B9/1/45ufQ6w7ofVfk7lOY1Gth1dN+j8zKOM+usvnudlhyj+95HLtCPZBS6RQVsirx2IlUZHlzuP7yF9+7tXWrX5E4eDC88QacdZbfxPqMM+Af//DlIo548+DngATY9iUce1nVCFjgP+egf8LeZf6PWiS4XFjxBMy4FBqnQK/bI3Of4iTfBTWSYO718VOIVQqXNgOW3gstR0LmblhwW6xbJBI1CllSIbRs6SfI/+9/sGMHTJnia3AtXw6/+x0cc4xfyfinP8H06ZBTu73fSLlO66oxTBis9ShfD2zJvbBrYXivfWCDnxM19zpofhKc+IGvXxVttRpD77t9iN70bvTvL6HJPuB3AKjbDn7yX+h2A6x5DnbMjHXLRKJCw4VSoTnn63HlDSt+8w3k5PherjFj4IwxuZw2IoEmVW104vAO+KiHL3kwckb5t7dxDta+6Msn4KD/I9Dpsthusp2bDZ/0g+z9cMZSqF4ndm2Rws3+rS+pcuqX0OJEP5z9YQ+o3RxOnw0J1WPdQpGw0HChVEpm0L07/OEP8NVXfguff/8bRo+Gjz6CCy5MoFkzGDDAF0X97DM4dCjWrY6C2k1hwBOQPgdWPFa+ax3aCl+fBbMug8b9YcxC6Hx5bAMW+D/QAx6DA+v9dj4SX7Z8Aquf8ZuEtzjRH6vRAAY8Arvmw6p/xLZ9IlGgniyptHJy/IT5zz7zjxkzICsLatWC4cP93oojRvgVi9XK2dETl5zz4Wjb53DGYr/XYWlt+A/M+S3kHIQ+9/mh13grgzHtPF+EdexSX5RWYi9jp1+AUaspjJrja5zlcQ6+HAU7Z/pJ8HVaxq6dImGi1YVS5e3fD1On5oeuRYv88caN4ZRT8kPXsWXIInHr4Cb4sKdfZXnKlNB7nzJ2wpyr4fu3/HuHvAyJ3SPb1rLav84PQeVmQsPu0HiAn5DfJAUa9Y187S45knPw7S/8Fkinz/b/DArauwo+7gXtfwbDXot+G0XCTCFLpIAffoAvvvCT6D/7zNflAujY0Qeu007z4atphHaJiZpV/4A5v4PBz0On35R8/uYPYdYVkLkTku+EHn+M/7kz6fNg8wewMxXSU+HwD/64JfhK+HnBq/GAQPAqrPaHhMW612HGxdDn7367p6IsvAMW3w2nfgEtTo5e+8BvAZW114dylZOQMFDIEimGc7BypQ9bU6bAl1/C3r35pSTyermGD4fatUu+XlxxufDZSf4Py9ilUKdV4edl7fWbTK99AZJ6w9BXoFGfqDY1bA5u8fXR0lPznw9v869ZAiQed2TwSuqjifPhcGAjfJzsv9/Tpha/4CL7EHx0nB9KHL0AqtWMThs3vQ9TzwUCf/tqNYWG3XzgCn7U6xD//3EhcUMhS6QUsrP9dj95oStvPlft2vCTn/jANXIkJCdDQpxNUSrU3pV++5s2Y+Enbx/9+g9fwMxfw6FN0OMWSJ4Q2UKm0eYcHNrsA9fOoOCVkeZft2qQ2CvQ09XHh65GfaBmUmzbXZG4XPhipJ9rNfo7aNCp5Pds/gi+Hgt974Oet0S+jTtT4bMTIakX9JoA+1bA3uWwN/CcF8TBlyZp0OXo8NWwG9RoGPm2SoWikCVSDvv3w9df+8A1ZQosXeqPN2+e38s1YoSvSB+3ltwL3/0JfvI/v70JQPZBWHArrHwCGnSFoS9D0yGxbWe0OOfnrAX3dqXPyw9e4CfSJ/XxQ4x54at+x/ib/B8PVjzui8MO+qffrD1UU8+FrZNh7DKo1z5y7TuwASYN8T1nI2dCnUI2Rc3clR+4gsPXvtXgsvPPq9PKB676nXxPWK0mULOJfw7+uWYj9YZVEQpZImG0eXN+L9eUKbB9uz/es2d+4DrxRKhfP7btPEJuFkwaBId+8MOGe5bDzEth3yroeh30vVdzlZzz87l2LYBd38Hu7/zzvhW+pwagegNo1Du/tyupr+8Zqcrf3Z5l8Gl/aHGqL1BbmvIeBzb4hQutRsEJ70SmfZl7YMrxPlSPnA6JPUv3/tws2L/2yOC1d7lfdJG5079elBpJR4ev4J9rN/PfW63G5fuMElMKWSIRkpvrVyrmBa6pU+HwYahRA4YOzQ9dKSlxUCoifZ4PWonHwZ7FvhL3kBejP/G4osk+CHuW+MC1a0F++Mre51+3BD+0lBTo8eo4Duq2jW2boyU3CyYP9fXKxiwuW0mGJffBd7fBiR9BmzHhb99XZ/jdAU6eBC1PCe/1nfMFcTN2+sCVEfTILOLnjB35/+6A7/FKvgu6XBWbHRSk3BSyRKLk8GH49tv80DVvnj/eoIHv6ere3T969PDPxx7rA1nULLgVlt7vK7b3f1jzS8rKOR8sjuj1WuCP1W0PI6ZFdvgrXiycAIv/CsPfhvbnle0aOZnwSR9fhmPM4vAtQnAOZo/3W/kMfgE6/To81w2HnEzITPc9ZIsmwA+f+ZWw/R+B1qfHunWVw85U2DnbD19HeNhWIUskRtLS4PPPYdo0v9fi8uWwZUv+69WrQ+fO+aEr+NEwEvnH5fogUJbipFKy9Hnw+SlQqxmMmFr0as7KYMdsmDIMjrkIhr1Svmtt+9J/b73ugN53had9S+/3/1Fx3J+hz9/Cc81IcM6XIJl3E+xfDa3HQv+HoGHXWLes4nLODxHvXwtnrvK7DUSQQpZIHNm7Nz9wBT9WrfIrG/O0bn10z9dxx/kNs2O9q40UI206fDnSlwE47evKWYsp+6DfOzLnEIxZBDUTy3/Nb38JG9/2vVkNu5TvWhv+44uiHnMhDHu9YvwPJifDLyBYfDfkHoau10Kvv2iVa1msfwOm/zL0+oDlpJAlUgFkZcHatUcGr2XL/POePfnnNWsGffoc+ejRI8rDjlK8H76Ar8b4SfGnfB6eEBJP5lwDq57yny1c85wObYUPu0PToXDSJ2UPRmkz4POTfdX/Uz47clufiuDQNlh4O6x53q9e7PM3OPay8m/0XlVkH4QPu0Gt5n5bpyisBlbIEqnAnINt23zgWrQIvvvOPxYvhowMf06NGn7OV8HwVeEr1ldkmz+Caef6rYlOnlR5tvjZOhm+PB263eA3fA6nvFIQw/8L7c8v/fv3rYHJQ/yqvpEz/GbpFVX6fP9dpE3zq1kHPAotTop1q+Lfortg0Z2+IG7zn0TllgpZIpVQdravVJ8XuvIeW7fmn9O69dHBq2vXOFjpWFV8/7Yftmpxii9vEK1elewDsPwRH+za/wLqtg7PdTPSfVX3Gokwam74K+XnZsOkgXA4zdfOKs1cmox0P0fscJqvhVXeIcd44JwfQp1/sy930e486PeAr9cmRzuw0fditTkThr8VtdsqZIlUIWlpRwevpUvz53vVrOkLp7ZtC+3aFf7crFkFqWZfEax9xdckaz3W14KK9DL99Pkw/UJf0wkA82U6OlzkC9HWbFT2a397EXz/Xzh9FjTuH5bmHiVthg9LPf7gA0UocjJ879qOGX6IMEo9GFGTfQiWP+SLCrsc6H6j3xsywhO6K5zpF/v/sBm7HOp3iNptFbJEqrjMTD/cmBe4Nm70m2LnPWcVqKeYF8SKCmHt2vkgVhHmE8eFvI262/8chr0Rmfk1LheWPwrf3ernowx7Feq0hvVvwoY3fOHZhJrQerRfEdhmbOmKqG54C769AHrfDb1uD3/7g826Ata+6Pc1TOpV/LnOwYxLYf2rfpJ7h4si27ZYOrjZr5hc/5pfudrnXl+XLV52IcjaB4e2+O2Hom3HTF+zLQarSRWyRKRIubm+9ys4dBV8LiyI1avny0907gxduhz5aNFCAewoyx70wz7H/sqvegrnH8ZDP/ig8cNkaHsODH7uyFWNzvntgza86R+HtkL1+tD2XB9KWp5afA/bwc1+mLBBN18DLNLbxRze4Yd9Eo/zKzSL+5cpbw5ONMJfvNgxE+beADtnQeOBMOAxaDY0tm3av9Yv9ti3Gk6bFt32uFwfsA5uhLEroUZ0t9soV8gys1HAY0A14Dnn3H0FXn8EyCsZXRdo7pxLCryWAywKvPa9c+6sku6nkCUSf/KCWHD4WrvWl51Ytcr/HFx+on79wsNX585+z8cqG8AW3gmL74Ku18CAx8PzRWz+yG/wnb3fT47udEXx183NgbSpfpn7929D1m5f16v9z3zgajr0yADoHHw1GrZP8z1L0ZrrtPo5mH0FDHkZjr2k8HPWvQozLoGOl/rdC6rSv1gu1/8zXHCL3w6q7//5YcRYfAc75/jNvnOz/NZTOP/vSrS2C1r3GswYB0NegmMvjc49g5Q5ZJlZNWAlMALYBMwBLnTOLS3i/GuBfs653wR+3++cK1WkVMgSqXiys2HDBli9Oj94rVrlf1+37sgA1rBhfgDr1g369YP+/f0QZKX/G+mc781a/hD0vMUP95T1Q+cchvl/9Bt8J/WB49+ExB6lvEYGbP3U/7HePNFfs94xvr5Uh4sgKRlWPg2pV8PAp6HLb8vW1rJwuTD5eDiw1s+xKTiXbNvX8OUIaDYcTvoUqtWMXtviSdY+H7I3/g86jIPBz0a3bMWmiX4YuXZLOOljH/anDAvsR/l+5P9HnX0APujmh8ZPnxmTodPyhKyhwJ3OudMDv98G4Jy7t4jzpwMTnHNTAr8rZIlUcVlZPoDlha7gELZune8lA19uon9/GDAg/7lDh0oYvJzz87NWP1P2Ia7dS/zk9t2LfCmFvveW/w9r1j7Y9J6fw/XDZD/BOrEX7F8DzU/0f0Cj/Q8jfT5MSoHOV8HAp/KP713hh4dqt/CbPpdnMn9l4HJh8T2w6A4/fHjCu1C3TeTvu/IpmHsdNBrgV8/WaeGPr3jCH+/3EPS4MbJtWHiHL+A64ltoNiyy9ypCeULW+cAo59zlgd/HAYOdc9cUcu4xwEygrXMuJ3AsG1gAZAP3OefeK+I+44HxAO3btx+wYcOGUnw8EamoDh2ChQth7ly/z+Pcub7+V17PV6NGPnDlha4BA/x+jxV+5aPLhRm/8pO1+z8M3X8f4vucn0Q//ya/7+SQl/xE9nA7nOZXEW54Aw5ughHTw1cGorRSr/e9dafP9gVGD6f5WlhZ+3zPhbaIyrfpfb/Crnp9H7SaDonMfVyuH6Zc9qAvl3D8m0fWgXMOvjnf93KNmBa5dhz43s/da3uOb0OMRCtk3YIPWNcGHWvjnNtsZscCXwCnOufWFHdP9WSJVG0ZGb7oal7omjfPB7HMTP96w4b5wSsvfHXqVAEr3udm+2GWjf+DQf/0G9kW5/AOmHWZH9ZrNcoHrLyeg8osc4+vBF+3HZz2JXwxAnbNh1O/jNwf74ps92KYerYPxwOfCf/G2DmH/Ty47/8LXa72k+4LWy2budtvveRyYfT8yMzP+vZCHyzHLo/phuxFhaxQlodsBtoF/d42cKwwFwBXBx9wzm0OPK81s6+AfkCxIUtEqrZatSAlxT/yZGb60hPBPV5PPw2HD/vXzfyE+jZtin8kJsbR8GNCdV/OYeo5MPsqqFYPOv6y8HN/+NxP7M3YCf0fhW7Xxs+y/Uirmeg3TJ7+S/9He98qXxFeAatwSb3g9Dm+CO6s38Du76Dfg+FZEZqx0we4tG/9NYubaF8zCYb/x2/UPPPXcMJ74f0fX9q3sOHfflPxGAas4oTSk1UdP/H9VHy4mgNc5JxbUuC87sCnQEcXuKiZNQIOOucyzKwpMAM4u6hJ83nUkyUiocjO9rW/5s3zc7s2bz7ysXPn0e+pV89Xwc8LXW3bHhnC2rf35SeiOhyZfcgvfU+b5sNDu3PzX8vJhIV/gWUPQMPufkikUZ8oNi5OOAdfnArbvvSr6HreHOsWxb/cbL/IYsWj0OJUXwG9PJuV71vj/z09sMHXYGv/s9Det/wxmHdD6YbFS+JyYdJgX4rkzBUx37KqvCUcxgCP4ks4vOCcu8fM/gqkOucmBs65E6jtnLs16H3DgH8CuUAC8Khz7vmS7qeQJSLhcPgwbNlydPgKfmzZkj8MmadmTb/SsX37/Mcxx+T/3K4d1C1FDc+QZO2DL0bCrrlwwkRoPQr2roTpF/n6Vp2v8r05pSkeWtkc2gZp3/iq9XHTHVkBrH0JZl8Jddv61X4lFXctzI5Z8PWZfjHEiROh2fGhv9c5mPZT2PwhjPgGmg4u/f0LWvsyzPwVDH0VOl5c/uuVk4qRiogUwjnYscMHrk2b4Pvv8x8bNvjnLVvyV0Dmadr0yOBV8NG8eRl6wzJ3w+cnw97lfhhmxWOQUMsXLm13Ttg+s1RBO2bC1HN9eYWhr5bu36dN7/u5T3VawUmfQMOupb9/5i74pD++ftb88q0GzdoPH3aFuu39ytI4GDZXyBIRKaOsLB+08kJXwRC2YQMcOHDke2rUyN9+qOAjrzesUaNCOmQOp8FnJ8LeZX6/waGvRmcpvlR+Bzf7HqWdsyH5Ll86pKSAsuIJmHs9NBnke7BqNy/7/XfMhs+GQ+sx8JN3y94b+d2fYcnfYeSMuJmXp5AlIhIhzsHu3fmha+PG/Oe8x+bNRxZkBT/kWFgI69RmO50Tp1K/+7kkNaqmkTEJn5zDMGu8Lx3S7jy/QrWwLWhcbqBo7sO+PMKw18MzVL38EZh3I/R/BLrfUPr371/vV5q2Px+GvVb+9oSJQpaISAzl5MC2bUcGr7xHXiD74Qcf2ILVqgUtW/pHq1ZFPzdv7ueSiZTIOR92Ftzs94Y84X2o3zH/9exDfiXrxv9B12t9IArXhubO+dW0Wz+B076BpoNK9/5vfu7ndp250s8xixMKWSIicS4z0w9L5u0P+cMP/rF165HPO3YU/v6mTY8MXy1b+mONGkHjxv45+OcGDTR/vErbOhm++YUPUMP/64enD++AqWf5OVz9H/K7CYT7X5KMdPi0P2Awel7o87O2T/VD6cl3QvKE8LapnBSyREQqicxM2L796PBV2HPBlZPBqlWDpKQjA1jBQNaoETRp4nvKmjXzzwpnlcjeVb7u1b6V0GsCrHsFDm70Q3Htz4/cfXfMginDoc1Y+Mk7Jf8LlZsDkwZCxg5feDTOVtmWpxipiIjEkZo1/aT6tiWMljjnJ+Tv2gXp6f4571HY7+npsGZN/rGCKyrz1Krlw1Zw8Cr4CD5eO4p7FUspNezityaafrHf97BWEzj1i8jvAdh0MPS9328PtfIJ6HZd8eeve9lX+R/2RtwFrOKoJ0tERI6Smwv79vmwtXMnpKX53rO8R8Hft2/Pr75fUIMG+dX4iyp50aBBdD+fFOByYf3rvv5VtPaCdM73om391G/u3GRg4edl7YUPuvp2jfg2LrtR1ZMlIiIhS0jwWxAlJkKHDiWfn9drVjB45YWxbdv8Cstp0/x8s5ycI9/fqNHRwSu4DlnLln54UyLEEqDjuCjf0/zqxk/6+blho+f5rXgKWnIvHN4GJ34QlwGrOApZIiJSbmZQv75/HFtCR0hOjp8vFlxzLLj22LRpviRGsOrV87dAatzYzyULnjdW8Pe8Y3XrVri/y1VLrcZw/L/hsxP85ufD3z7yH9j+tb6MRMdLiu7pimMKWSIiElXVquXPKRtWxNSfvXvzy1sEPzZv9scXLvRBbM+e4u9Vo0bhQaxhQx8IGzQI7bl+ffWkRUyzodD3Xl+Xa+WTfvPzPPNvBqsOfe6NXfvKQSFLRETiTsOGcNxx/lGcnBwftIIn8e/efeTvwcd37IBVq/x8s3374NCh0NtUp86R4atRI+jUKf/RubN/Tkws32evkrrfCNu+hvl/gKZDoUkKbPsKNr4Dve+Guq1j3cIy0cR3ERGpsnJyYP9+/9i3r3TPaWmwdq2fbxasSZP8wBUcvjp1ghYtNHxZpIydfn6WVYdRqfD5KZC1G85YBtXrxLp1xdLEdxERkQKqVcuf4F9W+/b5sLVmjX+sXu2fp0+Hf//7yFIY9eod2fvVqZMvcZGd7ffILMtzdrb/HK1b+zlrec9t2vjCtBVmJ4BaTfLnZ32aAgfWwfFvxX3AKo5CloiISDk0aAB9+vhHQZmZsH790QFs+XL4+GPIyCj5+gkJfm5Z9epFP2dm+sUEhV0vr3xGwQAW/Ch0s/JYaDbMz79a8EdoNhza/yzWLSoXhSwREZEIqVkTunb1j4Jyc30wyso6MjAV/DkhIbR7OecLym7eMEjvDQAAIABJREFUXPhj0yaYNcsPcxZUu7YPYHkrMuvW9b1uBX8u7Fhx59aqVYbw1uMmqNEAWo2Kk+RXdgpZIiIiMZCQ4HuRwsXMzwdr0gR69y76vIwMH+7ywteWLfk/79vn653t2ePPOXAADh70zwcOFL0LQHFtKm04q1s3gXr1rqJZMz/cmfeoiAVrFbJERESqkFq1fIHZUIrMBnPO97oFB6+DB8v+c1qar4tW8LWi1uPVr39k6GrduvDfExPjpwNMIUtERERKZOaHP2vW9HO4IsE5vz1T3u4BW7f6x5Yt+T9v3Qpz58KHH/rzCqpdOz94tWnjFx+EOuQabgpZIiIiEhfMfD2yOnWgaVPo2bP48/ftKzyE5f2+cWPsAhYoZImIiEgF1aCBfxS2sCAexDDfiYiIiFReClkiIiIiERBSyDKzUWa2wsxWm9mthbz+KzNLM7MFgcflQa9damarAo9Lw9l4ERERkXhV4pwsM6sGPAWMADYBc8xsonNuaYFT33LOXVPgvY2BCUAK4IC5gffuCkvrRUREROJUKD1Zg4DVzrm1zrlM4N/A2SFe/3RginMuPRCspgCjytZUERERkYojlJDVBtgY9PumwLGCzjOzhWb2tpm1K+V7MbPxZpZqZqlphdX8FxEREalAwjXx/QOgg3OuN7636uXSXsA596xzLsU5l9KsWbMwNUtEREQkNkIJWZuBdkG/tw0c+5FzbqdzLm/v7+eAAaG+V0RERKQyCiVkzQG6mFlHM6sJXABMDD7BzFoF/XoWsCzw8yRgpJk1MrNGwMjAMREREZFKrcTVhc65bDO7Bh+OqgEvOOeWmNlfgVTn3ETgOjM7C8gG0oFfBd6bbmZ344MawF+dc+kl3XPu3Lk7zGxDmT5R6JoCOyJ8j6pK321k6fuNHH23kaXvN3L03UZWSd/vMYUdNFfUdteVnJmlOudSYt2OykjfbWTp+40cfbeRpe83cvTdRlZZv19VfBcRERGJAIUs+X/27jtMqvJu4/j3x8LSe5deBQQUWUADRAVR7CXGiLElllhjLFFMjN2oGFsivgmKsYvdYAMRUBFUWKrSi/S2tC20bc/7x3PWHZbtM7Oz5f5c17l25pwz5zxzXN3bp4qIiEgUVOWQNS7WBajE9GyjS883evRso0vPN3r0bKOrVM+3yvbJEhEREYmmqlyTJSIiIhI1ClkiIiIiUVDlQpaZjTSz5Wa2ysxGx7o8lY2ZrTWzH8xsgZklxro8FZ2ZvWhm283sx5B9TcxsipmtDH42jmUZK6oCnu19ZrYp+P1dYGanx7KMFZWZtTOz6Wa2xMwWm9nNwX797kZAIc9Xv79hMrNaZjbbzBYGz/b+YH8nM/s+yA5vBZOzF329qtQny8zigBXACPxi1XOAUc65JTEtWCViZmuBBOecJsWLADP7JZAGvOKc6x3sGwPscs49GvyPQmPn3J2xLGdFVMCzvQ9Ic879I5Zlq+iCVUBaO+fmmVl9YC5wLn6iav3uhqmQ53sh+v0Ni5kZUNc5l2ZmNYBvgJuBW4H3nXMTzOzfwELn3P8Vdb2qVpM1EFjlnFvjnEsHJgDnxLhMIgVyzn2NX0Uh1DnkLsL+Mv4/rlJCBTxbiQDn3Bbn3LzgdSp+qbU26Hc3Igp5vhIm56UFb2sEmwOGAe8G+4v9u1vVQlYbYEPI+43oFzPSHPC5mc01s2tiXZhKqqVzbkvweivQMpaFqYRuNLNFQXOimrPCZGYdgX7A9+h3N+LyPF/Q72/YzCzOzBYA24EpwGpgj3MuMzil2NmhqoUsib4hzrljgdOAG4ImGYkS59v7q06bf/T9H9AFOAbYAjwR2+JUbGZWD3gP+JNzLiX0mH53w5fP89XvbwQ457Kcc8cAbfEtYD1Ke62qFrI2Ae1C3rcN9kmEOOc2BT+3Ax/gf0ElsrYFfTJy+mZsj3F5Kg3n3LbgP7DZwPPo97fUgv4s7wGvO+feD3brdzdC8nu++v2NLOfcHmA6cDzQyMyqB4eKnR2qWsiaA3QLRgnEAxcBE2NcpkrDzOoGnTAxs7rAKcCPhX9KSmEicHnw+nLgfzEsS6WSEwAC56Hf31IJOg+PB5Y6554MOaTf3Qgo6Pnq9zd8ZtbczBoFr2vjB8otxYetC4LTiv27W6VGFwIEQ1qfBuKAF51zD8e4SJWGmXXG114BVAfe0PMNj5m9CZwINAO2AfcCHwJvA+2BdcCFzjl14C6hAp7tifimFgesBf4Q0odIisnMhgAzgB+A7GD3X/D9hvS7G6ZCnu8o9PsbFjPri+/YHoeviHrbOfdA8PdtAtAEmA9c4pw7WOT1qlrIEhERESkLVa25UERERKRMKGSJiIiIRIFCloiIiEgUKGSJiIiIRIFCloiIiEgUKGSJSIVgZllmtiBkGx3Ba3c0M80pJCIRVb3oU0REyoX9wVIXIiIVgmqyRKRCM7O1ZjbGzH4ws9lm1jXY39HMpgWL5U41s/bB/pZm9oGZLQy2XwSXijOz581ssZl9Hsz2jJn90cyWBNeZEKOvKSIVkEKWiFQUtfM0F/4m5Fiyc64P8Cx+RQeAfwEvO+f6Aq8D/wz2/xP4yjl3NHAssDjY3w0Y65w7CtgD/CrYPxroF1zn2mh9ORGpfDTju4hUCGaW5pyrl8/+tcAw59yaYNHcrc65pma2A2jtnMsI9m9xzjUzsySgbeiSGGbWEZjinOsWvL8TqOGce8jMJgFp+OWMPnTOpUX5q4pIJaGaLBGpDFwBr0sidB2yLHL7rJ4BjMXXes0xM/VlFZFiUcgSkcrgNyE/vw1ezwIuCl7/Fr+gLsBU4DoAM4szs4YFXdTMqgHtnHPTgTuBhsBhtWkiIvnR/5GJSEVR28wWhLyf5JzLmcahsZktwtdGjQr23QT818z+DCQBvwv23wyMM7Mr8TVW1wFbCrhnHPBaEMQM+Kdzbk/EvpGIVGrqkyUiFVrQJyvBObcj1mUREQml5kIRERGRKFBNloiIiEgUqCZLREREJAoUskRERESiQCFLpIoys/vM7LUoXn+xmZ0YvDYz+6+Z7Q6WvhlqZsujcM/2ZpZmZnGRvraISEkpZIlUYmZ2sZklBsFji5l9ZmZDyuLezrmjnHNfBm+HACPwM60PdM7NcM4dGe49gnULTw6553rnXD3nXFa41y7gfmZma8xsSTSuLyKVi0KWSCVlZrfi1/H7O9ASaA88B5wTg+J0ANY65/bG4N6R9EugBdDZzAaU5Y0107xIxaOQJVIJBZNnPgDc4Jx73zm31zmX4Zz7yDn35wI+846ZbTWzZDP72syOCjl2upktMbNUM9tkZrcH+5uZ2cdmtsfMdpnZjGCW9J9rmYJJP18Ajg9q1O43sxPNbGPI9duZ2ftmlmRmO83s2WB/FzObFuzbYWavm1mj4Nir+OD4UXDdO8yso5m5nEBiZkeY2cSgbKvM7OqQe95nZm+b2SvB91psZglFPNrLgf8BnwavQ5/fUWY2JbjXNjP7S7A/zsz+Ymarg/vMDb7vIWUNzv3SzK4KXl9hZjPN7Ckz2wncV9jzKOg5mll8UKY+Iee1MLN9Zta8iO8rImFQyBKpnI4HagEflOAznwHd8DU184DXQ46NB/7gnKsP9AamBftvAzYCzfG1ZX8hz9qBzrnxwLXAt0FT3r2hx4P+Ux8D64COQBtgQs5h4BHgCKAn0A64L7jupcB64KzgumPy+U4TgvIdAVwA/N3MhoUcPzs4pxEwEXi2oIdjZnWCa7webBeZWXxwrD7wBTApuFdX/PI9ALfiZ6E/HWgA/B7YV9B98hgErME/24cLex4FPUfnXHrwHS8Jue4oYKpzLqmY5RCRUlDIEqmcmgI7nHOZxf2Ac+5F51yqc+4g/g/30Za7rl8G0MvMGjjndjvn5oXsbw10CGrKZriST743EB8a/hzUuB1wzn0TlGmVc26Kc+5gEAieBE4ozkXNrB0wGLgzuOYCfI3aZSGnfeOc+zTow/UqcHQhlzwfv2zP58AnQA384tEAZwJbnXNPBPdKdc59Hxy7CrjbObfceQudczuL8x2Azc65fznnMp1z+4t4HgU+R+BlYJSZWfD+0uD7ikgUKWSJVE47gWbF7ccTNGk9GjRppQBrg0PNgp+/wtfErDOzr8zs+GD/48Aq4POgQ/hoSq4dsC6/QGhmLc1sQtBEmQK8FlKmohwB7HLOpYbsW4ev4cmxNeT1PqBWIc/scuDtIPAcAN4jt8mwHbC6gM8VdqwoG0LfFPE8CnyOQeDbB5xoZj3wNW0TS1kmESkmhSyRyulbfK3LucU8/2J8h/iTgYb45ibwzVM45+Y4587BNyV+CLwd7E91zt3mnOuMb3q71cyGl7CsG4D2BYSbv+ObH/s45xrgm7ws5HhhtWabgSZBU16O9sCmEpYPM2sLDAMuCfqtbcU3HZ5uZs2C79C5gI9vALrksz9nEECdkH2t8pyT9/sV9jwKe47ga7MuwddivRsERRGJIoUskUrIOZcM3AOMNbNzzayOmdUws9PMLL++S/XxoWwn/o/+33MOBB2nf2tmDZ1zGUAKkB0cO9PMugbNUMlAVs6xEpgNbAEeNbO6ZlbLzAaHlCsNSDazNkDeTvvbKCDcOOc2ALOAR4Jr9gWuxNf+lNSlwArgSOCYYOuO7+81Ct8XqrWZ/cnMappZfTMbFHz2BeBBM+tmXl8zaxo0923CB7c4M/s9+YexUIU9j8KeI8H3Pg8ftF4pxTMQkRJSyBKppJxzT+A7Xd8NJOFrOm7E10Tl9Qq+KW0TsAT4Ls/xS4G1QRPVtcBvg/3d8B2+0/C1Z88556aXsJxZwFn4Jqz1+ODym+Dw/cCx+AD3CfB+no8/AtxtfnTj7flcfhS+Vm4zfhDAvc65L0pSvsDl+O+2NXQD/g1cHjRJjgi+x1ZgJXBS8Nkn8TV/n+MD6nigdnDsanxQ2gkchQ+FhSnweRTxHHNC5zx8TdiMkj8CESkpLRAtIlJFmNmL+M70d8e6LCJVgSa3ExGpAsysI36EZL/YlkSk6lBzoYhIJWdmDwI/Ao87536KdXlEqgo1F4qIiIhEgWqyRERERKKgXPbJatasmevYsWOsiyEiIiJSpLlz5+5wzh22Fmi5DFkdO3YkMTEx1sUQERERKZKZrctvv5oLRURERKJAIUtEREQkChSyRERERKJAIUtEREQkChSyRERERKJAIUtEREQkChSyRERERKJAIUtEREQkCsKajNTMRgLPAHHAC865R/Mcbw+8DDQKzhntnPs0nHuKiIhI1ZKdDSkpkJx86LZnz+H7Qo85B99+G7tylzpkmVkcMBYYAWwE5pjZROfckpDT7gbeds79n5n1Aj4FOoZRXhEREakgsrNh714fkEK35OTD9+Vs1dK3cXW/G3hn7u/4aO4ZJCdDamrR96pZExo2PHRr0sQHLbPof9f8hFOTNRBY5ZxbA2BmE4BzgNCQ5YAGweuGwOYw7iciIiJRsm+fDzN79+ZuaWmHvi/OsbS03MCUmupDTlHq1oUGDeDItut56bIRdGiyguM6TKJx2+/ZW/0oGjU6PEDlbDnHataM/jMqqXBCVhtgQ8j7jcCgPOfcB3xuZjcBdYGTC7qYmV0DXAPQvn37MIolIiJSMeQEkEjWtGRmwu7dsHNn7rZr16Hv8267dsGBA8W/R40aPhjl3Zo0gXbtfOhp0KB4W/36UL06kLoKpg6HjD0w6G1qJ97Ew6eeB6fOhvhGkXtAZSjaC0SPAl5yzj1hZscDr5pZb+dcdt4TnXPjgHEACQkJxci9IiIi5Z9zsHUrrFhx6LZyJaxeDenpUK0axMX5sFG9eslfZ2XlBqY9ewouS/XqPgg1beq3zp1hwAD/ukkTH3pyAlO9evkHqbp1IT4+wg9pz48wbQS4TBg+HZocC7Vaw9STYNalcML/wCreWL1wQtYmoF3I+7bBvlBXAiMBnHPfmlktoBmwPYz7ioiIlDt79hweonJep6XlnlezJnTtCj16wJln+tCSmem3rKzSva5WDbp0yQ1PBW3168euf1KBds6B6SMhrhYM/woa9vL7WwyBY5+CuTfBDw9A3/tiWszSCCdkzQG6mVknfLi6CLg4zznrgeHAS2bWE6gFJIVxTxERkajKzPT9kwrqi7R3rz++Y8ehQSop5K9btWrQsSN07w5Dhvif3btDt26+OS0uLmZfr3zZ/jV8eSbUbAbDv4B6nQ893v0G2DUHfrwfmvSHtmfFppylVOqQ5ZzLNLMbgcn46RledM4tNrMHgETn3ETgNuB5M7sF3wn+CueK0wVORESkeLKzD+1sHTpyraDXhXXoTk8v/r1btfLh6ZxzcoNU9+6+Ga48dsQu0IEkWPUfaHwstDm9bO65eRLMOA/qdoRhX0CdNoefYwYD/u2bE7+9BE6dAw26l035IsDKY+ZJSEhwiYmJsS6GiIjESHo6bNwI69Ydum3efHhgKu4Itvr1fYfs+vX9VqfO4X2N8ttX0P5GjXy/pQotOwtWPw8L/wLpu/2+I86E/k9D/S7Ru+/6d2HWxdCwN5w0GWo1L/z8vetgUn+o1RJO+Q5q1I9e2UrBzOY65xLy7o92x3cREZHDpKUdHqBCty1bDg9OrVtDmzY+3LRufegItpzX+e3LGcFWreL1m46unXNgzvWwKxFanOiD1dYv4If74JNe0PPPcNRdUL1uZO+75mX4/vfQ9Dg48ZPijRys2wEGvw3TR8B3v4Mh75TDzmWHU02WiIiUmHN+yH9qqg9MqamHvs67LyXF10LlhKhduw69Xo0avq9Shw5+a98+93WHDv5YhWp+K88O7vQ1V6ueh9qtoN8T0OGi3NCybzMsuBPWvgZ12sGxT0K7X0Um1Cx/1ndkb3Uy/PLDkge4pf+A+X+Gox+Bo0aHX54IKagmSyFLRKSKyszMnT8p7zxKoe93784/OGVlFe8+NWr4mqRWrQ4NTqFbq1bqDB51LhtWj4cFoyEjGY68GfrcCzUa5H/+9hmQeCPsWQQth0PCP3NH/pXG4kd8uGt7Dgye4EcTlvg7OJg5Cja8Ayd+Bq1PKX15IkghS0Skoti9ANa/B92uzb8zcCH27IH163O3pKT8J57cudP3aypI9eq5cyc1bpzb5Fav3qE/C3odui/icypJye2a65sGd86GFr+EhLHQqHfRn8vO9B3iF94NmWlBMLun4GCWH+d8uFryKHS4GI5/CarVKPVXIXMvTD4O9m+CkXOhXqfSXytCFLJERCqCzZPhmwv8H7S4Wv6PWq87Ib4xGRm+yS00ROXdUlIOv2SjRrmBKXTOpLzvQ/eVy/mUpOQO7oJFd8PKf0OtFtDvH9DxtyX/h3sgCRb+FVa/4Duf9xsDHS8p+jouGxJvgpXPQdc/wIDnIjOpaOoqmDTA99U6ZRZUrxP+NcOgkCUiUk4552ugDix5iVbrriLZ+vBF8nN0znyOYxq/TtrBRvxr2mj+/u5N7DtY+5DPNmvm+y/lt7VrBy1aBEuWVAarnodV42DgOGjSL9alKd9cNqx5yfetSt8F3W+CPvdDfMPwrrtzjm9C3Dkbmg+GhGeh8TH5n5udCd9fCT+9Aj1vh2PGRDa5b/oUvjoTOl4Mx78a0/8rUMgSESlD+/b5prrt24vekpIcd57xEA/++h4+/2EEFzzzLqn7GxAfDyMSFnL3WX/huPafkpzRhkXuPg4ecQXtOlSnXTs/tUCVsOVz+PI0/7paPAx8ATr9NrZlKq92zYfEG2DHt0EQGguNj47c9X8OcKMhfSd0vRb6Pgg1m+Sek3XQT9Gw4X3o8wD0vjs6IejHh2DR3+DYp6HHzZG/fjEpZImIRJBzsG0bLF/utxUrcn9u3nzoMiqh6tb1tUs5W6uWmVx19PUMbPI8a9xlrGn+PM1bxtOypT/+87QD27+G+XfCzu+gwZHQ92Fod37VaNNLWQ6TB0Hd9jD0Az/8f/vXcOSfoN/jUK2yVNWFKX2PDxwrn4P4pv7ZdLo0emv+pe+BRffCymchvrEf8df595B9EGacD1sm+2VxevwpOvcHH/hmnA+bPvYTmrY8MXr3KoRClohIKezb55dOCQ1SOVto/6datXJn+27fHpo3PzRMtWjh99UNHbGeuRe++Q1s/gSO+gv0fajw0OQcbPyf70ScshSaDoRjHoWWJ0Xt+8dc+m7fyTl9l5/tu15HyM6AebfDin/6+Z2GvF30ZJaVmXO+SW7BHXBwB3S9Do5+qHjzT0XC7kV+WobtX/ulb6rV9LVog56HLldG//4ZKTB5oO9/NnIu1G1X9GciTCFLRCQf2dl+tN327bBp06Ehavly35k8VLt2cOSRh2/t2pVwsssD2/2abbvn+uacbteWoNBZ/o/qD/fCvg3Q+lRfi1DZ+illZ/omwu1fwbCp0GLoocfXvAJz/gA1m8MvP/B/4Kua3Qt8H6mkmX5yzwFjocmxZV8O52DdBJh/u//d/sVr0OE3ZXf/5KU+aDXoCSO+Lt30EGFQyBKRKsE531RXeB+oQ19nZx96jfr18w9S3bpFqA9U6iqYPhL2b/bzBbU9u3TXyToAK8bC4r/7mp4OF/nasGguh1KWEv8IK/4Fg8ZDl9/nf86uufD1+XBgm+8Q3/mysi1jrKTvhoV/g1X/B/FNfI1m599Fr2mwuDL3+n8WeRd6LgsbPvBNh12uhIHPl2lTukKWiFQaWVmwejUsWuS3H37w69zlBKcDB/L/XIMGhzfhHdI/Kljst1WrKP73ecf3fkQUwAkfQbPjwr9mejIsfRyWPQXZ6dD1Guj9Nz+bd3FkZ/qQdnBHyLYz93WNBtDzDqheu+hrRcrK/8Cca6HHrXDsE4WfeyAJZv4Gtk33o+iOfSK8eZjKM5cNq1+EhXf5f2bdroe+D/g+UeLn81r8sF9Uutsfyuy2ClkiUiHt3HlomFq0CH78Efbv98erVfM1TJ06FR6gmjf3/aZiauNHPgzUbg0nToIG3SJ7/f1b4McH/VQH1eJ9QGmakCc87YADwc/0IEjlLAycn+p1fe1E00F+GZTiBrdwbPsSpo3wS6+c8DFUK8ZU8NmZMP8OWP6Un2xz8NtQu2XUi1qmds6BOTfArjnQfEgwfUIERw1WBtlZ/n9itk2F4V9B8+PL5LYKWSJSrqWnw7JluUEqZ9u8OfecZs3g6KOhTx/o29dvvXpB7TKsYCm1lf+BxOuh8bE+OEQzAKSu8v9Hv/6tQ/fH1fL9l2o2hZrNCthCjsU39bVXGz6AWZf4fSd8BI37Rq/saWv8JJO1WsAp35V8XqefXofZV/vvMfR9aDogOuUsSwd2+MEOP08E+njpJhStKtJ3w6QEyNrvO8LXbh31WypkiUi5kJ4Oq1b5QLVsGSxZ4sPU0qV+LT3wy7D07JkbpHK2li0r4N8V5/yw+sUPwxGnw+C3oEa9srl3ykrITM0NTeHMir1rHnx1lh/JNfgtaHN65MqZIyMFPj/e18idOhvqdy3ddXbNhxnnwf6tMOD/oMvvIlvOspKdBavH+ZnWM1JLt6RNVbV7kf9datIPhk2DuOiu7VRQyNLkIiISFXv25AappUtzX69efejCwm3b+gB1xhm5Yap7d7+ocIWXnQHfXw0/vew74w74d9nO6RTJ5sgmx/rg89VZ8PVZfv6j7jdFLvVmZ8HMi/2cWCd9XvqABf4P66mJMPMiP6fWrkRf3ij/oY2opFl+QtHdC6DlMEj4V3iLM1c1jfv6ARNrX4PsAzH7Z6+aLBEptexs3+E8NETlhKpt23LPq1HDB6cePXK3nj39vvr1Y1f+qMpIhRm/gq1ToM990PueClgNl4/Mvb7pcOOH0O066P/PyATH+Xf4zvsDnvPXjYTsTN/MtvRxP/P5kHfLpk9ZOPZv9Uvh/PQK1GkLxz4J7S6oHL87seBcmTw71WSJSIk45+eP2rQpd9u48dD3q1b5yTpzNGrkw9Ppp/ufOYGqU6dKtH5ecezfAl+eDnt+KHz6gYqoel0Y+h4suAuWjoHU1TDkrfAmvlzzsg9C3a6PXMACH/76jfHzZ333e5jU35c9EiM6Iy07w0/H8cO9vi9Rr7v8BLVl1bRcWcU4nKomS6QKysiALVsOD02hYWrz5sOnQjDzI/XatPFbly6H1kw1bx7z/6bFXvISH7AO7oAh78ARp8W6RNGzejzMvhbqd4MTPy7d3EhJs2DqSX603EmTojf1wu5Fvp/Wvo1+8tfOv4OsvZCR5mvnMkN/5vM6Iy3k/GCfy4QaDX3ArNHI/yzsdfUG+Y+U3PYlJN4EyT9C65HQ/xlo0D06z0GiQh3fRaqoAwdgwQKYMwcSE/3PZct8TVWoWrVyw1PerW1b/7N160rSVypSMvf7PjM7Z/th9TtnQ+pKPzLuhE/89AmV3bbpvlnU4vwUD80HF/+ze9f5Wbqr1/f9vUIXGI6Gg7v8osVbJpfwg+Zr8KrXy/1Zo57/zhkpfg2/9D2QkQwU8Te1ev1DAxhA0gyo2xH6Pw1tztb/qVRAClkiVUBGhp9DKidMzZnj3+eM2mvZEgYMgH79/Pp6oSGqcWP9t71Q2VmQssTPVbRztt/2/OBrM8APE286yE8Z0PHSmKyfFjMpK+DLM2Dfehj0InT6bdGfyUiDKUNg709+qoaGPaNfTghG7L0AB7YGoSlPcIqr63+G7o+rXbx/OVy274uXsSckeOXzOnRfZiq0OavsJ3uViFLIEqlksrL8gsU5YSox0ddY5TTxNW4MCQk+VOX8bNNGQapYnPO1LDlhatccv3xL5l5/vEZDaJLgF2huOtAHqzptYlvmWDu409dobf/Kzzbf576Cl3hx2TDjAtj0P1/jd8TIMi2qSKSp47tIBZaa6gPV8uUwf74PVXPn+jX6AOrWhf794YYbcgNV584FBCqX7Zu06nUp2+kEyrOMNNjxLeyYlRusDu7wx6rFQ+N+0Pn3Pkw1Hej7IMV6jbjypmZTP/XCnGv9rPMpy+G4l/KvnVnLVNrqAAAgAElEQVR0L2z8wE+roIAllZj+CytSTmRkwNq1PkjlBKqc11u25J5XsyYccwxcfrkPUwMG+MWL44qx8ggA8273S49Urw/NjocWQ6H5UB8eqkpzxYEkSPoGts/w/WF2zweXBZifi6jNWbmBqmGfijW/UizFxfvRlA16wILRvjYw71I8a9+ExQ/5ecOOvDl2ZRUpA2E1F5rZSOAZIA54wTn3aJ7jTwEnBW/rAC2cc0WO81VzoVRWzvkFjEODVM7P1atz+04BNG3qw1P37v5nzutu3fyM6KWSs0p9+wt9zcP2GX5EE/gamyYJuaGr+eDwhuWXFzlNf0kzckNVyjJ/rFpNaDYo+L5D/Tpnmk07MjZ8ALN+65fxOfFjaNTH92f74pfQZAAM+0LhVSqNiPfJMrM4YAUwAtgIzAFGOeeWFHD+TUA/51yRE8YoZEllcPBgbif0uXNh4UIfppKTc8+pWRO6dj00ROX8bNo0wgVKWwOfHQv1u8OIGRBXMyjoLkiamRtCdiUGnbnN/2FsPjQ3eNU5IsKFigKXDcmLg0D1jf9e+zb6YzUa+vCY852aJOQ+B4m8XXPhq7P9CLz+/4RFf/Vh/tQ5UKt5rEsnEjHRCFnHA/c5504N3t8F4Jx7pIDzZwH3OuemFHVthSypaNLT/cLGc+f6LTHRv8/I8McbNfIj+nr2PDRMtW9fgma+cGQdgM8H+6B12jyo16ngczP3wc7vc2t9dnyb2+G7XueQ0DXEB7by0JM+eSls+hi2fw07ZvoFYsGP+MuppWoxFBr2zn+eIomefZv8Ujy75/uReqfM8uFdpBKJRsf3NsCGkPcbgUEF3LwD0AmYVkgBrwGuAWjfvn0YxRKJrvT0Q2uo5s71CxyHBqr+/eGWW3wn9P79/YznMc0i826D3fN8/5jCAhb4RYRbnuQ38EuT7J6fG7o2f+LX4gO/3MfgN6I3gWRxbJ4EX58D2ek+9LU7PzdU1Y31gxfqtIGTv4ZF90CbMxSwpEopq47vFwHvOueyCjrBOTcOGAe+JquMyiVSKOdg8WL49ttDa6jS0/3x0EDVv78PVTEPVHmtewtWPgc9boO255T889WqB53AB0DPW/1DSVkO696EHx+AmdkweEJsgtaWKfD1udDwKDhhol/rTcqfGvWg/5OxLoVImQsnZG0CQmfbaxvsy89FwA1h3EukzKSkwBdfwGef+W1T8FudE6j+9Cf/s3//QqZJKC9SVsD3V/tRhMfk25JfcmbQsAf0vR/im8C8P8HMUTD4zbINWlunwddnQ4MjYdgU35FfRKQcCSdkzQG6mVknfLi6CLg470lm1gNoDHwbxr1EosY5XzuVE6pmzvSj/Bo0gBEj/GLHJ5xQAQJVXpn74Ztf++Az+K3oBKAeNwMO5t0CMy8uu6bD7V/7fj71uvhRagpYIlIOlTpkOecyzexGYDJ+CocXnXOLzewBINE5NzE49SJggiuPU8tLlZWcnFtbNWlSbm3V0UfD7bfDaafB8cdX8HX65t4Mexb5GbWjucRLjz/hg9atMMvgF29Ed5LTpJl+Aea67WHYVI1SE5FyK6z/EjrnPgU+zbPvnjzv7wvnHiKRUFht1Smn+FA1ciQcUQFmKCiWn16D1c9Dr9HQ5vTo36/HLf4hz78NMPjF69EJWju+g+mnQe02MHwa1G4Z+XuIiESIZnyXSiunb9Wnn1bi2qr8JC+F2X/wI+z6Plh29+15K+Bg/u34oPVaZIPWzjkw/VSo1SIIWK0jd20RkShQyJJKZcUK+OQTv82amUH3lov5afcxlbO2Kj+Ze30/rOp1g47oZfyveM/b/GSgC+7wHdiOfzUyZdg1D6adAvFNYfh0LcYsIhWCQpZUaOnpMGMGfPyxD1YrV/r9vXo5Zv7jcvo1fpPMhPFU717kQgOVQ+KNkLwETpocuyDS68+AgwV3AtXg+JfDC1q7F8K0kyG+IZw8Pbr9y0REIkghSyqcbdt8E+Ann8Dnn0Nqql+e5qST4Oab4YwzoGPqg/DDm1C7DdXnXQ/NjoYm/WNd9Oha/V9Y8xL0vgdaj4htWXrdgQ9ao/37418p3Uzre36AacN9zdzwaVC3Q0SLKSISTQpZUu5lZ8P8+T5UffwxzJnj97dpA6NG+VA1fDjUrRt8YN3b8MO90Oky6PcPmJQAM34FI+dW3qH+e36ExBug5TAfssqDXnf6zvAL7/JNh8e9XLKglbwEpg73izgPn+6X9BERqUAUsqRcSkvzndY//tjXWm3Z4v9ODxoEDz3kg9XRR+czb9WO2fDd5X5dvYHj/OK/Q9+FKUP8PE4nflr51q7LSINvLoAaDYJRfeXo+x01GsiGhX8FqsFx/y1e+VKWw9RhYHE+YNXvGu2SiohEnEKWlBtZWTBtGrzyCrz/PuzbBw0bwqmnwpln+k7rzQubEmnvBr+GXa3WMPR9H7DALweTMBZmX+1ruI5+qEy+T5lwzo8kTF3pJ+Ws3SrWJTrcUX/x5Vx0d5CUXyw8aKWshKknAc4HrAbdy6yoIiKRpJAlMbdsGbz8Mrz2Gmzc6JevuewyuPBCGDKkmFMsZKT5JVay9vmwkXeCyq5Xwc7vYPHD0HQgtD07Kt+lzK1+Hta94adqyFnQuTzq/VfAwaK/AQaDxucftNLWwLRhkJ0Bw7+Ehj3LuKAiIpGjkCUxsWsXTJjgw9Xs2RAX52uqnnwSzjoLatUqwcVcNnx7Se7s5o2Oyv+8hGdh9wL49lI4NREadIvId4mZ3Qsg8Y/Q6hRfW1Te9b47mBX2nqBGazxYtdzjaWvhi5Mgc5+vwSron6OISAWhkCVlJiPDTwr68svw0Ud++oW+feGJJ+Dii6FVaVu6FtwFG/8H/Z+BI0YWfF5cLRj6HkzqDzPOh1O/86PWKqKMFJjxa6jZzE/6GRpWyrM+fwOy4Yf78DVaL/iy713vmwgzUvwowsZ9Y1xQEZHwKWRJ1C1Y4IPV669DUpLvV3X99XD55XDMMWFefM1LsHQMdLsOut9U9Pl1O8Av3vQzh39/te8oXqFWfcbXBn1/Fez9yTepVbS1+/rc67/Dj/cDBn3u853c03f7pt4m/WJdQhGRiFDIkqjYutWHqldegUWLID7eNwNefrlvFozIUjbbv4bZ10Crk30tVnHDUusRvvP7wr9Cs+PgyD9GoDBlaOVzsP4dOOYxaDEk1qUpnb73AQ5+fADWvwVUg2FToGlCjAsmIhI5ClkSMc7B5Mnw7LO+WTArCwYOhLFj4aKLoEmTCN4sdbVv8qvXGYa8A9VKmNp6jYads2HebdD42IoTVnYmwrxb4IgzoOftsS5NePrc55sKVz4HQz+AZoNiXSIRkYgy51ysy3CYhIQEl5iYGOtiSDGlp8Obb8I//gE//gitW/saq8svhx49onHDZPj8eDiwDU79vvRzKKUnw+QBkJEKp80rvwsOu2xI+gZ+ehXWvw01GsJp8yvPxKrOVbwmWxGREGY21zl3WFW8arKk1JKTYdw4eOYZ2LQJeveGl17ys7DHx0fpptmZ8M2FwbxQU8KbpDK+oZ9Pa/Igf83h00peIxZNyctg7auw9nXYu8530m97vh9JWFkCFihgiUilpZAlJbZxow9W48ZBSgoMGwYvvOAnDTWXBXvXQo1O0RnxNu8W2Pq5H5XW8sTwr9eot59KYNYomP9n6P90+NcMx4HtsG6Cr7XaleifYasR0PchaHdexR0NKSJSBSlkSbH98INvEnzjDb+e4IUXwu23Q/+cdZf3b4VZF8O2YJ25zr+DzldAnbaRKcCKsbDiWehxG3S5MjLXBOh4Eez8HpY/DU0HQcdRkbt2cWTuh00TfbDaMglcFjQ+Bvo94ctSXpsxRUSkUOqTJYVyDqZPh8cf953Z69SBq66CW26Bjh1DTtw6FWb91s9z1OM22DHThy2rBq1OhS6/hzZnQ1wp2xG3fA5fng5HnO47SUd6fb7sDL8Y8a65fv6sRn0ie/28XDZs/yroZ/UuZKZC7TbQ8bfQ6VJfwyYiIhWC+mRJiWRmwrvv+nA1bx60aOEXZr7uujyjBLOzYPFD8MP90OBIP89RTkBIWwOr/ws/vQTfBBNndrzUB66ShIjkpf7zDY+CX7wRnQWQq9WAIW/DpGPh6/Nh5ByIbxT5+yQv8cFq7euwbwNUrwftL4COl0CLE8vX4s4iIhIW1WTJIdLS4MUX4amnYO1a6N7dNwleemk+S93s3+Zrr7ZN9eFpwHNQo97hF83Ogq1TYPV42PQ/X2vUdKBv8utwEdRoUHCBDuyAzwdB5l44dTbUbR/Jr3u4pJnwxYlwxGnwyw/D71eWnemnitjyOWz6CHbPA4vzS+F0uhTangPV60Sk6CIiEhsF1WQpZAkAu3f7YPXss/714MHw5z/7CUSr5Zcztk2HmRdDxh5IGOv7XxVnlNiBJF+Ls3o8JP8IcbWh/a994Go+9NBrZKXD9BGw43s4+Us/cWhZWP4vmPtH39m8919L9lnn/MjHrVP8tm26b0LFoOkA6DDKb7VbRqXoIiJS9tRcKPlKToann/YLM6ekwLnnwh13wPHHF/CB7CxY/LBfEqV+dxj2ecn6L9VqDj3+BEfe7EfPrR4P696En16Bel19U2Kny31n7znX+lndf/FG2QUsgO43wo7vYNHffDBqfUrh5x/c6fuk5QSrvev8/rodof1v/OdbDoOakZyNVUREyjvVZFVRqanwr3/50YK7d8N558F99/kFmwu0fxt8ewls/cJ30B7w7/ybB0sqc5/v/L3mRd8Z3KpB4/6waw70vgf63h/+PUpcpr1+wtN9m2DkXKjXMfdY1kHYMQu2TPHTSeyaBzg/SWjLYX7ZnlYjoF4XzQElIlIFqLlQANi71y9zM2YM7NwJZ54J998Pxx5bxAe3fQkzRwXNg89C599HJ0CkroI1//Wdw1ucCMe/HLugkroKJiX4sDToef8Mtk7xtWtZ+8CqQ7PjfaBqPQKaJEA1VQ6LiFQ1UQlZZjYSeAaIA15wzj2azzkXAvcBDljonLu4qOsqZEXe/v3w73/Do4/C9u1+4tD774dBRS0X57Jh8d/hh3t9c96Qd6BxYdVdlcymj+Grs3LfN+jhQ1WrEX4y1Br1Y1Y0EREpHyLeJ8vM4oCxwAhgIzDHzCY655aEnNMNuAsY7JzbbWYtSns/KZ0DB+D55+GRR2DLFhg+3IerwYOL8+HtMOsSX3vTYRQM/E/VCxVtzvTBMiMVWp0MddvFukQiIlJBhNO2MRBY5ZxbA2BmE4BzgCUh51wNjHXO7QZwzm0P435SAunpfiqGhx/2y+AMHeoXcT7hhGJeYPvXvnnw4E4YOA66XFV1+xe1vyDWJRARkQoonEmA2gAbQt5vDPaF6g50N7OZZvZd0LyYLzO7xswSzSwxKSkpjGJVbRkZMH68n9/quuugXTv44gv46qtiBqyc5sGpJ/l18k79HrpeXXUDloiISClFYQXfQ1QHugEnAqOA580s32m0nXPjnHMJzrmE5s2bR7lYlU9mJrz8MvTo4Ze9adECPvsMZs70TYTFykgHkvzSNQv/Cu0v9KPqGh8d9bKLiIhURuE0F24CQjuotA32hdoIfO+cywB+MrMV+NA1J4z7Sh5Tp8L118OKFdCvH3z0EZxxRgkrn/ZugC9OgP2b/dQMXa9R7ZWIiEgYwqnJmgN0M7NOZhYPXARMzHPOh/haLMysGb75cE0Y95QQ+/bBH/8IJ5/sJxp//32YO9dPy1CifLRvs28eTN8JJ38N3f6ggCUiIhKmUtdkOecyzexGYDJ+CocXnXOLzewBINE5NzE4doqZLQGygD8753ZGouBV3ezZcNllsHy5D1qPPAJ1SrME3v5tMG04HNgGw6ZAs4ERL6uIiEhVpMlIK5iMDHjwQfj736F1a3jpJd/nqlQO7PA1WGlr4KRJ0GJoJIsqIiJSJWjtwkpgyRK49FKYN8/XYj3zDDTKdxhBMRzc5RdfTlsFJ3yigCUiIhJh0R5dKBGQnQ1PPeWXvlm/Ht57z48kLHXASk+G6adC8hIY+iG0GhbR8oqIiIhqssq9tWvhiiv8PFdnnQXjxkGrVmFcMCMVvjwN9iyEoe/DEadGqKQiIiISSjVZ5ZRz8N//Qt++vnlw/Hj43//CDFiZe+GrM2HnbBj8ll8yRkRERKJCNVnl0LZtcM01MHGin6X9pZegY8cwL5q5H746G5K+gV+8Ae3Oi0BJRUREpCCqySpnPvgAeveGyZPhiSdg2rQIBKysgzDjfNg2HY57CTr8JgIlFRERkcIoZJUTyclw+eVw/vl+vcG5c+HWW6FauP+EstLhm1/Dlkkw6HnodGlEyisiIiKFU8gqB6ZOhT594PXX4W9/g+++g6OOisCFszNh1sWw6SMY8Bx0uTICFxUREZHiUMiKIefgrrv8sji1a/vFnB94AOLjI3Dx7Cz49lLY8B4c+zR0uy4CFxUREZHiUsiKEedg9Gh49FG4+mqYPx8GDYrUxbPh+yth3QQ45jHocXOELiwiIiLFpdGFMfLwwzBmDFx3HYwdG8H1mF02zP4D/PQy9HkAet0RoQuLiIhISagmKwaeftr3vbr0Unj22UgGLAeJN8HqF+Cov0Kfv0XowiIiIlJSClllbPx4uOUWP4rwxRcjMHowh3Mw7zZY+Rz0vB36PhihC4uIiEhpKGSVoQkTfP+rkSPhjTegeqQaa52DhX+B5U9B9z/CMWMiWD0mIiIipaE+WWXko4988+DQoX6B55o1I3ThzP0w9yZYPR66Xgv9n1bAEhERKQcUssrA1Knw619Dv34+bNWpE6ELp62BGRfA7vm+D1bfBxSwREREygmFrCibNQvOPhu6dYPPPoMGDSJ04Y0fwbeX+VB1wsfQ5owIXVhEREQiQX2yomjePDj9dGjTBqZMgaZNI3DR7ExY8Bf4+myo1xlGzlXAEhERKYdUkxUlS5bAqadCw4bwxRfQqlUELrp/G8wa5Rd67noN9H8G4mpF4MIiIiISaQpZUbBmDYwYAXFxPmC1bx+BiybNhG8uhPTdcNxL0PnyCFxUREREokXNhRG2cSMMHw4HDviA1a1bmBd0DpY9BV+cCHF14JTvFLBEREQqANVkRdD27X6x5507Ydo06N07zAtmpMB3V8KGd6HteXDcfyG+YUTKKiIiItGlkBUhu3fDKafA+vUweTIkJIR5wT0/woxfQdpq6PcP6HGrpmcQERGpQBSyIiA11Y8iXLoUJk70E46G5afX/CLPNRrA8GnQ4pcRKaeIiIiUHYWsMO3fD+ecA3PmwDvv+BGFpZZ1EObdAiv/zwerwW9B7UgMSxQREZGyFlbHdzMbaWbLzWyVmY3O5/gVZpZkZguC7apw7lfepKf7mdy//BJeegnOOy+Mi+1dB1OG+oDV8w4YNlUBS0REpAIrdU2WmcUBY4ERwEZgjplNdM4tyXPqW865G8MoY7mUleXXIvzkE/j3v+GSS8K42OZJMOu34DJh6AfQ7tyIlVNERERiI5yarIHAKufcGudcOjABOCcyxSr/XngB3n4bHn8c/vCHUl4kOwMW/g2+PB3qtPWztytgiYiIVArhhKw2wIaQ9xuDfXn9yswWmdm7ZtauoIuZ2TVmlmhmiUlJSWEUK/oyM2HMGBg0CG67rZQXSVkOnw+GxQ9B5yvglG+hftdIFlNERERiKNqTkX4EdHTO9QWmAC8XdKJzbpxzLsE5l9C8efMoFys877zjZ3UfPboUsyo45/tdfdbPT88w5B047kWoXicqZRUREZHYCGd04SYgtGaqbbDvZ865nSFvXwDGhHG/csE5ePRR6NkTzj67hB/ev8VPLrrlM2g90oer2q2jUk4RERGJrXBC1hygm5l1woeri4CLQ08ws9bOuS3B27OBpWHcr1yYNAkWLfKjCauVpB5ww/sw+xrI3AcJz0K36zW5qIiISCVW6pDlnMs0sxuByUAc8KJzbrGZPQAkOucmAn80s7OBTGAXcEUEyhxTjzwC7drBqFHF/EB6Msy9GX56GZokwPGvQsMeUS2jiIiIxF5Yk5E65z4FPs2z756Q13cBd4Vzj/Jk5kyYMQOefhri44vxge1fw7eXwb4N0PtvfqtWI+rlFBERkdjTjO8l8Nhj0LQpXFXUlKpZB2HRPbD0cajXGUbMhGbHlUkZRUREpHxQyCqmH3+Ejz6C+++HunULOXHPjzDrEtizELpeA/2egBr1yqycIiIiUj4oZBXTY4/5cHVjQXPXu2xY9jQsvAviG8EJH0GbM8u0jCIiIlJ+KGQVw9q18OabcPPN0KRJPifsXQ/fXQHbpkObs2HQ81CrRRmXUkRERMoThaxieOIJP13DLbfkOeAcrH0DEm8AlwWDxkPn32lqBhEREVHIKsr27X6dwksvhbZtQw5kHYBvr4D1b0GzX8AvXvWd3EVERERQyCrSP/8JBw/CHXfkObDhfR+wet8bTM0QF5PyiYiISPkU7bULK7SUFBg7Fs4/H448Ms/B5MVg1aH3XxWwRERE5DAKWYX4z39gzx6/EPRhkpdC/a6aXFRERETypZBVgAMH4Mkn4eSTISEhnxNSlkEDLY8jIiIi+VOfrAK8+ips3QqvvZbPwewMSF0Jbc8t83KJiIhIxaCarHxkZcGYMb4Ga9iwfE5IWwMuUzVZIiIiUiDVZOXjvfdg1Sp4990CprxKXup/NuxZpuUSERGRikM1WXk4B48+6kcTnndeASelBCFLNVkiIiJSANVk5TFlCsyfD+PH+1ne85WyDGq3gRr1y7RsIiIiUnGoJiuPRx6BNm3gkksKOSl5qZoKRUREpFAKWSG++w6+/BJuuw3i4ws4yTlN3yAiIiJFUsgK8dhj0LgxXH11ISft3wyZqdBANVkiIiJSMIWswNKl8OGHcNNNUK9eISemaGShiIiIFE0hK/DYY1Cnjg9ZhUpe5n+quVBEREQKoZAFrF8Pr7/umwmbNSvi5JSlUKMh1GpVJmUTERGRikkhC79GIcCttxbj5JSlvj9WvrOUioiIiHhVPmTt2AHPP++nbGjfvhgfSFkGDdVUKCIiIoWr8iHrX/+CffvgjjuKcXJ6MuzfopGFIiIiUqSwQpaZjTSz5Wa2ysxGF3Ler8zMmVlCOPeLtLQ0H7LOPRd6Fic3pajTu4iIiBRPqUOWmcUBY4HTgF7AKDPrlc959YGbge9Le69oGTcOdu+G0QXGwzx+XrNQNVkiIiJSuHBqsgYCq5xza5xz6cAE4Jx8znsQeAw4EMa9Iu7gQXjiCTjpJBg0qJgfSl4K1eKhXqeolk1EREQqvnBCVhtgQ8j7jcG+n5nZsUA759wnRV3MzK4xs0QzS0xKSgqjWMXz+uuweXMJarHANxfW7wbVtK62iIiIFC5qHd/NrBrwJHBbcc53zo1zziU45xKaN28erWIBkJUFY8ZAv34wYkQJPpgzfYOIiIhIEcIJWZuAdiHv2wb7ctQHegNfmtla4DhgYnno/P7hh7B8Odx1Vwmmu8o6CGlr1OldREREiiWckDUH6GZmncwsHrgImJhz0DmX7Jxr5pzr6JzrCHwHnO2cSwyrxGFyDh59FLp2hfPPL8EHU1eBy9KahSIiIlIspe5c5JzLNLMbgclAHPCic26xmT0AJDrnJhZ+hdi54w6Ij4e4uBJ8SCMLRUREpATC6sHtnPsU+DTPvnsKOPfEcO4VKWbw61+X4oM/z5HVPaLlERERkcqpys/4XmzJS6FuB6heN9YlERERkQpAIau4Upap07uIiIgUm0JWcbjsIGSpP5aIiIgUj0JWcezbAFn7NLJQREREik0hqziStTC0iIiIlIxCVnFo+gYREREpIYWs4khZCjWbQq3oLvcjIiIilYdCVnFoZKGIiIiUkEJWcSRrYWgREREpGYWsohzcCQeTVJMlIiIiJaKQVZSfl9NRTZaIiIgUn0JWUZKDkYWaI0tERERKQCGrKCnLIK4W1Gkf65KIiIhIBaKQVZSUpVD/SKgWF+uSiIiISAWikFUUTd8gIiIipaCQVZjM/ZD2k/pjiYiISIkpZBUmdQXgNLJQRERESkwhqzApWhhaRERESkchqzDJSwGDBt1jXRIRERGpYBSyCpOyFOp18lM4iIiIiJSAQlZhUpapP5aIiIiUikJWQbKzIGW5RhaKiIhIqShkFWTfOsg+qE7vIiIiUioKWQXJWbNQzYUiIiJSCmGFLDMbaWbLzWyVmY3O5/i1ZvaDmS0ws2/MrFc49ytTKTkhSzVZIiIiUnKlDllmFgeMBU4DegGj8glRbzjn+jjnjgHGAE+WuqRlLWUZ1GoBNZvEuiQiIiJSAYVTkzUQWOWcW+OcSwcmAOeEnuCcSwl5WxdwYdyvbCUvVVOhiIiIlFo4IasNsCHk/cZg3yHM7AYzW42vyfpjQRczs2vMLNHMEpOSksIoVgQ455sL1VQoIiIipRT1ju/OubHOuS7AncDdhZw3zjmX4JxLaN68ebSLVbiDSZC+WzVZIiIiUmrhhKxNQLuQ922DfQWZAJwbxv3KTs7IQs2RJSIiIqUUTsiaA3Qzs05mFg9cBEwMPcHMuoW8PQNYGcb9yo4WhhYREZEwVS/tB51zmWZ2IzAZiANedM4tNrMHgETn3ETgRjM7GcgAdgOXR6LQUZeyFKrXhTptY10SERERqaBKHbIAnHOfAp/m2XdPyOubw7l+zKQsg/pHgmmuVhERESkdpYj8JC9VfywREREJi0JWXhlpsG+9RhaKiIhIWBSy8kpd4X+q07uIiIiEQSErL03fICIiIhGgkJVXylKwOKjXNdYlERERkQpMISuvlGVQrwvExce6JCIiIlKBKWTllaKRhSIiIhI+haxQ2ZmQulKd3kVERCRsClmh0tZAdoambxAREZGwKWSFSglGFipkiYiISJgUskL9vDD0kbEth4iIiFR4ClmhkpdC7SMgvmGsSyIiIiIVnEJWqJRl6vQuIiIiEaGQlcM538YkOhYAAApTSURBVCdL/bFEREQkAhSycuzfAhkpmiNLREREIkIhK8fPnd7VXCgiIiLhU8jKoekbREREJIIUsnIkL4Xq9aF261iXRERERCoBhawcKct8fyyzWJdEREREKgGFrBwaWSgiIiIRpJAFflTh/s3q9C4iIiIRo5AFkByMLNT0DSIiIhIhClkQMrJQNVkiIiISGQpZ4Du9V6sB9brEuiQiIiJSSYQVssxspJktN7NVZjY6n+O3mtkSM1tkZlPNrEM494ualKVQvxtUqx7rkoiIiEglUeqQZWZxwFjgNKAXMMrMeuU5bT6Q4JzrC7wLjCnt/aJKC0OLiIhIhIVTkzUQWOWcW+OcSwcmAOeEnuCcm+6c2xe8/Q5oG8b9oiMrHVJXafoGERERiahwQlYbYEPI+43BvoJcCXxW0EEzu8bMEs0sMSkpKYxilVDaKnBZClkiIiISUWXS8d3MLgESgMcLOsc5N845l+CcS2jevHlZFMvLWRi6oZoLRUREJHLC6em9CWgX8r5tsO8QZnYy8FfgBOfcwTDuFx3JwfQN9Y+MbTlERESkUgmnJmsO0M3MOplZPHARMDH0BDPrB/wHONs5tz2Me0VPyjKo0w5q1It1SURERKQSKXXIcs5lAjcCk4GlwNvOucVm9oCZnR2c9jhQD3jHzBaY2cQCLhc7WrNQREREoiCsiaGcc58Cn+bZd0/I65PDuX7UuWxfk9XlqliXRERERCqZqj3j+75NkLlXc2SJiIhIxFXtkPXzmoVqLhQREZHIqtohK1kLQ4uIiEh0VO2QlbIM4htDrRaxLomIiIhUMlU8ZAUjC81iXRIRERGpZKp4yNLC0CIiIhIdVTdkpe+GA9ugoTq9i4iISORV3ZClTu8iIiISRVU3ZOUsDK3pG0RERCQKqnDIWgrVakLdjrEuiYiIiFRCVTdkJS+DBt2hWlysSyIiIiKVUNUNWVoYWkRERKKoaoasrAOw9yeFLBEREYmaqhmyUleCy9bIQhEREYmaqhmycqZv0BxZIiIiEiVVM2SlLAUM6nePdUlERESkkqqiIWuZn7qheu1Yl0REREQqqaoZspKXqqlQREREoqp6rAsQEwn/BNP8WCIiIhI9VTNktfhlrEsgIiIilVzVbC4UERERiTKFLBEREZEoUMgSERERiQKFLBEREZEoCCtkmdlIM1tuZqvMbHQ+x39pZvPMLNPMLgjnXiIiIiIVSalDlpnFAWOB04BewCgz65XntPXAFcAbpb2P/H97dx8i11WHcfz7uEYMqbSpCSG0qakaENTallhQipSCggpWUdvEClWEarEaESTFP7QWBSkqEi3VFCsRqzHY1z9KbaihKkqbbN003QR8KVts2CYbStQF8SV5/OOejUPct+yd08nMPB8Y5t4zs+ee+XHY/e05Z+6JiIiIftTmFg5XAH+y/SyApJ3ANcDBmTfYniivnWxxnYiIiIi+02a68ALgLx3nz5eyJZF0o6R9kvZNTU21aFZERERE7501C99tb7e90fbG1atX97o5EREREa20mS48DKzrOL+wlLU2Ojp6TNJz3ahrHquAY5WvMawS27oS33oS27oS33oS27oWiu9rZitsk2TtBTZIupgmudoEfKRFfafYrj6UJWmf7Y21rzOMEtu6Et96Etu6Et96Etu6lhrfJU8X2v4PcDPwC+AQsMv2uKTbJL2vNOqtkp4HPgx8X9L4Uq8XERER0U9abRBt+2Hg4dPKvtRxvJdmGjEiIiJiqJw1C997YHuvGzDAEtu6Et96Etu6Et96Etu6lhRf2e52QyIiIiKG3jCPZEVERERUkyQrIiIiooKhS7IW2tQ62pE0IemApDFJ+3rdnn4n6W5JRyU901F2vqTdkv5Ynlf2so39ao7Y3irpcOm/Y5Le08s29itJ6yTtkXRQ0rikLaU8fbcL5olv+m9Lkl4p6UlJ+0tsv1LKL5b0RMkdfibpFYuqb5jWZJVNrf8AvJNmG6C9wGbbB+f9wVg0SRPARtu5KV4XSHoHMA38yPabStntwIu2v17+UVhpe2sv29mP5ojtrcC07W/0sm39TtJaYK3tpyS9ChgF3g98jPTd1uaJ77Wk/7YiScAK29OSlgG/AbYAnwfus71T0veA/bbvXKi+YRvJOrWpte1/ATObWkeclWz/CnjxtOJrgB3leAfNL9c4Q3PENrrA9qTtp8rx32nupXgB6btdMU98oyU3psvpsvIwcDXw81K+6L47bElWVze1jlkZeFTSqKQbe92YAbXG9mQ5fgFY08vGDKCbJT1dphMzndWSpPXAZcATpO923WnxhfTf1iSNSBoDjgK7gT8Dx8tN2OEMcodhS7KivittXw68G/h0mZKJStzM9w/PnH99dwKvAy4FJoFv9rY5/U3SOcC9wOds/63ztfTd9maJb/pvF9g+YftSmpupXwG8Yal1DVuSVW1T62jYPlyejwL303TQ6K4jZU3GzNqMoz1uz8CwfaT8gj0J3EX675KV9Sz3AvfYvq8Up+92yWzxTf/tLtvHgT3A24DzJM3skrPo3GHYkqxTm1qXbwZsAh7qcZsGhqQVZREmklYA7wKemf+nYgkeAm4oxzcAD/awLQNlJgEoPkD675KUxcM/AA7Z/lbHS+m7XTBXfNN/25O0WtJ55Xg5zRflDtEkWx8qb1t03x2qbxcClK+0fhsYAe62/bUeN2lgSHotzegVNPti/iTxbUfST4GrgFXAEeDLwAPALuAi4DngWttZwH2G5ojtVTRTLQYmgE92rCGKRZJ0JfBr4ABwshR/kWbdUPpuS/PEdzPpv61IuoRmYfsIzUDULtu3lb9vO4Hzgd8DH7X9zwXrG7YkKyIiIuKlMGzThREREREviSRZERERERUkyYqIiIioIElWRERERAVJsiIiIiIqSJIVEX1B0glJYx2PW7pY93pJuadQRHTVyxd+S0TEWeEfZauLiIi+kJGsiOhrkiYk3S7pgKQnJb2+lK+X9MuyWe5jki4q5Wsk3S9pf3m8vVQ1IukuSeOSHi13e0bSZyUdLPXs7NHHjIg+lCQrIvrF8tOmC6/reO2vtt8MfJdmRweA7wA7bF8C3ANsK+XbgMdtvwW4HBgv5RuAO2y/ETgOfLCU3wJcVur5VK0PFxGDJ3d8j4i+IGna9jmzlE8AV9t+tmya+4LtV0s6Bqy1/e9SPml7laQp4MLOLTEkrQd2295QzrcCy2x/VdIjwDTNdkYP2J6u/FEjYkBkJCsiBoHnOD4TnfuQneB/a1bfC9xBM+q1V1LWskbEoiTJiohBcF3H8+/K8W+BTeX4epoNdQEeA24CkDQi6dy5KpX0MmCd7T3AVuBc4P9G0yIiZpP/yCKiXyyXNNZx/ojtmds4rJT0NM1o1OZS9hngh5K+AEwBHy/lW4Dtkj5BM2J1EzA5xzVHgB+XREzANtvHu/aJImKgZU1WRPS1siZro+1jvW5LRESnTBdGREREVJCRrIiIiIgKMpIVERERUUGSrIiIiIgKkmRFREREVJAkKyIiIqKCJFkRERERFfwXge+V/4Gui0UAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Testing the model- Best Model\n",
    "model.load_weights('/content/drive/MyDrive/27 CNN on CIFR/Model Output/model_7_Rev01.h5')\n",
    "score = model.evaluate(X_test, y_test, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "print(\"--\"*25)\n",
    "\n",
    "\n",
    "summarize_diagnostics(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kRXlHnzfOg1r"
   },
   "source": [
    "▶ **Loading Model from 100th Epoch for further Training:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x5qPntw4Og1r",
    "outputId": "fba8bb5d-c302-4b19-bbde-7af0b147d63a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.5568 - accuracy: 0.8081\n",
      "Epoch 00001: val_accuracy did not improve from 0.80440\n",
      "390/390 [==============================] - 59s 151ms/step - loss: 0.5568 - accuracy: 0.8081 - val_loss: 0.6223 - val_accuracy: 0.7954\n",
      "Epoch 2/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.5425 - accuracy: 0.8139\n",
      "Epoch 00002: val_accuracy did not improve from 0.80440\n",
      "390/390 [==============================] - 59s 150ms/step - loss: 0.5425 - accuracy: 0.8139 - val_loss: 0.6359 - val_accuracy: 0.7921\n",
      "Epoch 3/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.5372 - accuracy: 0.8150\n",
      "Epoch 00003: val_accuracy did not improve from 0.80440\n",
      "390/390 [==============================] - 58s 150ms/step - loss: 0.5372 - accuracy: 0.8150 - val_loss: 0.6458 - val_accuracy: 0.7857\n",
      "Epoch 4/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.5257 - accuracy: 0.8170\n",
      "Epoch 00004: val_accuracy did not improve from 0.80440\n",
      "390/390 [==============================] - 58s 149ms/step - loss: 0.5257 - accuracy: 0.8170 - val_loss: 0.6106 - val_accuracy: 0.8014\n",
      "Epoch 5/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.5275 - accuracy: 0.8183\n",
      "Epoch 00005: val_accuracy did not improve from 0.80440\n",
      "390/390 [==============================] - 59s 150ms/step - loss: 0.5275 - accuracy: 0.8183 - val_loss: 0.8104 - val_accuracy: 0.7517\n",
      "Epoch 6/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.5188 - accuracy: 0.8225\n",
      "Epoch 00006: val_accuracy did not improve from 0.80440\n",
      "390/390 [==============================] - 59s 151ms/step - loss: 0.5188 - accuracy: 0.8225 - val_loss: 0.6442 - val_accuracy: 0.7918\n",
      "Epoch 7/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.5165 - accuracy: 0.8242\n",
      "Epoch 00007: val_accuracy improved from 0.80440 to 0.81350, saving model to /content/drive/MyDrive/27 CNN on CIFR/Model Output/model_7_Rev01.h5\n",
      "390/390 [==============================] - 59s 152ms/step - loss: 0.5165 - accuracy: 0.8242 - val_loss: 0.5637 - val_accuracy: 0.8135\n",
      "Epoch 8/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.5072 - accuracy: 0.8263\n",
      "Epoch 00008: val_accuracy did not improve from 0.81350\n",
      "390/390 [==============================] - 59s 150ms/step - loss: 0.5072 - accuracy: 0.8263 - val_loss: 0.6781 - val_accuracy: 0.7847\n",
      "Epoch 9/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.5026 - accuracy: 0.8275\n",
      "Epoch 00009: val_accuracy did not improve from 0.81350\n",
      "390/390 [==============================] - 59s 150ms/step - loss: 0.5026 - accuracy: 0.8275 - val_loss: 0.6033 - val_accuracy: 0.8033\n",
      "Epoch 10/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4983 - accuracy: 0.8289\n",
      "Epoch 00010: val_accuracy did not improve from 0.81350\n",
      "390/390 [==============================] - 58s 150ms/step - loss: 0.4983 - accuracy: 0.8289 - val_loss: 0.6608 - val_accuracy: 0.7877\n",
      "Epoch 11/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4922 - accuracy: 0.8299\n",
      "Epoch 00011: val_accuracy did not improve from 0.81350\n",
      "390/390 [==============================] - 58s 150ms/step - loss: 0.4922 - accuracy: 0.8299 - val_loss: 0.6390 - val_accuracy: 0.7935\n",
      "Epoch 12/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4892 - accuracy: 0.8327\n",
      "Epoch 00012: val_accuracy did not improve from 0.81350\n",
      "390/390 [==============================] - 58s 149ms/step - loss: 0.4892 - accuracy: 0.8327 - val_loss: 0.7020 - val_accuracy: 0.7791\n",
      "Epoch 13/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4824 - accuracy: 0.8354\n",
      "Epoch 00013: val_accuracy did not improve from 0.81350\n",
      "390/390 [==============================] - 58s 149ms/step - loss: 0.4824 - accuracy: 0.8354 - val_loss: 0.5863 - val_accuracy: 0.8075\n",
      "Epoch 14/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4743 - accuracy: 0.8384\n",
      "Epoch 00014: val_accuracy improved from 0.81350 to 0.81600, saving model to /content/drive/MyDrive/27 CNN on CIFR/Model Output/model_7_Rev01.h5\n",
      "390/390 [==============================] - 59s 152ms/step - loss: 0.4743 - accuracy: 0.8384 - val_loss: 0.5677 - val_accuracy: 0.8160\n",
      "Epoch 15/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4753 - accuracy: 0.8364\n",
      "Epoch 00015: val_accuracy did not improve from 0.81600\n",
      "390/390 [==============================] - 58s 149ms/step - loss: 0.4753 - accuracy: 0.8364 - val_loss: 0.6107 - val_accuracy: 0.8028\n",
      "Epoch 16/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4636 - accuracy: 0.8409\n",
      "Epoch 00016: val_accuracy improved from 0.81600 to 0.81860, saving model to /content/drive/MyDrive/27 CNN on CIFR/Model Output/model_7_Rev01.h5\n",
      "390/390 [==============================] - 59s 152ms/step - loss: 0.4636 - accuracy: 0.8409 - val_loss: 0.5522 - val_accuracy: 0.8186\n",
      "Epoch 17/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4639 - accuracy: 0.8399\n",
      "Epoch 00017: val_accuracy did not improve from 0.81860\n",
      "390/390 [==============================] - 58s 150ms/step - loss: 0.4639 - accuracy: 0.8399 - val_loss: 0.6256 - val_accuracy: 0.7995\n",
      "Epoch 18/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4583 - accuracy: 0.8427\n",
      "Epoch 00018: val_accuracy did not improve from 0.81860\n",
      "390/390 [==============================] - 58s 149ms/step - loss: 0.4583 - accuracy: 0.8427 - val_loss: 0.5932 - val_accuracy: 0.8100\n",
      "Epoch 19/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4572 - accuracy: 0.8450\n",
      "Epoch 00019: val_accuracy did not improve from 0.81860\n",
      "390/390 [==============================] - 58s 149ms/step - loss: 0.4572 - accuracy: 0.8450 - val_loss: 0.6992 - val_accuracy: 0.7861\n",
      "Epoch 20/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4550 - accuracy: 0.8437\n",
      "Epoch 00020: val_accuracy did not improve from 0.81860\n",
      "390/390 [==============================] - 58s 149ms/step - loss: 0.4550 - accuracy: 0.8437 - val_loss: 0.6505 - val_accuracy: 0.7935\n",
      "Epoch 21/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4474 - accuracy: 0.8450\n",
      "Epoch 00021: val_accuracy improved from 0.81860 to 0.84400, saving model to /content/drive/MyDrive/27 CNN on CIFR/Model Output/model_7_Rev01.h5\n",
      "390/390 [==============================] - 59s 151ms/step - loss: 0.4474 - accuracy: 0.8450 - val_loss: 0.4762 - val_accuracy: 0.8440\n",
      "Epoch 22/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4426 - accuracy: 0.8476\n",
      "Epoch 00022: val_accuracy did not improve from 0.84400\n",
      "390/390 [==============================] - 58s 150ms/step - loss: 0.4426 - accuracy: 0.8476 - val_loss: 0.5812 - val_accuracy: 0.8140\n",
      "Epoch 23/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4350 - accuracy: 0.8499\n",
      "Epoch 00023: val_accuracy did not improve from 0.84400\n",
      "390/390 [==============================] - 58s 149ms/step - loss: 0.4350 - accuracy: 0.8499 - val_loss: 0.5185 - val_accuracy: 0.8280\n",
      "Epoch 24/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4342 - accuracy: 0.8503\n",
      "Epoch 00024: val_accuracy did not improve from 0.84400\n",
      "390/390 [==============================] - 59s 150ms/step - loss: 0.4342 - accuracy: 0.8503 - val_loss: 0.5355 - val_accuracy: 0.8227\n",
      "Epoch 25/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4333 - accuracy: 0.8499\n",
      "Epoch 00025: val_accuracy did not improve from 0.84400\n",
      "390/390 [==============================] - 59s 150ms/step - loss: 0.4333 - accuracy: 0.8499 - val_loss: 0.9520 - val_accuracy: 0.7374\n",
      "Epoch 26/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4227 - accuracy: 0.8544\n",
      "Epoch 00026: val_accuracy did not improve from 0.84400\n",
      "390/390 [==============================] - 59s 150ms/step - loss: 0.4227 - accuracy: 0.8544 - val_loss: 0.5033 - val_accuracy: 0.8425\n",
      "Epoch 27/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4248 - accuracy: 0.8557\n",
      "Epoch 00027: val_accuracy did not improve from 0.84400\n",
      "390/390 [==============================] - 58s 150ms/step - loss: 0.4248 - accuracy: 0.8557 - val_loss: 0.6383 - val_accuracy: 0.7968\n",
      "Epoch 28/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4230 - accuracy: 0.8555\n",
      "Epoch 00028: val_accuracy did not improve from 0.84400\n",
      "390/390 [==============================] - 59s 150ms/step - loss: 0.4230 - accuracy: 0.8555 - val_loss: 0.5170 - val_accuracy: 0.8315\n",
      "Epoch 29/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4236 - accuracy: 0.8566\n",
      "Epoch 00029: val_accuracy did not improve from 0.84400\n",
      "390/390 [==============================] - 59s 151ms/step - loss: 0.4236 - accuracy: 0.8566 - val_loss: 0.6148 - val_accuracy: 0.8075\n",
      "Epoch 30/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4151 - accuracy: 0.8593\n",
      "Epoch 00030: val_accuracy did not improve from 0.84400\n",
      "390/390 [==============================] - 59s 150ms/step - loss: 0.4151 - accuracy: 0.8593 - val_loss: 0.7307 - val_accuracy: 0.7778\n",
      "Epoch 31/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4155 - accuracy: 0.8585\n",
      "Epoch 00031: val_accuracy did not improve from 0.84400\n",
      "390/390 [==============================] - 59s 151ms/step - loss: 0.4155 - accuracy: 0.8585 - val_loss: 0.6505 - val_accuracy: 0.7997\n",
      "Epoch 32/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4131 - accuracy: 0.8595\n",
      "Epoch 00032: val_accuracy did not improve from 0.84400\n",
      "390/390 [==============================] - 59s 150ms/step - loss: 0.4131 - accuracy: 0.8595 - val_loss: 0.5093 - val_accuracy: 0.8368\n",
      "Epoch 33/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4094 - accuracy: 0.8595\n",
      "Epoch 00033: val_accuracy did not improve from 0.84400\n",
      "390/390 [==============================] - 59s 150ms/step - loss: 0.4094 - accuracy: 0.8595 - val_loss: 0.5817 - val_accuracy: 0.8165\n",
      "Epoch 34/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3996 - accuracy: 0.8644\n",
      "Epoch 00034: val_accuracy did not improve from 0.84400\n",
      "390/390 [==============================] - 59s 150ms/step - loss: 0.3996 - accuracy: 0.8644 - val_loss: 0.5753 - val_accuracy: 0.8239\n",
      "Epoch 35/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4028 - accuracy: 0.8611\n",
      "Epoch 00035: val_accuracy did not improve from 0.84400\n",
      "390/390 [==============================] - 58s 149ms/step - loss: 0.4028 - accuracy: 0.8611 - val_loss: 0.5219 - val_accuracy: 0.8345\n",
      "Epoch 36/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4010 - accuracy: 0.8614\n",
      "Epoch 00036: val_accuracy did not improve from 0.84400\n",
      "390/390 [==============================] - 58s 149ms/step - loss: 0.4010 - accuracy: 0.8614 - val_loss: 0.5489 - val_accuracy: 0.8245\n",
      "Epoch 37/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3949 - accuracy: 0.8630\n",
      "Epoch 00037: val_accuracy did not improve from 0.84400\n",
      "390/390 [==============================] - 59s 150ms/step - loss: 0.3949 - accuracy: 0.8630 - val_loss: 0.6574 - val_accuracy: 0.7968\n",
      "Epoch 38/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3930 - accuracy: 0.8645\n",
      "Epoch 00038: val_accuracy did not improve from 0.84400\n",
      "390/390 [==============================] - 59s 150ms/step - loss: 0.3930 - accuracy: 0.8645 - val_loss: 0.6616 - val_accuracy: 0.8027\n",
      "Epoch 39/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3953 - accuracy: 0.8649\n",
      "Epoch 00039: val_accuracy did not improve from 0.84400\n",
      "390/390 [==============================] - 59s 151ms/step - loss: 0.3953 - accuracy: 0.8649 - val_loss: 0.5605 - val_accuracy: 0.8268\n",
      "Epoch 40/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3893 - accuracy: 0.8671\n",
      "Epoch 00040: val_accuracy did not improve from 0.84400\n",
      "390/390 [==============================] - 59s 151ms/step - loss: 0.3893 - accuracy: 0.8671 - val_loss: 0.6029 - val_accuracy: 0.8138\n",
      "Epoch 41/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3843 - accuracy: 0.8669\n",
      "Epoch 00041: val_accuracy did not improve from 0.84400\n",
      "390/390 [==============================] - 59s 150ms/step - loss: 0.3843 - accuracy: 0.8669 - val_loss: 0.5074 - val_accuracy: 0.8401\n",
      "Epoch 42/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3828 - accuracy: 0.8706\n",
      "Epoch 00042: val_accuracy did not improve from 0.84400\n",
      "390/390 [==============================] - 59s 150ms/step - loss: 0.3828 - accuracy: 0.8706 - val_loss: 0.5885 - val_accuracy: 0.8113\n",
      "Epoch 43/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3796 - accuracy: 0.8709\n",
      "Epoch 00043: val_accuracy did not improve from 0.84400\n",
      "390/390 [==============================] - 59s 150ms/step - loss: 0.3796 - accuracy: 0.8709 - val_loss: 0.6383 - val_accuracy: 0.8102\n",
      "Epoch 44/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3794 - accuracy: 0.8702\n",
      "Epoch 00044: val_accuracy did not improve from 0.84400\n",
      "390/390 [==============================] - 58s 149ms/step - loss: 0.3794 - accuracy: 0.8702 - val_loss: 0.6054 - val_accuracy: 0.8165\n",
      "Epoch 45/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3759 - accuracy: 0.8720\n",
      "Epoch 00045: val_accuracy did not improve from 0.84400\n",
      "390/390 [==============================] - 59s 150ms/step - loss: 0.3759 - accuracy: 0.8720 - val_loss: 0.6385 - val_accuracy: 0.8083\n",
      "Epoch 46/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3746 - accuracy: 0.8733\n",
      "Epoch 00046: val_accuracy did not improve from 0.84400\n",
      "390/390 [==============================] - 59s 150ms/step - loss: 0.3746 - accuracy: 0.8733 - val_loss: 0.6052 - val_accuracy: 0.8187\n",
      "Epoch 47/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3712 - accuracy: 0.8744\n",
      "Epoch 00047: val_accuracy improved from 0.84400 to 0.85730, saving model to /content/drive/MyDrive/27 CNN on CIFR/Model Output/model_7_Rev01.h5\n",
      "390/390 [==============================] - 59s 152ms/step - loss: 0.3712 - accuracy: 0.8744 - val_loss: 0.4450 - val_accuracy: 0.8573\n",
      "Epoch 48/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3692 - accuracy: 0.8733\n",
      "Epoch 00048: val_accuracy did not improve from 0.85730\n",
      "390/390 [==============================] - 59s 150ms/step - loss: 0.3692 - accuracy: 0.8733 - val_loss: 0.5246 - val_accuracy: 0.8365\n",
      "Epoch 49/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3689 - accuracy: 0.8732\n",
      "Epoch 00049: val_accuracy did not improve from 0.85730\n",
      "390/390 [==============================] - 59s 150ms/step - loss: 0.3689 - accuracy: 0.8732 - val_loss: 0.5151 - val_accuracy: 0.8391\n",
      "Epoch 50/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3635 - accuracy: 0.8752\n",
      "Epoch 00050: val_accuracy did not improve from 0.85730\n",
      "390/390 [==============================] - 59s 150ms/step - loss: 0.3635 - accuracy: 0.8752 - val_loss: 0.5628 - val_accuracy: 0.8254\n",
      "Epoch 51/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3643 - accuracy: 0.8762\n",
      "Epoch 00051: val_accuracy did not improve from 0.85730\n",
      "390/390 [==============================] - 59s 151ms/step - loss: 0.3643 - accuracy: 0.8762 - val_loss: 0.5115 - val_accuracy: 0.8415\n",
      "Epoch 52/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3618 - accuracy: 0.8766\n",
      "Epoch 00052: val_accuracy did not improve from 0.85730\n",
      "390/390 [==============================] - 59s 150ms/step - loss: 0.3618 - accuracy: 0.8766 - val_loss: 0.6983 - val_accuracy: 0.8010\n",
      "Epoch 53/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3612 - accuracy: 0.8772\n",
      "Epoch 00053: val_accuracy did not improve from 0.85730\n",
      "390/390 [==============================] - 58s 150ms/step - loss: 0.3612 - accuracy: 0.8772 - val_loss: 0.4470 - val_accuracy: 0.8563\n",
      "Epoch 54/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3560 - accuracy: 0.8783\n",
      "Epoch 00054: val_accuracy improved from 0.85730 to 0.86240, saving model to /content/drive/MyDrive/27 CNN on CIFR/Model Output/model_7_Rev01.h5\n",
      "390/390 [==============================] - 60s 153ms/step - loss: 0.3560 - accuracy: 0.8783 - val_loss: 0.4339 - val_accuracy: 0.8624\n",
      "Epoch 55/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3573 - accuracy: 0.8780\n",
      "Epoch 00055: val_accuracy improved from 0.86240 to 0.86320, saving model to /content/drive/MyDrive/27 CNN on CIFR/Model Output/model_7_Rev01.h5\n",
      "390/390 [==============================] - 59s 152ms/step - loss: 0.3573 - accuracy: 0.8780 - val_loss: 0.4298 - val_accuracy: 0.8632\n",
      "Epoch 56/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3529 - accuracy: 0.8794\n",
      "Epoch 00056: val_accuracy did not improve from 0.86320\n",
      "390/390 [==============================] - 58s 150ms/step - loss: 0.3529 - accuracy: 0.8794 - val_loss: 0.6363 - val_accuracy: 0.8093\n",
      "Epoch 57/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3536 - accuracy: 0.8781\n",
      "Epoch 00057: val_accuracy did not improve from 0.86320\n",
      "390/390 [==============================] - 59s 150ms/step - loss: 0.3536 - accuracy: 0.8781 - val_loss: 0.4800 - val_accuracy: 0.8471\n",
      "Epoch 58/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3454 - accuracy: 0.8827\n",
      "Epoch 00058: val_accuracy did not improve from 0.86320\n",
      "390/390 [==============================] - 59s 150ms/step - loss: 0.3454 - accuracy: 0.8827 - val_loss: 0.6286 - val_accuracy: 0.8142\n",
      "Epoch 59/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3470 - accuracy: 0.8814\n",
      "Epoch 00059: val_accuracy did not improve from 0.86320\n",
      "390/390 [==============================] - 59s 150ms/step - loss: 0.3470 - accuracy: 0.8814 - val_loss: 0.4674 - val_accuracy: 0.8505\n",
      "Epoch 60/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3462 - accuracy: 0.8807\n",
      "Epoch 00060: val_accuracy did not improve from 0.86320\n",
      "390/390 [==============================] - 59s 151ms/step - loss: 0.3462 - accuracy: 0.8807 - val_loss: 0.5054 - val_accuracy: 0.8441\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.backend as k\n",
    "k.set_value(model.optimizer.lr, 0.01)\n",
    "\n",
    "callback_list = [checkpoint]\n",
    "model.load_weights('/content/drive/MyDrive/27 CNN on CIFR/Model Output/model_7_30Epoch_Rev01_depthwise.h5')\n",
    "epochs = 60\n",
    "history = model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),\\\n",
    "                    steps_per_epoch=X_train.shape[0] // batch_size,epochs=epochs,\\\n",
    "                    verbose=1,validation_data=(X_test,y_test),callbacks=callback_list)\n",
    "\n",
    "model.save_weights('/content/drive/MyDrive/27 CNN on CIFR/Model Output/model_7_90Epoch_Rev01_depthwise.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3aY1CZlOg1s"
   },
   "source": [
    "▶ **Training Model further from 115th Epoch :**\n",
    "<br>Let's try changing learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QNMIAHmNOg1s",
    "outputId": "57806106-2d38-4afc-ba22-9f86727b80eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3416 - accuracy: 0.8836\n",
      "Epoch 00001: val_accuracy improved from 0.86320 to 0.87060, saving model to /content/drive/MyDrive/27 CNN on CIFR/Model Output/model_7_Rev01.h5\n",
      "390/390 [==============================] - 59s 152ms/step - loss: 0.3416 - accuracy: 0.8836 - val_loss: 0.4056 - val_accuracy: 0.8706\n",
      "Epoch 2/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3406 - accuracy: 0.8837\n",
      "Epoch 00002: val_accuracy did not improve from 0.87060\n",
      "390/390 [==============================] - 59s 150ms/step - loss: 0.3406 - accuracy: 0.8837 - val_loss: 0.5601 - val_accuracy: 0.8282\n",
      "Epoch 3/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3434 - accuracy: 0.8824\n",
      "Epoch 00003: val_accuracy did not improve from 0.87060\n",
      "390/390 [==============================] - 59s 151ms/step - loss: 0.3434 - accuracy: 0.8824 - val_loss: 0.5448 - val_accuracy: 0.8301\n",
      "Epoch 4/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3381 - accuracy: 0.8855\n",
      "Epoch 00004: val_accuracy did not improve from 0.87060\n",
      "390/390 [==============================] - 59s 150ms/step - loss: 0.3381 - accuracy: 0.8855 - val_loss: 0.7139 - val_accuracy: 0.7955\n",
      "Epoch 5/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3340 - accuracy: 0.8862\n",
      "Epoch 00005: val_accuracy did not improve from 0.87060\n",
      "390/390 [==============================] - 59s 150ms/step - loss: 0.3340 - accuracy: 0.8862 - val_loss: 0.6740 - val_accuracy: 0.8042\n",
      "Epoch 6/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3317 - accuracy: 0.8865\n",
      "Epoch 00006: val_accuracy did not improve from 0.87060\n",
      "390/390 [==============================] - 58s 149ms/step - loss: 0.3317 - accuracy: 0.8865 - val_loss: 0.4712 - val_accuracy: 0.8500\n",
      "Epoch 7/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3351 - accuracy: 0.8869\n",
      "Epoch 00007: val_accuracy did not improve from 0.87060\n",
      "390/390 [==============================] - 58s 150ms/step - loss: 0.3351 - accuracy: 0.8869 - val_loss: 0.5234 - val_accuracy: 0.8430\n",
      "Epoch 8/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3270 - accuracy: 0.8893\n",
      "Epoch 00008: val_accuracy did not improve from 0.87060\n",
      "390/390 [==============================] - 58s 150ms/step - loss: 0.3270 - accuracy: 0.8893 - val_loss: 0.5646 - val_accuracy: 0.8264\n",
      "Epoch 9/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3336 - accuracy: 0.8869\n",
      "Epoch 00009: val_accuracy did not improve from 0.87060\n",
      "390/390 [==============================] - 59s 150ms/step - loss: 0.3336 - accuracy: 0.8869 - val_loss: 0.4422 - val_accuracy: 0.8629\n",
      "Epoch 10/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3280 - accuracy: 0.8880\n",
      "Epoch 00010: val_accuracy did not improve from 0.87060\n",
      "390/390 [==============================] - 59s 150ms/step - loss: 0.3280 - accuracy: 0.8880 - val_loss: 0.5410 - val_accuracy: 0.8325\n",
      "Epoch 11/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3310 - accuracy: 0.8864\n",
      "Epoch 00011: val_accuracy did not improve from 0.87060\n",
      "390/390 [==============================] - 58s 150ms/step - loss: 0.3310 - accuracy: 0.8864 - val_loss: 0.4628 - val_accuracy: 0.8569\n",
      "Epoch 12/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3238 - accuracy: 0.8888\n",
      "Epoch 00012: val_accuracy did not improve from 0.87060\n",
      "390/390 [==============================] - 58s 150ms/step - loss: 0.3238 - accuracy: 0.8888 - val_loss: 0.4227 - val_accuracy: 0.8636\n",
      "Epoch 13/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3250 - accuracy: 0.8889\n",
      "Epoch 00013: val_accuracy did not improve from 0.87060\n",
      "390/390 [==============================] - 59s 150ms/step - loss: 0.3250 - accuracy: 0.8889 - val_loss: 0.4435 - val_accuracy: 0.8623\n",
      "Epoch 14/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3260 - accuracy: 0.8896\n",
      "Epoch 00014: val_accuracy did not improve from 0.87060\n",
      "390/390 [==============================] - 59s 151ms/step - loss: 0.3260 - accuracy: 0.8896 - val_loss: 0.4894 - val_accuracy: 0.8495\n",
      "Epoch 15/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3193 - accuracy: 0.8900\n",
      "Epoch 00015: val_accuracy did not improve from 0.87060\n",
      "390/390 [==============================] - 59s 150ms/step - loss: 0.3193 - accuracy: 0.8900 - val_loss: 0.4393 - val_accuracy: 0.8645\n",
      "Epoch 16/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3250 - accuracy: 0.8906\n",
      "Epoch 00016: val_accuracy did not improve from 0.87060\n",
      "390/390 [==============================] - 59s 150ms/step - loss: 0.3250 - accuracy: 0.8906 - val_loss: 0.5289 - val_accuracy: 0.8381\n",
      "Epoch 17/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3221 - accuracy: 0.8903\n",
      "Epoch 00017: val_accuracy did not improve from 0.87060\n",
      "390/390 [==============================] - 59s 150ms/step - loss: 0.3221 - accuracy: 0.8903 - val_loss: 0.4560 - val_accuracy: 0.8558\n",
      "Epoch 18/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3147 - accuracy: 0.8926\n",
      "Epoch 00018: val_accuracy did not improve from 0.87060\n",
      "390/390 [==============================] - 59s 150ms/step - loss: 0.3147 - accuracy: 0.8926 - val_loss: 0.4962 - val_accuracy: 0.8429\n",
      "Epoch 19/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3154 - accuracy: 0.8917\n",
      "Epoch 00019: val_accuracy did not improve from 0.87060\n",
      "390/390 [==============================] - 59s 150ms/step - loss: 0.3154 - accuracy: 0.8917 - val_loss: 0.4406 - val_accuracy: 0.8654\n",
      "Epoch 20/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3136 - accuracy: 0.8931\n",
      "Epoch 00020: val_accuracy did not improve from 0.87060\n",
      "390/390 [==============================] - 59s 150ms/step - loss: 0.3136 - accuracy: 0.8931 - val_loss: 0.4639 - val_accuracy: 0.8550\n",
      "Epoch 21/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3162 - accuracy: 0.8922\n",
      "Epoch 00021: val_accuracy did not improve from 0.87060\n",
      "390/390 [==============================] - 59s 151ms/step - loss: 0.3162 - accuracy: 0.8922 - val_loss: 0.4990 - val_accuracy: 0.8499\n",
      "Epoch 22/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3109 - accuracy: 0.8937\n",
      "Epoch 00022: val_accuracy did not improve from 0.87060\n",
      "390/390 [==============================] - 59s 151ms/step - loss: 0.3109 - accuracy: 0.8937 - val_loss: 0.4361 - val_accuracy: 0.8627\n",
      "Epoch 23/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3127 - accuracy: 0.8935\n",
      "Epoch 00023: val_accuracy did not improve from 0.87060\n",
      "390/390 [==============================] - 59s 151ms/step - loss: 0.3127 - accuracy: 0.8935 - val_loss: 0.5325 - val_accuracy: 0.8419\n",
      "Epoch 24/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3052 - accuracy: 0.8967\n",
      "Epoch 00024: val_accuracy did not improve from 0.87060\n",
      "390/390 [==============================] - 59s 151ms/step - loss: 0.3052 - accuracy: 0.8967 - val_loss: 0.4713 - val_accuracy: 0.8578\n",
      "Epoch 25/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3072 - accuracy: 0.8955\n",
      "Epoch 00025: val_accuracy did not improve from 0.87060\n",
      "390/390 [==============================] - 59s 151ms/step - loss: 0.3072 - accuracy: 0.8955 - val_loss: 0.4395 - val_accuracy: 0.8610\n",
      "Epoch 26/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3066 - accuracy: 0.8944\n",
      "Epoch 00026: val_accuracy did not improve from 0.87060\n",
      "390/390 [==============================] - 58s 150ms/step - loss: 0.3066 - accuracy: 0.8944 - val_loss: 0.4428 - val_accuracy: 0.8657\n",
      "Epoch 27/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3031 - accuracy: 0.8966\n",
      "Epoch 00027: val_accuracy did not improve from 0.87060\n",
      "390/390 [==============================] - 59s 150ms/step - loss: 0.3031 - accuracy: 0.8966 - val_loss: 0.4589 - val_accuracy: 0.8577\n",
      "Epoch 28/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3031 - accuracy: 0.8975\n",
      "Epoch 00028: val_accuracy did not improve from 0.87060\n",
      "390/390 [==============================] - 59s 151ms/step - loss: 0.3031 - accuracy: 0.8975 - val_loss: 0.4749 - val_accuracy: 0.8558\n",
      "Epoch 29/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3041 - accuracy: 0.8957\n",
      "Epoch 00029: val_accuracy did not improve from 0.87060\n",
      "390/390 [==============================] - 59s 151ms/step - loss: 0.3041 - accuracy: 0.8957 - val_loss: 0.4732 - val_accuracy: 0.8511\n",
      "Epoch 30/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3037 - accuracy: 0.8957\n",
      "Epoch 00030: val_accuracy did not improve from 0.87060\n",
      "390/390 [==============================] - 58s 149ms/step - loss: 0.3037 - accuracy: 0.8957 - val_loss: 0.5053 - val_accuracy: 0.8465\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.backend as k\n",
    "k.set_value(model.optimizer.lr, 0.01)\n",
    "callback_list = [checkpoint]\n",
    "model.load_weights('/content/drive/MyDrive/27 CNN on CIFR/Model Output/model_7_90Epoch_Rev01_depthwise.h5')\n",
    "\n",
    "epochs = 30\n",
    "history = model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),\\\n",
    "                    steps_per_epoch=X_train.shape[0] // batch_size,epochs=epochs,\\\n",
    "                    verbose=1,validation_data=(X_test,y_test),callbacks=callback_list)\n",
    "\n",
    "model.save_weights('/content/drive/MyDrive/27 CNN on CIFR/Model Output/model_7_120Epoch_Rev01_depthwise.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lIa_NHACOg1s",
    "outputId": "9a6913d8-0da0-47b0-b8d3-59840541df1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2677 - accuracy: 0.9087\n",
      "Epoch 00001: val_accuracy improved from 0.87060 to 0.88380, saving model to /content/drive/MyDrive/27 CNN on CIFR/Model Output/model_7_Rev01.h5\n",
      "390/390 [==============================] - 59s 152ms/step - loss: 0.2677 - accuracy: 0.9087 - val_loss: 0.3748 - val_accuracy: 0.8838\n",
      "Epoch 2/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2577 - accuracy: 0.9133\n",
      "Epoch 00002: val_accuracy did not improve from 0.88380\n",
      "390/390 [==============================] - 58s 150ms/step - loss: 0.2577 - accuracy: 0.9133 - val_loss: 0.3791 - val_accuracy: 0.8824\n",
      "Epoch 3/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2566 - accuracy: 0.9140\n",
      "Epoch 00003: val_accuracy improved from 0.88380 to 0.88520, saving model to /content/drive/MyDrive/27 CNN on CIFR/Model Output/model_7_Rev01.h5\n",
      "390/390 [==============================] - 59s 152ms/step - loss: 0.2566 - accuracy: 0.9140 - val_loss: 0.3706 - val_accuracy: 0.8852\n",
      "Epoch 4/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2502 - accuracy: 0.9164\n",
      "Epoch 00004: val_accuracy did not improve from 0.88520\n",
      "390/390 [==============================] - 59s 150ms/step - loss: 0.2502 - accuracy: 0.9164 - val_loss: 0.3929 - val_accuracy: 0.8810\n",
      "Epoch 5/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2522 - accuracy: 0.9145\n",
      "Epoch 00005: val_accuracy improved from 0.88520 to 0.88790, saving model to /content/drive/MyDrive/27 CNN on CIFR/Model Output/model_7_Rev01.h5\n",
      "390/390 [==============================] - 60s 153ms/step - loss: 0.2522 - accuracy: 0.9145 - val_loss: 0.3698 - val_accuracy: 0.8879\n",
      "Epoch 6/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2466 - accuracy: 0.9171\n",
      "Epoch 00006: val_accuracy did not improve from 0.88790\n",
      "390/390 [==============================] - 59s 151ms/step - loss: 0.2466 - accuracy: 0.9171 - val_loss: 0.3780 - val_accuracy: 0.8846\n",
      "Epoch 7/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2494 - accuracy: 0.9152\n",
      "Epoch 00007: val_accuracy did not improve from 0.88790\n",
      "390/390 [==============================] - 59s 150ms/step - loss: 0.2494 - accuracy: 0.9152 - val_loss: 0.3777 - val_accuracy: 0.8841\n",
      "Epoch 8/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2436 - accuracy: 0.9174\n",
      "Epoch 00008: val_accuracy did not improve from 0.88790\n",
      "390/390 [==============================] - 59s 150ms/step - loss: 0.2436 - accuracy: 0.9174 - val_loss: 0.3704 - val_accuracy: 0.8865\n",
      "Epoch 9/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2434 - accuracy: 0.9168\n",
      "Epoch 00009: val_accuracy did not improve from 0.88790\n",
      "390/390 [==============================] - 59s 150ms/step - loss: 0.2434 - accuracy: 0.9168 - val_loss: 0.3737 - val_accuracy: 0.8858\n",
      "Epoch 10/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2486 - accuracy: 0.9155\n",
      "Epoch 00010: val_accuracy improved from 0.88790 to 0.88850, saving model to /content/drive/MyDrive/27 CNN on CIFR/Model Output/model_7_Rev01.h5\n",
      "390/390 [==============================] - 59s 152ms/step - loss: 0.2486 - accuracy: 0.9155 - val_loss: 0.3629 - val_accuracy: 0.8885\n",
      "Epoch 11/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2455 - accuracy: 0.9173\n",
      "Epoch 00011: val_accuracy did not improve from 0.88850\n",
      "390/390 [==============================] - 58s 149ms/step - loss: 0.2455 - accuracy: 0.9173 - val_loss: 0.3701 - val_accuracy: 0.8878\n",
      "Epoch 12/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2475 - accuracy: 0.9154\n",
      "Epoch 00012: val_accuracy did not improve from 0.88850\n",
      "390/390 [==============================] - 59s 151ms/step - loss: 0.2475 - accuracy: 0.9154 - val_loss: 0.3898 - val_accuracy: 0.8822\n",
      "Epoch 13/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2416 - accuracy: 0.9189\n",
      "Epoch 00013: val_accuracy did not improve from 0.88850\n",
      "390/390 [==============================] - 59s 150ms/step - loss: 0.2416 - accuracy: 0.9189 - val_loss: 0.3672 - val_accuracy: 0.8881\n",
      "Epoch 14/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2425 - accuracy: 0.9181\n",
      "Epoch 00014: val_accuracy did not improve from 0.88850\n",
      "390/390 [==============================] - 58s 149ms/step - loss: 0.2425 - accuracy: 0.9181 - val_loss: 0.3725 - val_accuracy: 0.8857\n",
      "Epoch 15/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2441 - accuracy: 0.9185\n",
      "Epoch 00015: val_accuracy did not improve from 0.88850\n",
      "390/390 [==============================] - 58s 150ms/step - loss: 0.2441 - accuracy: 0.9185 - val_loss: 0.3824 - val_accuracy: 0.8851\n",
      "Epoch 16/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2400 - accuracy: 0.9167\n",
      "Epoch 00016: val_accuracy did not improve from 0.88850\n",
      "390/390 [==============================] - 58s 150ms/step - loss: 0.2400 - accuracy: 0.9167 - val_loss: 0.3771 - val_accuracy: 0.8867\n",
      "Epoch 17/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2428 - accuracy: 0.9184\n",
      "Epoch 00017: val_accuracy improved from 0.88850 to 0.88940, saving model to /content/drive/MyDrive/27 CNN on CIFR/Model Output/model_7_Rev01.h5\n",
      "390/390 [==============================] - 59s 152ms/step - loss: 0.2428 - accuracy: 0.9184 - val_loss: 0.3664 - val_accuracy: 0.8894\n",
      "Epoch 18/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2400 - accuracy: 0.9186\n",
      "Epoch 00018: val_accuracy did not improve from 0.88940\n",
      "390/390 [==============================] - 59s 151ms/step - loss: 0.2400 - accuracy: 0.9186 - val_loss: 0.3826 - val_accuracy: 0.8845\n",
      "Epoch 19/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2409 - accuracy: 0.9190\n",
      "Epoch 00019: val_accuracy improved from 0.88940 to 0.89020, saving model to /content/drive/MyDrive/27 CNN on CIFR/Model Output/model_7_Rev01.h5\n",
      "390/390 [==============================] - 60s 153ms/step - loss: 0.2409 - accuracy: 0.9190 - val_loss: 0.3640 - val_accuracy: 0.8902\n",
      "Epoch 20/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2389 - accuracy: 0.9191\n",
      "Epoch 00020: val_accuracy did not improve from 0.89020\n",
      "390/390 [==============================] - 58s 149ms/step - loss: 0.2389 - accuracy: 0.9191 - val_loss: 0.3827 - val_accuracy: 0.8843\n",
      "Epoch 21/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2375 - accuracy: 0.9195\n",
      "Epoch 00021: val_accuracy did not improve from 0.89020\n",
      "390/390 [==============================] - 59s 150ms/step - loss: 0.2375 - accuracy: 0.9195 - val_loss: 0.3655 - val_accuracy: 0.8891\n",
      "Epoch 22/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2381 - accuracy: 0.9173\n",
      "Epoch 00022: val_accuracy did not improve from 0.89020\n",
      "390/390 [==============================] - 58s 150ms/step - loss: 0.2381 - accuracy: 0.9173 - val_loss: 0.3804 - val_accuracy: 0.8846\n",
      "Epoch 23/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2355 - accuracy: 0.9196\n",
      "Epoch 00023: val_accuracy did not improve from 0.89020\n",
      "390/390 [==============================] - 59s 150ms/step - loss: 0.2355 - accuracy: 0.9196 - val_loss: 0.3986 - val_accuracy: 0.8807\n",
      "Epoch 24/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2370 - accuracy: 0.9196\n",
      "Epoch 00024: val_accuracy did not improve from 0.89020\n",
      "390/390 [==============================] - 58s 150ms/step - loss: 0.2370 - accuracy: 0.9196 - val_loss: 0.3743 - val_accuracy: 0.8848\n",
      "Epoch 25/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2364 - accuracy: 0.9197\n",
      "Epoch 00025: val_accuracy did not improve from 0.89020\n",
      "390/390 [==============================] - 59s 150ms/step - loss: 0.2364 - accuracy: 0.9197 - val_loss: 0.3767 - val_accuracy: 0.8840\n",
      "Epoch 26/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2365 - accuracy: 0.9190\n",
      "Epoch 00026: val_accuracy did not improve from 0.89020\n",
      "390/390 [==============================] - 59s 150ms/step - loss: 0.2365 - accuracy: 0.9190 - val_loss: 0.3861 - val_accuracy: 0.8873\n",
      "Epoch 27/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2339 - accuracy: 0.9211\n",
      "Epoch 00027: val_accuracy did not improve from 0.89020\n",
      "390/390 [==============================] - 59s 150ms/step - loss: 0.2339 - accuracy: 0.9211 - val_loss: 0.3710 - val_accuracy: 0.8881\n",
      "Epoch 28/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2332 - accuracy: 0.9214\n",
      "Epoch 00028: val_accuracy did not improve from 0.89020\n",
      "390/390 [==============================] - 59s 151ms/step - loss: 0.2332 - accuracy: 0.9214 - val_loss: 0.3917 - val_accuracy: 0.8830\n",
      "Epoch 29/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2339 - accuracy: 0.9213\n",
      "Epoch 00029: val_accuracy did not improve from 0.89020\n",
      "390/390 [==============================] - 59s 150ms/step - loss: 0.2339 - accuracy: 0.9213 - val_loss: 0.3724 - val_accuracy: 0.8874\n",
      "Epoch 30/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2349 - accuracy: 0.9205\n",
      "Epoch 00030: val_accuracy improved from 0.89020 to 0.89200, saving model to /content/drive/MyDrive/27 CNN on CIFR/Model Output/model_7_Rev01.h5\n",
      "390/390 [==============================] - 59s 152ms/step - loss: 0.2349 - accuracy: 0.9205 - val_loss: 0.3615 - val_accuracy: 0.8920\n",
      "Epoch 31/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2349 - accuracy: 0.9196\n",
      "Epoch 00031: val_accuracy did not improve from 0.89200\n",
      "390/390 [==============================] - 59s 150ms/step - loss: 0.2349 - accuracy: 0.9196 - val_loss: 0.3892 - val_accuracy: 0.8837\n",
      "Epoch 32/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2378 - accuracy: 0.9202\n",
      "Epoch 00032: val_accuracy did not improve from 0.89200\n",
      "390/390 [==============================] - 59s 151ms/step - loss: 0.2378 - accuracy: 0.9202 - val_loss: 0.3728 - val_accuracy: 0.8889\n",
      "Epoch 33/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2343 - accuracy: 0.9209\n",
      "Epoch 00033: val_accuracy did not improve from 0.89200\n",
      "390/390 [==============================] - 59s 151ms/step - loss: 0.2343 - accuracy: 0.9209 - val_loss: 0.3783 - val_accuracy: 0.8872\n",
      "Epoch 34/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2362 - accuracy: 0.9196\n",
      "Epoch 00034: val_accuracy did not improve from 0.89200\n",
      "390/390 [==============================] - 59s 150ms/step - loss: 0.2362 - accuracy: 0.9196 - val_loss: 0.3846 - val_accuracy: 0.8867\n",
      "Epoch 35/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2371 - accuracy: 0.9205\n",
      "Epoch 00035: val_accuracy did not improve from 0.89200\n",
      "390/390 [==============================] - 59s 150ms/step - loss: 0.2371 - accuracy: 0.9205 - val_loss: 0.3786 - val_accuracy: 0.8854\n",
      "Epoch 36/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2332 - accuracy: 0.9214\n",
      "Epoch 00036: val_accuracy did not improve from 0.89200\n",
      "390/390 [==============================] - 58s 150ms/step - loss: 0.2332 - accuracy: 0.9214 - val_loss: 0.3615 - val_accuracy: 0.8908\n",
      "Epoch 37/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2330 - accuracy: 0.9197\n",
      "Epoch 00037: val_accuracy did not improve from 0.89200\n",
      "390/390 [==============================] - 59s 150ms/step - loss: 0.2330 - accuracy: 0.9197 - val_loss: 0.3931 - val_accuracy: 0.8834\n",
      "Epoch 38/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2358 - accuracy: 0.9206\n",
      "Epoch 00038: val_accuracy did not improve from 0.89200\n",
      "390/390 [==============================] - 58s 150ms/step - loss: 0.2358 - accuracy: 0.9206 - val_loss: 0.3619 - val_accuracy: 0.8892\n",
      "Epoch 39/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2309 - accuracy: 0.9216\n",
      "Epoch 00039: val_accuracy did not improve from 0.89200\n",
      "390/390 [==============================] - 59s 151ms/step - loss: 0.2309 - accuracy: 0.9216 - val_loss: 0.3745 - val_accuracy: 0.8882\n",
      "Epoch 40/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2324 - accuracy: 0.9220\n",
      "Epoch 00040: val_accuracy did not improve from 0.89200\n",
      "390/390 [==============================] - 59s 150ms/step - loss: 0.2324 - accuracy: 0.9220 - val_loss: 0.3897 - val_accuracy: 0.8834\n",
      "Epoch 41/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2362 - accuracy: 0.9194\n",
      "Epoch 00041: val_accuracy did not improve from 0.89200\n",
      "390/390 [==============================] - 59s 150ms/step - loss: 0.2362 - accuracy: 0.9194 - val_loss: 0.3720 - val_accuracy: 0.8903\n",
      "Epoch 42/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2313 - accuracy: 0.9217\n",
      "Epoch 00042: val_accuracy did not improve from 0.89200\n",
      "390/390 [==============================] - 59s 150ms/step - loss: 0.2313 - accuracy: 0.9217 - val_loss: 0.3786 - val_accuracy: 0.8885\n",
      "Epoch 43/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2337 - accuracy: 0.9202\n",
      "Epoch 00043: val_accuracy did not improve from 0.89200\n",
      "390/390 [==============================] - 59s 150ms/step - loss: 0.2337 - accuracy: 0.9202 - val_loss: 0.3915 - val_accuracy: 0.8860\n",
      "Epoch 44/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2335 - accuracy: 0.9213\n",
      "Epoch 00044: val_accuracy did not improve from 0.89200\n",
      "390/390 [==============================] - 59s 151ms/step - loss: 0.2335 - accuracy: 0.9213 - val_loss: 0.3730 - val_accuracy: 0.8894\n",
      "Epoch 45/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2267 - accuracy: 0.9235\n",
      "Epoch 00045: val_accuracy did not improve from 0.89200\n",
      "390/390 [==============================] - 59s 151ms/step - loss: 0.2267 - accuracy: 0.9235 - val_loss: 0.3811 - val_accuracy: 0.8840\n",
      "Epoch 46/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2247 - accuracy: 0.9235\n",
      "Epoch 00046: val_accuracy did not improve from 0.89200\n",
      "390/390 [==============================] - 59s 151ms/step - loss: 0.2247 - accuracy: 0.9235 - val_loss: 0.3786 - val_accuracy: 0.8857\n",
      "Epoch 47/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2307 - accuracy: 0.9226\n",
      "Epoch 00047: val_accuracy did not improve from 0.89200\n",
      "390/390 [==============================] - 59s 150ms/step - loss: 0.2307 - accuracy: 0.9226 - val_loss: 0.3953 - val_accuracy: 0.8815\n",
      "Epoch 48/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2297 - accuracy: 0.9209\n",
      "Epoch 00048: val_accuracy did not improve from 0.89200\n",
      "390/390 [==============================] - 59s 151ms/step - loss: 0.2297 - accuracy: 0.9209 - val_loss: 0.4200 - val_accuracy: 0.8750\n",
      "Epoch 49/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2310 - accuracy: 0.9216\n",
      "Epoch 00049: val_accuracy did not improve from 0.89200\n",
      "390/390 [==============================] - 59s 151ms/step - loss: 0.2310 - accuracy: 0.9216 - val_loss: 0.3679 - val_accuracy: 0.8879\n",
      "Epoch 50/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2305 - accuracy: 0.9224\n",
      "Epoch 00050: val_accuracy did not improve from 0.89200\n",
      "390/390 [==============================] - 59s 150ms/step - loss: 0.2305 - accuracy: 0.9224 - val_loss: 0.3833 - val_accuracy: 0.8857\n",
      "Epoch 51/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2322 - accuracy: 0.9211\n",
      "Epoch 00051: val_accuracy did not improve from 0.89200\n",
      "390/390 [==============================] - 59s 150ms/step - loss: 0.2322 - accuracy: 0.9211 - val_loss: 0.3803 - val_accuracy: 0.8855\n",
      "Epoch 52/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2303 - accuracy: 0.9218\n",
      "Epoch 00052: val_accuracy did not improve from 0.89200\n",
      "390/390 [==============================] - 59s 150ms/step - loss: 0.2303 - accuracy: 0.9218 - val_loss: 0.3721 - val_accuracy: 0.8900\n",
      "Epoch 53/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2295 - accuracy: 0.9222\n",
      "Epoch 00053: val_accuracy did not improve from 0.89200\n",
      "390/390 [==============================] - 58s 150ms/step - loss: 0.2295 - accuracy: 0.9222 - val_loss: 0.3829 - val_accuracy: 0.8865\n",
      "Epoch 54/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2253 - accuracy: 0.9242\n",
      "Epoch 00054: val_accuracy did not improve from 0.89200\n",
      "390/390 [==============================] - 59s 151ms/step - loss: 0.2253 - accuracy: 0.9242 - val_loss: 0.4043 - val_accuracy: 0.8786\n",
      "Epoch 55/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2279 - accuracy: 0.9226\n",
      "Epoch 00055: val_accuracy did not improve from 0.89200\n",
      "390/390 [==============================] - 59s 150ms/step - loss: 0.2279 - accuracy: 0.9226 - val_loss: 0.3741 - val_accuracy: 0.8889\n",
      "Epoch 56/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2280 - accuracy: 0.9217\n",
      "Epoch 00056: val_accuracy did not improve from 0.89200\n",
      "390/390 [==============================] - 58s 150ms/step - loss: 0.2280 - accuracy: 0.9217 - val_loss: 0.3704 - val_accuracy: 0.8902\n",
      "Epoch 57/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2306 - accuracy: 0.9212\n",
      "Epoch 00057: val_accuracy did not improve from 0.89200\n",
      "390/390 [==============================] - 58s 149ms/step - loss: 0.2306 - accuracy: 0.9212 - val_loss: 0.3683 - val_accuracy: 0.8890\n",
      "Epoch 58/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2259 - accuracy: 0.9238\n",
      "Epoch 00058: val_accuracy did not improve from 0.89200\n",
      "390/390 [==============================] - 58s 150ms/step - loss: 0.2259 - accuracy: 0.9238 - val_loss: 0.3767 - val_accuracy: 0.8894\n",
      "Epoch 59/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2264 - accuracy: 0.9230\n",
      "Epoch 00059: val_accuracy did not improve from 0.89200\n",
      "390/390 [==============================] - 59s 150ms/step - loss: 0.2264 - accuracy: 0.9230 - val_loss: 0.3839 - val_accuracy: 0.8880\n",
      "Epoch 60/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2282 - accuracy: 0.9223\n",
      "Epoch 00060: val_accuracy did not improve from 0.89200\n",
      "390/390 [==============================] - 58s 149ms/step - loss: 0.2282 - accuracy: 0.9223 - val_loss: 0.3802 - val_accuracy: 0.8870\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.backend as k\n",
    "k.set_value(model.optimizer.lr, 0.001)\n",
    "\n",
    "model.load_weights('/content/drive/MyDrive/27 CNN on CIFR/Model Output/model_7_120Epoch_Rev01_depthwise.h5')\n",
    "\n",
    "epochs = 60\n",
    "history = model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),\\\n",
    "                    steps_per_epoch=X_train.shape[0] // batch_size,epochs=epochs,\\\n",
    "                    verbose=1,validation_data=(X_test,y_test),callbacks=callback_list)\n",
    "\n",
    "model.save_weights('/content/drive/MyDrive/27 CNN on CIFR/Model Output/model_7_180Epoch_Rev01_depthwise.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "933OzNBEOg1t",
    "outputId": "2ff5a669-2f73-4b36-c584-59c16a51e4c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2227 - accuracy: 0.9241\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.88980, saving model to /content/drive/MyDrive/27 CNN on CIFR/Model Output/model_7_Rev01_depthwise.h5\n",
      "390/390 [==============================] - 68s 156ms/step - loss: 0.2227 - accuracy: 0.9241 - val_loss: 0.3760 - val_accuracy: 0.8898\n",
      "Epoch 2/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2249 - accuracy: 0.9241\n",
      "Epoch 00002: val_accuracy improved from 0.88980 to 0.89020, saving model to /content/drive/MyDrive/27 CNN on CIFR/Model Output/model_7_Rev01_depthwise.h5\n",
      "390/390 [==============================] - 59s 152ms/step - loss: 0.2249 - accuracy: 0.9241 - val_loss: 0.3734 - val_accuracy: 0.8902\n",
      "Epoch 3/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2256 - accuracy: 0.9234\n",
      "Epoch 00003: val_accuracy did not improve from 0.89020\n",
      "390/390 [==============================] - 58s 149ms/step - loss: 0.2256 - accuracy: 0.9234 - val_loss: 0.3746 - val_accuracy: 0.8900\n",
      "Epoch 4/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2218 - accuracy: 0.9252\n",
      "Epoch 00004: val_accuracy improved from 0.89020 to 0.89070, saving model to /content/drive/MyDrive/27 CNN on CIFR/Model Output/model_7_Rev01_depthwise.h5\n",
      "390/390 [==============================] - 59s 152ms/step - loss: 0.2218 - accuracy: 0.9252 - val_loss: 0.3735 - val_accuracy: 0.8907\n",
      "Epoch 5/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2209 - accuracy: 0.9249\n",
      "Epoch 00005: val_accuracy improved from 0.89070 to 0.89100, saving model to /content/drive/MyDrive/27 CNN on CIFR/Model Output/model_7_Rev01_depthwise.h5\n",
      "390/390 [==============================] - 59s 152ms/step - loss: 0.2209 - accuracy: 0.9249 - val_loss: 0.3701 - val_accuracy: 0.8910\n",
      "Epoch 6/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2253 - accuracy: 0.9234\n",
      "Epoch 00006: val_accuracy did not improve from 0.89100\n",
      "390/390 [==============================] - 58s 149ms/step - loss: 0.2253 - accuracy: 0.9234 - val_loss: 0.3725 - val_accuracy: 0.8905\n",
      "Epoch 7/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2193 - accuracy: 0.9271\n",
      "Epoch 00007: val_accuracy did not improve from 0.89100\n",
      "390/390 [==============================] - 58s 149ms/step - loss: 0.2193 - accuracy: 0.9271 - val_loss: 0.3728 - val_accuracy: 0.8903\n",
      "Epoch 8/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2223 - accuracy: 0.9253\n",
      "Epoch 00008: val_accuracy did not improve from 0.89100\n",
      "390/390 [==============================] - 58s 150ms/step - loss: 0.2223 - accuracy: 0.9253 - val_loss: 0.3729 - val_accuracy: 0.8899\n",
      "Epoch 9/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2236 - accuracy: 0.9234\n",
      "Epoch 00009: val_accuracy improved from 0.89100 to 0.89120, saving model to /content/drive/MyDrive/27 CNN on CIFR/Model Output/model_7_Rev01_depthwise.h5\n",
      "390/390 [==============================] - 59s 151ms/step - loss: 0.2236 - accuracy: 0.9234 - val_loss: 0.3708 - val_accuracy: 0.8912\n",
      "Epoch 10/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2240 - accuracy: 0.9248\n",
      "Epoch 00010: val_accuracy did not improve from 0.89120\n",
      "390/390 [==============================] - 58s 149ms/step - loss: 0.2240 - accuracy: 0.9248 - val_loss: 0.3726 - val_accuracy: 0.8907\n",
      "Epoch 11/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2263 - accuracy: 0.9232\n",
      "Epoch 00011: val_accuracy improved from 0.89120 to 0.89130, saving model to /content/drive/MyDrive/27 CNN on CIFR/Model Output/model_7_Rev01_depthwise.h5\n",
      "390/390 [==============================] - 59s 151ms/step - loss: 0.2263 - accuracy: 0.9232 - val_loss: 0.3706 - val_accuracy: 0.8913\n",
      "Epoch 12/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2197 - accuracy: 0.9256\n",
      "Epoch 00012: val_accuracy did not improve from 0.89130\n",
      "390/390 [==============================] - 58s 149ms/step - loss: 0.2197 - accuracy: 0.9256 - val_loss: 0.3698 - val_accuracy: 0.8912\n",
      "Epoch 13/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2221 - accuracy: 0.9242\n",
      "Epoch 00013: val_accuracy did not improve from 0.89130\n",
      "390/390 [==============================] - 58s 149ms/step - loss: 0.2221 - accuracy: 0.9242 - val_loss: 0.3735 - val_accuracy: 0.8900\n",
      "Epoch 14/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2245 - accuracy: 0.9241\n",
      "Epoch 00014: val_accuracy improved from 0.89130 to 0.89170, saving model to /content/drive/MyDrive/27 CNN on CIFR/Model Output/model_7_Rev01_depthwise.h5\n",
      "390/390 [==============================] - 59s 151ms/step - loss: 0.2245 - accuracy: 0.9241 - val_loss: 0.3690 - val_accuracy: 0.8917\n",
      "Epoch 15/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2218 - accuracy: 0.9249\n",
      "Epoch 00015: val_accuracy did not improve from 0.89170\n",
      "390/390 [==============================] - 58s 150ms/step - loss: 0.2218 - accuracy: 0.9249 - val_loss: 0.3716 - val_accuracy: 0.8901\n",
      "Epoch 16/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2197 - accuracy: 0.9266\n",
      "Epoch 00016: val_accuracy did not improve from 0.89170\n",
      "390/390 [==============================] - 58s 149ms/step - loss: 0.2197 - accuracy: 0.9266 - val_loss: 0.3691 - val_accuracy: 0.8910\n",
      "Epoch 17/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2236 - accuracy: 0.9239\n",
      "Epoch 00017: val_accuracy did not improve from 0.89170\n",
      "390/390 [==============================] - 58s 149ms/step - loss: 0.2236 - accuracy: 0.9239 - val_loss: 0.3738 - val_accuracy: 0.8903\n",
      "Epoch 18/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2188 - accuracy: 0.9260\n",
      "Epoch 00018: val_accuracy did not improve from 0.89170\n",
      "390/390 [==============================] - 58s 149ms/step - loss: 0.2188 - accuracy: 0.9260 - val_loss: 0.3711 - val_accuracy: 0.8906\n",
      "Epoch 19/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2254 - accuracy: 0.9249\n",
      "Epoch 00019: val_accuracy did not improve from 0.89170\n",
      "390/390 [==============================] - 58s 150ms/step - loss: 0.2254 - accuracy: 0.9249 - val_loss: 0.3700 - val_accuracy: 0.8899\n",
      "Epoch 20/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2213 - accuracy: 0.9266\n",
      "Epoch 00020: val_accuracy did not improve from 0.89170\n",
      "390/390 [==============================] - 58s 149ms/step - loss: 0.2213 - accuracy: 0.9266 - val_loss: 0.3713 - val_accuracy: 0.8914\n",
      "Epoch 21/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2230 - accuracy: 0.9253\n",
      "Epoch 00021: val_accuracy did not improve from 0.89170\n",
      "390/390 [==============================] - 58s 149ms/step - loss: 0.2230 - accuracy: 0.9253 - val_loss: 0.3698 - val_accuracy: 0.8907\n",
      "Epoch 22/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2246 - accuracy: 0.9251\n",
      "Epoch 00022: val_accuracy did not improve from 0.89170\n",
      "390/390 [==============================] - 58s 149ms/step - loss: 0.2246 - accuracy: 0.9251 - val_loss: 0.3711 - val_accuracy: 0.8898\n",
      "Epoch 23/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2199 - accuracy: 0.9264\n",
      "Epoch 00023: val_accuracy did not improve from 0.89170\n",
      "390/390 [==============================] - 58s 149ms/step - loss: 0.2199 - accuracy: 0.9264 - val_loss: 0.3729 - val_accuracy: 0.8899\n",
      "Epoch 24/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2213 - accuracy: 0.9247\n",
      "Epoch 00024: val_accuracy did not improve from 0.89170\n",
      "390/390 [==============================] - 58s 149ms/step - loss: 0.2213 - accuracy: 0.9247 - val_loss: 0.3740 - val_accuracy: 0.8891\n",
      "Epoch 25/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2192 - accuracy: 0.9262\n",
      "Epoch 00025: val_accuracy did not improve from 0.89170\n",
      "390/390 [==============================] - 58s 149ms/step - loss: 0.2192 - accuracy: 0.9262 - val_loss: 0.3705 - val_accuracy: 0.8903\n",
      "Epoch 26/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2197 - accuracy: 0.9260\n",
      "Epoch 00026: val_accuracy did not improve from 0.89170\n",
      "390/390 [==============================] - 58s 149ms/step - loss: 0.2197 - accuracy: 0.9260 - val_loss: 0.3706 - val_accuracy: 0.8898\n",
      "Epoch 27/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2189 - accuracy: 0.9241\n",
      "Epoch 00027: val_accuracy did not improve from 0.89170\n",
      "390/390 [==============================] - 58s 150ms/step - loss: 0.2189 - accuracy: 0.9241 - val_loss: 0.3695 - val_accuracy: 0.8911\n",
      "Epoch 28/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2218 - accuracy: 0.9248\n",
      "Epoch 00028: val_accuracy did not improve from 0.89170\n",
      "390/390 [==============================] - 58s 150ms/step - loss: 0.2218 - accuracy: 0.9248 - val_loss: 0.3740 - val_accuracy: 0.8890\n",
      "Epoch 29/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2220 - accuracy: 0.9231\n",
      "Epoch 00029: val_accuracy did not improve from 0.89170\n",
      "390/390 [==============================] - 58s 150ms/step - loss: 0.2220 - accuracy: 0.9231 - val_loss: 0.3715 - val_accuracy: 0.8899\n",
      "Epoch 30/30\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2184 - accuracy: 0.9260\n",
      "Epoch 00030: val_accuracy did not improve from 0.89170\n",
      "390/390 [==============================] - 58s 149ms/step - loss: 0.2184 - accuracy: 0.9260 - val_loss: 0.3726 - val_accuracy: 0.8901\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.backend as k\n",
    "k.set_value(model.optimizer.lr, 0.0001)\n",
    "\n",
    "#Model Compilation\n",
    "#model.compile(loss='categorical_crossentropy',\n",
    "              #optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001, beta_1=0.9, beta_2=0.999, epsilon=1e-06, amsgrad=True, name='Adam'),\n",
    "              #metrics=['accuracy'])\n",
    "\n",
    "model.load_weights('/content/drive/MyDrive/27 CNN on CIFR/Model Output/model_7_180Epoch_Rev01_depthwise.h5')\n",
    "callback_list = [checkpoint]\n",
    "epochs = 30\n",
    "history = model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),\\\n",
    "                    steps_per_epoch=X_train.shape[0] // batch_size,epochs=epochs,\\\n",
    "                    verbose=1,validation_data=(X_test,y_test),callbacks=callback_list)\n",
    "\n",
    "model.save_weights('/content/drive/MyDrive/27 CNN on CIFR/Model Output/model_7_210Epoch_Rev01_depthwise.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dg1UbwvDOg1t",
    "outputId": "6e56cf7b-e1c1-488f-b5c3-cde7fd7ee7bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.1987 - accuracy: 0.9387\n",
      "Epoch 00001: val_accuracy improved from 0.89170 to 0.89250, saving model to /content/drive/MyDrive/27 CNN on CIFR/Model Output/model_7_Rev01_depthwise.h5\n",
      "19/19 [==============================] - 7s 362ms/step - loss: 0.1987 - accuracy: 0.9387 - val_loss: 0.3623 - val_accuracy: 0.8925\n",
      "Epoch 2/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2205 - accuracy: 0.9248\n",
      "Epoch 00002: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 317ms/step - loss: 0.2205 - accuracy: 0.9248 - val_loss: 0.3637 - val_accuracy: 0.8921\n",
      "Epoch 3/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2162 - accuracy: 0.9268\n",
      "Epoch 00003: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 320ms/step - loss: 0.2162 - accuracy: 0.9268 - val_loss: 0.3645 - val_accuracy: 0.8913\n",
      "Epoch 4/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2100 - accuracy: 0.9243\n",
      "Epoch 00004: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 318ms/step - loss: 0.2100 - accuracy: 0.9243 - val_loss: 0.3648 - val_accuracy: 0.8909\n",
      "Epoch 5/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.1939 - accuracy: 0.9359\n",
      "Epoch 00005: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 319ms/step - loss: 0.1939 - accuracy: 0.9359 - val_loss: 0.3661 - val_accuracy: 0.8911\n",
      "Epoch 6/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2042 - accuracy: 0.9272\n",
      "Epoch 00006: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 316ms/step - loss: 0.2042 - accuracy: 0.9272 - val_loss: 0.3675 - val_accuracy: 0.8906\n",
      "Epoch 7/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2145 - accuracy: 0.9280\n",
      "Epoch 00007: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 316ms/step - loss: 0.2145 - accuracy: 0.9280 - val_loss: 0.3675 - val_accuracy: 0.8911\n",
      "Epoch 8/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2424 - accuracy: 0.9198\n",
      "Epoch 00008: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 316ms/step - loss: 0.2424 - accuracy: 0.9198 - val_loss: 0.3705 - val_accuracy: 0.8897\n",
      "Epoch 9/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2104 - accuracy: 0.9276\n",
      "Epoch 00009: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 316ms/step - loss: 0.2104 - accuracy: 0.9276 - val_loss: 0.3696 - val_accuracy: 0.8901\n",
      "Epoch 10/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2302 - accuracy: 0.9211\n",
      "Epoch 00010: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 316ms/step - loss: 0.2302 - accuracy: 0.9211 - val_loss: 0.3683 - val_accuracy: 0.8911\n",
      "Epoch 11/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2244 - accuracy: 0.9215\n",
      "Epoch 00011: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 317ms/step - loss: 0.2244 - accuracy: 0.9215 - val_loss: 0.3698 - val_accuracy: 0.8904\n",
      "Epoch 12/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2235 - accuracy: 0.9243\n",
      "Epoch 00012: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 317ms/step - loss: 0.2235 - accuracy: 0.9243 - val_loss: 0.3715 - val_accuracy: 0.8896\n",
      "Epoch 13/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2310 - accuracy: 0.9211\n",
      "Epoch 00013: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 316ms/step - loss: 0.2310 - accuracy: 0.9211 - val_loss: 0.3713 - val_accuracy: 0.8897\n",
      "Epoch 14/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2189 - accuracy: 0.9317\n",
      "Epoch 00014: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 318ms/step - loss: 0.2189 - accuracy: 0.9317 - val_loss: 0.3716 - val_accuracy: 0.8897\n",
      "Epoch 15/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2261 - accuracy: 0.9248\n",
      "Epoch 00015: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 318ms/step - loss: 0.2261 - accuracy: 0.9248 - val_loss: 0.3721 - val_accuracy: 0.8901\n",
      "Epoch 16/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2139 - accuracy: 0.9309\n",
      "Epoch 00016: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 317ms/step - loss: 0.2139 - accuracy: 0.9309 - val_loss: 0.3714 - val_accuracy: 0.8905\n",
      "Epoch 17/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2112 - accuracy: 0.9264\n",
      "Epoch 00017: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 317ms/step - loss: 0.2112 - accuracy: 0.9264 - val_loss: 0.3705 - val_accuracy: 0.8910\n",
      "Epoch 18/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2335 - accuracy: 0.9198\n",
      "Epoch 00018: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 318ms/step - loss: 0.2335 - accuracy: 0.9198 - val_loss: 0.3702 - val_accuracy: 0.8913\n",
      "Epoch 19/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2168 - accuracy: 0.9219\n",
      "Epoch 00019: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 315ms/step - loss: 0.2168 - accuracy: 0.9219 - val_loss: 0.3710 - val_accuracy: 0.8905\n",
      "Epoch 20/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2396 - accuracy: 0.9165\n",
      "Epoch 00020: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 316ms/step - loss: 0.2396 - accuracy: 0.9165 - val_loss: 0.3698 - val_accuracy: 0.8909\n",
      "Epoch 21/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2228 - accuracy: 0.9268\n",
      "Epoch 00021: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 316ms/step - loss: 0.2228 - accuracy: 0.9268 - val_loss: 0.3707 - val_accuracy: 0.8908\n",
      "Epoch 22/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2155 - accuracy: 0.9350\n",
      "Epoch 00022: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 316ms/step - loss: 0.2155 - accuracy: 0.9350 - val_loss: 0.3711 - val_accuracy: 0.8904\n",
      "Epoch 23/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2207 - accuracy: 0.9215\n",
      "Epoch 00023: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 317ms/step - loss: 0.2207 - accuracy: 0.9215 - val_loss: 0.3711 - val_accuracy: 0.8902\n",
      "Epoch 24/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2191 - accuracy: 0.9169\n",
      "Epoch 00024: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 314ms/step - loss: 0.2191 - accuracy: 0.9169 - val_loss: 0.3720 - val_accuracy: 0.8903\n",
      "Epoch 25/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2310 - accuracy: 0.9231\n",
      "Epoch 00025: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 317ms/step - loss: 0.2310 - accuracy: 0.9231 - val_loss: 0.3718 - val_accuracy: 0.8903\n",
      "Epoch 26/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2332 - accuracy: 0.9194\n",
      "Epoch 00026: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 316ms/step - loss: 0.2332 - accuracy: 0.9194 - val_loss: 0.3734 - val_accuracy: 0.8896\n",
      "Epoch 27/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2433 - accuracy: 0.9186\n",
      "Epoch 00027: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 316ms/step - loss: 0.2433 - accuracy: 0.9186 - val_loss: 0.3721 - val_accuracy: 0.8897\n",
      "Epoch 28/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2331 - accuracy: 0.9182\n",
      "Epoch 00028: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 317ms/step - loss: 0.2331 - accuracy: 0.9182 - val_loss: 0.3718 - val_accuracy: 0.8906\n",
      "Epoch 29/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2536 - accuracy: 0.9190\n",
      "Epoch 00029: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 316ms/step - loss: 0.2536 - accuracy: 0.9190 - val_loss: 0.3730 - val_accuracy: 0.8895\n",
      "Epoch 30/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2302 - accuracy: 0.9182\n",
      "Epoch 00030: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 313ms/step - loss: 0.2302 - accuracy: 0.9182 - val_loss: 0.3727 - val_accuracy: 0.8898\n",
      "Epoch 31/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2047 - accuracy: 0.9297\n",
      "Epoch 00031: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 316ms/step - loss: 0.2047 - accuracy: 0.9297 - val_loss: 0.3730 - val_accuracy: 0.8900\n",
      "Epoch 32/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2347 - accuracy: 0.9227\n",
      "Epoch 00032: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 318ms/step - loss: 0.2347 - accuracy: 0.9227 - val_loss: 0.3726 - val_accuracy: 0.8898\n",
      "Epoch 33/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2504 - accuracy: 0.9174\n",
      "Epoch 00033: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 318ms/step - loss: 0.2504 - accuracy: 0.9174 - val_loss: 0.3700 - val_accuracy: 0.8907\n",
      "Epoch 34/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.1957 - accuracy: 0.9293\n",
      "Epoch 00034: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 316ms/step - loss: 0.1957 - accuracy: 0.9293 - val_loss: 0.3717 - val_accuracy: 0.8903\n",
      "Epoch 35/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2106 - accuracy: 0.9248\n",
      "Epoch 00035: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 317ms/step - loss: 0.2106 - accuracy: 0.9248 - val_loss: 0.3717 - val_accuracy: 0.8907\n",
      "Epoch 36/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2178 - accuracy: 0.9241\n",
      "Epoch 00036: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 313ms/step - loss: 0.2178 - accuracy: 0.9241 - val_loss: 0.3712 - val_accuracy: 0.8905\n",
      "Epoch 37/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2081 - accuracy: 0.9280\n",
      "Epoch 00037: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 316ms/step - loss: 0.2081 - accuracy: 0.9280 - val_loss: 0.3714 - val_accuracy: 0.8902\n",
      "Epoch 38/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2226 - accuracy: 0.9309\n",
      "Epoch 00038: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 316ms/step - loss: 0.2226 - accuracy: 0.9309 - val_loss: 0.3700 - val_accuracy: 0.8904\n",
      "Epoch 39/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2168 - accuracy: 0.9268\n",
      "Epoch 00039: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 317ms/step - loss: 0.2168 - accuracy: 0.9268 - val_loss: 0.3713 - val_accuracy: 0.8899\n",
      "Epoch 40/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2315 - accuracy: 0.9211\n",
      "Epoch 00040: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 317ms/step - loss: 0.2315 - accuracy: 0.9211 - val_loss: 0.3701 - val_accuracy: 0.8905\n",
      "Epoch 41/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2260 - accuracy: 0.9219\n",
      "Epoch 00041: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 317ms/step - loss: 0.2260 - accuracy: 0.9219 - val_loss: 0.3714 - val_accuracy: 0.8903\n",
      "Epoch 42/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2365 - accuracy: 0.9202\n",
      "Epoch 00042: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 318ms/step - loss: 0.2365 - accuracy: 0.9202 - val_loss: 0.3725 - val_accuracy: 0.8898\n",
      "Epoch 43/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.1981 - accuracy: 0.9330\n",
      "Epoch 00043: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 319ms/step - loss: 0.1981 - accuracy: 0.9330 - val_loss: 0.3725 - val_accuracy: 0.8895\n",
      "Epoch 44/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2218 - accuracy: 0.9252\n",
      "Epoch 00044: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 319ms/step - loss: 0.2218 - accuracy: 0.9252 - val_loss: 0.3724 - val_accuracy: 0.8896\n",
      "Epoch 45/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2137 - accuracy: 0.9285\n",
      "Epoch 00045: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 320ms/step - loss: 0.2137 - accuracy: 0.9285 - val_loss: 0.3728 - val_accuracy: 0.8897\n",
      "Epoch 46/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2152 - accuracy: 0.9223\n",
      "Epoch 00046: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 322ms/step - loss: 0.2152 - accuracy: 0.9223 - val_loss: 0.3723 - val_accuracy: 0.8893\n",
      "Epoch 47/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2262 - accuracy: 0.9235\n",
      "Epoch 00047: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 320ms/step - loss: 0.2262 - accuracy: 0.9235 - val_loss: 0.3731 - val_accuracy: 0.8894\n",
      "Epoch 48/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2114 - accuracy: 0.9248\n",
      "Epoch 00048: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 319ms/step - loss: 0.2114 - accuracy: 0.9248 - val_loss: 0.3724 - val_accuracy: 0.8891\n",
      "Epoch 49/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2112 - accuracy: 0.9285\n",
      "Epoch 00049: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 319ms/step - loss: 0.2112 - accuracy: 0.9285 - val_loss: 0.3725 - val_accuracy: 0.8896\n",
      "Epoch 50/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2170 - accuracy: 0.9268\n",
      "Epoch 00050: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 318ms/step - loss: 0.2170 - accuracy: 0.9268 - val_loss: 0.3736 - val_accuracy: 0.8900\n",
      "Epoch 51/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2384 - accuracy: 0.9198\n",
      "Epoch 00051: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 316ms/step - loss: 0.2384 - accuracy: 0.9198 - val_loss: 0.3740 - val_accuracy: 0.8891\n",
      "Epoch 52/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2295 - accuracy: 0.9206\n",
      "Epoch 00052: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 318ms/step - loss: 0.2295 - accuracy: 0.9206 - val_loss: 0.3745 - val_accuracy: 0.8890\n",
      "Epoch 53/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2068 - accuracy: 0.9280\n",
      "Epoch 00053: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 318ms/step - loss: 0.2068 - accuracy: 0.9280 - val_loss: 0.3748 - val_accuracy: 0.8892\n",
      "Epoch 54/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2156 - accuracy: 0.9252\n",
      "Epoch 00054: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 317ms/step - loss: 0.2156 - accuracy: 0.9252 - val_loss: 0.3754 - val_accuracy: 0.8890\n",
      "Epoch 55/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2376 - accuracy: 0.9178\n",
      "Epoch 00055: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 317ms/step - loss: 0.2376 - accuracy: 0.9178 - val_loss: 0.3750 - val_accuracy: 0.8893\n",
      "Epoch 56/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2195 - accuracy: 0.9252\n",
      "Epoch 00056: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 317ms/step - loss: 0.2195 - accuracy: 0.9252 - val_loss: 0.3753 - val_accuracy: 0.8899\n",
      "Epoch 57/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2054 - accuracy: 0.9322\n",
      "Epoch 00057: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 316ms/step - loss: 0.2054 - accuracy: 0.9322 - val_loss: 0.3760 - val_accuracy: 0.8888\n",
      "Epoch 58/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2225 - accuracy: 0.9161\n",
      "Epoch 00058: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 317ms/step - loss: 0.2225 - accuracy: 0.9161 - val_loss: 0.3747 - val_accuracy: 0.8896\n",
      "Epoch 59/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2158 - accuracy: 0.9313\n",
      "Epoch 00059: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 318ms/step - loss: 0.2158 - accuracy: 0.9313 - val_loss: 0.3750 - val_accuracy: 0.8889\n",
      "Epoch 60/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2215 - accuracy: 0.9206\n",
      "Epoch 00060: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 319ms/step - loss: 0.2215 - accuracy: 0.9206 - val_loss: 0.3765 - val_accuracy: 0.8887\n",
      "Epoch 61/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2144 - accuracy: 0.9289\n",
      "Epoch 00061: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 319ms/step - loss: 0.2144 - accuracy: 0.9289 - val_loss: 0.3776 - val_accuracy: 0.8889\n",
      "Epoch 62/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2566 - accuracy: 0.9100\n",
      "Epoch 00062: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 323ms/step - loss: 0.2566 - accuracy: 0.9100 - val_loss: 0.3785 - val_accuracy: 0.8885\n",
      "Epoch 63/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2093 - accuracy: 0.9305\n",
      "Epoch 00063: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 321ms/step - loss: 0.2093 - accuracy: 0.9305 - val_loss: 0.3789 - val_accuracy: 0.8877\n",
      "Epoch 64/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2047 - accuracy: 0.9313\n",
      "Epoch 00064: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 320ms/step - loss: 0.2047 - accuracy: 0.9313 - val_loss: 0.3772 - val_accuracy: 0.8882\n",
      "Epoch 65/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2111 - accuracy: 0.9280\n",
      "Epoch 00065: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 321ms/step - loss: 0.2111 - accuracy: 0.9280 - val_loss: 0.3779 - val_accuracy: 0.8885\n",
      "Epoch 66/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2234 - accuracy: 0.9228\n",
      "Epoch 00066: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 319ms/step - loss: 0.2234 - accuracy: 0.9228 - val_loss: 0.3762 - val_accuracy: 0.8888\n",
      "Epoch 67/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2429 - accuracy: 0.9190\n",
      "Epoch 00067: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 323ms/step - loss: 0.2429 - accuracy: 0.9190 - val_loss: 0.3756 - val_accuracy: 0.8888\n",
      "Epoch 68/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2197 - accuracy: 0.9260\n",
      "Epoch 00068: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 320ms/step - loss: 0.2197 - accuracy: 0.9260 - val_loss: 0.3752 - val_accuracy: 0.8887\n",
      "Epoch 69/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2242 - accuracy: 0.9252\n",
      "Epoch 00069: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 320ms/step - loss: 0.2242 - accuracy: 0.9252 - val_loss: 0.3740 - val_accuracy: 0.8882\n",
      "Epoch 70/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2313 - accuracy: 0.9206\n",
      "Epoch 00070: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 320ms/step - loss: 0.2313 - accuracy: 0.9206 - val_loss: 0.3751 - val_accuracy: 0.8882\n",
      "Epoch 71/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2073 - accuracy: 0.9322\n",
      "Epoch 00071: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 319ms/step - loss: 0.2073 - accuracy: 0.9322 - val_loss: 0.3757 - val_accuracy: 0.8885\n",
      "Epoch 72/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2299 - accuracy: 0.9174\n",
      "Epoch 00072: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 320ms/step - loss: 0.2299 - accuracy: 0.9174 - val_loss: 0.3750 - val_accuracy: 0.8884\n",
      "Epoch 73/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2349 - accuracy: 0.9132\n",
      "Epoch 00073: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 320ms/step - loss: 0.2349 - accuracy: 0.9132 - val_loss: 0.3773 - val_accuracy: 0.8882\n",
      "Epoch 74/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2116 - accuracy: 0.9239\n",
      "Epoch 00074: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 322ms/step - loss: 0.2116 - accuracy: 0.9239 - val_loss: 0.3764 - val_accuracy: 0.8886\n",
      "Epoch 75/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2184 - accuracy: 0.9235\n",
      "Epoch 00075: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 323ms/step - loss: 0.2184 - accuracy: 0.9235 - val_loss: 0.3757 - val_accuracy: 0.8887\n",
      "Epoch 76/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2332 - accuracy: 0.9199\n",
      "Epoch 00076: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 319ms/step - loss: 0.2332 - accuracy: 0.9199 - val_loss: 0.3747 - val_accuracy: 0.8887\n",
      "Epoch 77/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2406 - accuracy: 0.9161\n",
      "Epoch 00077: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 321ms/step - loss: 0.2406 - accuracy: 0.9161 - val_loss: 0.3728 - val_accuracy: 0.8891\n",
      "Epoch 78/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2212 - accuracy: 0.9224\n",
      "Epoch 00078: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 320ms/step - loss: 0.2212 - accuracy: 0.9224 - val_loss: 0.3729 - val_accuracy: 0.8890\n",
      "Epoch 79/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2405 - accuracy: 0.9137\n",
      "Epoch 00079: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 323ms/step - loss: 0.2405 - accuracy: 0.9137 - val_loss: 0.3732 - val_accuracy: 0.8894\n",
      "Epoch 80/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2042 - accuracy: 0.9297\n",
      "Epoch 00080: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 322ms/step - loss: 0.2042 - accuracy: 0.9297 - val_loss: 0.3729 - val_accuracy: 0.8893\n",
      "Epoch 81/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2053 - accuracy: 0.9342\n",
      "Epoch 00081: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 321ms/step - loss: 0.2053 - accuracy: 0.9342 - val_loss: 0.3729 - val_accuracy: 0.8895\n",
      "Epoch 82/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2236 - accuracy: 0.9235\n",
      "Epoch 00082: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 321ms/step - loss: 0.2236 - accuracy: 0.9235 - val_loss: 0.3729 - val_accuracy: 0.8897\n",
      "Epoch 83/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2161 - accuracy: 0.9285\n",
      "Epoch 00083: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 320ms/step - loss: 0.2161 - accuracy: 0.9285 - val_loss: 0.3723 - val_accuracy: 0.8901\n",
      "Epoch 84/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2323 - accuracy: 0.9206\n",
      "Epoch 00084: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 323ms/step - loss: 0.2323 - accuracy: 0.9206 - val_loss: 0.3734 - val_accuracy: 0.8898\n",
      "Epoch 85/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2122 - accuracy: 0.9243\n",
      "Epoch 00085: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 325ms/step - loss: 0.2122 - accuracy: 0.9243 - val_loss: 0.3735 - val_accuracy: 0.8900\n",
      "Epoch 86/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2171 - accuracy: 0.9322\n",
      "Epoch 00086: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 320ms/step - loss: 0.2171 - accuracy: 0.9322 - val_loss: 0.3739 - val_accuracy: 0.8898\n",
      "Epoch 87/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2106 - accuracy: 0.9248\n",
      "Epoch 00087: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 320ms/step - loss: 0.2106 - accuracy: 0.9248 - val_loss: 0.3734 - val_accuracy: 0.8903\n",
      "Epoch 88/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2285 - accuracy: 0.9219\n",
      "Epoch 00088: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 319ms/step - loss: 0.2285 - accuracy: 0.9219 - val_loss: 0.3738 - val_accuracy: 0.8898\n",
      "Epoch 89/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2166 - accuracy: 0.9293\n",
      "Epoch 00089: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 320ms/step - loss: 0.2166 - accuracy: 0.9293 - val_loss: 0.3756 - val_accuracy: 0.8887\n",
      "Epoch 90/90\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2297 - accuracy: 0.9252\n",
      "Epoch 00090: val_accuracy did not improve from 0.89250\n",
      "19/19 [==============================] - 6s 319ms/step - loss: 0.2297 - accuracy: 0.9252 - val_loss: 0.3757 - val_accuracy: 0.8893\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.backend as k\n",
    "k.set_value(model.optimizer.lr, 0.0001)\n",
    "\n",
    "\n",
    "decay_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.99, patience= 5, \n",
    "                                                verbose=1, mode='auto', min_delta=0.001, \n",
    "                                                cooldown=0, min_lr=0.000001)\n",
    "\n",
    "callback_list = [checkpoint]\n",
    "\n",
    "\n",
    "model.load_weights('/content/drive/MyDrive/27 CNN on CIFR/Model Output/model_7_210Epoch_Rev01_depthwise.h5')\n",
    "\n",
    "epochs = 90\n",
    "history = model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),\\\n",
    "                    steps_per_epoch=0.05*X_train.shape[0] // batch_size,epochs=epochs,\\\n",
    "                    verbose=1,validation_data=(X_test,y_test),callbacks=callback_list)\n",
    "\n",
    "model.save_weights('/content/drive/MyDrive/27 CNN on CIFR/Model Output/model_7_300Epoch_Rev01_depthwise.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H52F8-4nOWN9"
   },
   "source": [
    "# **Model 2:** Depthwise Seperable Conv2D + without Dropout + Without Dense Layer + Image Augmentation + SGD + Weight Regularizer(L1+L2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fN-cj0iVOWN-"
   },
   "source": [
    "ADAM performs better than SGD ir seems. Let's using **more filters + bias=True + increased kernel size.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qu_Axm32OWN-"
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "if 'model' in locals():\n",
    "  del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uvJsEieaOWN-"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs =100\n",
    "l = 24\n",
    "num_filter = 36\n",
    "compression = 0.5\n",
    "dropout_rate = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vQm1IowLOWN-"
   },
   "outputs": [],
   "source": [
    "# Dense Block\n",
    "def denseblock(input, num_filter = num_filter, dropout_rate = dropout_rate):\n",
    "    global compression\n",
    "    temp = input\n",
    "    for _ in range(l): \n",
    "        BatchNorm = layers.BatchNormalization()(temp)\n",
    "        relu = layers.Activation('relu')(BatchNorm)\n",
    "        Conv2D_3_3 = layers.SeparableConv2D(int(num_filter*compression), (3,3), use_bias=False ,padding='same', \n",
    "                                   kernel_initializer='he_normal',kernel_regularizer=regularizers.L1L2(l1=0.000001, l2=0.00001))(relu)\n",
    "        #Conv2D_3_3 = layers.Conv2D(int(num_filter*compression), (3,3), use_bias=False ,padding='same')(relu)\n",
    "        if dropout_rate>0:\n",
    "            Conv2D_3_3 = layers.Dropout(dropout_rate)(Conv2D_3_3)\n",
    "        concat = layers.Concatenate(axis=-1)([temp,Conv2D_3_3])\n",
    "        \n",
    "        temp = concat\n",
    "        \n",
    "    return temp\n",
    "\n",
    "## transition Blosck\n",
    "def transition(input, num_filter = num_filter, dropout_rate = dropout_rate):\n",
    "    global compression\n",
    "    BatchNorm = layers.BatchNormalization()(input)\n",
    "    relu = layers.Activation('relu')(BatchNorm)\n",
    "    Conv2D_BottleNeck = layers.SeparableConv2D(int(num_filter*compression), (1,1), use_bias=False ,padding='same',  \n",
    "                                               depthwise_initializer='he_normal', depthwise_regularizer=regularizers.L1L2(l1=0.0001, l2=0.0001),\n",
    "                                               kernel_regularizer=regularizers.L2(l2=0.001))(relu)\n",
    "    #Conv2D_BottleNeck =  Conv2D(int(num_filter*compression), (1,1), use_bias=False, kernel_regularizer = regularizers.l1() ,padding='same')(relu)\n",
    "    if dropout_rate>0:\n",
    "         Conv2D_BottleNeck = layers.Dropout(dropout_rate)(Conv2D_BottleNeck)\n",
    "    avg = layers.AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n",
    "    return avg\n",
    "\n",
    "#output layer\n",
    "def output_layer(input):\n",
    "    global compression\n",
    "    BatchNorm = layers.BatchNormalization()(input)\n",
    "    relu = layers.Activation('relu')(BatchNorm)\n",
    "    AvgPooling = layers.AveragePooling2D(pool_size=(2,2))(relu)\n",
    "    #flat = layers.Flatten()(AvgPooling)\n",
    "    #output = layers.Dense(num_classes, activation='softmax')(flat)\n",
    "    out_conv = layers.Conv2D(num_classes, kernel_size = (2,2), activation='softmax')(AvgPooling)\n",
    "    output = layers.Flatten()(out_conv)\n",
    "\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ynW3rlOSOWN_"
   },
   "outputs": [],
   "source": [
    "#Model Architecture\n",
    "input = layers.Input(shape=(img_height, img_width, channel,))\n",
    "First_Conv2D = layers.Conv2D(num_filter, (3,3), use_bias=False ,padding='same')(input)\n",
    "#First_Conv2D = layers.SeparableConv2D(int(num_filter), (3,3), use_bias=True ,padding='same', \n",
    "                                      #kernel_regularizer=regularizers.L1L2(l1=0.0001, l2=0.0001))(input)\n",
    "\n",
    "First_Block = denseblock(First_Conv2D, num_filter, dropout_rate)\n",
    "First_Transition = transition(First_Block, num_filter, dropout_rate)\n",
    "\n",
    "Second_Block = denseblock(First_Transition, num_filter, dropout_rate)\n",
    "Second_Transition = transition(Second_Block, num_filter, dropout_rate)\n",
    "\n",
    "Third_Block = denseblock(Second_Transition, num_filter, dropout_rate)\n",
    "Third_Transition = transition(Third_Block, num_filter, dropout_rate)\n",
    "\n",
    "Last_Block = denseblock(Third_Transition,  num_filter, dropout_rate)\n",
    "output = output_layer(Last_Block)\n",
    "\n",
    "model = Model(inputs=[input], outputs=[output])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3TPabrzHOWOA"
   },
   "outputs": [],
   "source": [
    "# mormalize data\n",
    "#X_train = X_train.astype('float32') / 255\n",
    "#X_test = X_test.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E6e84d-cOWOA"
   },
   "outputs": [],
   "source": [
    "###Image data Generator class\n",
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(featurewise_center=False, \n",
    "                                                          samplewise_center=False,  \n",
    "                                                          featurewise_std_normalization=False,  \n",
    "                                                          samplewise_std_normalization=False,  \n",
    "                                                          zca_whitening=False, \n",
    "                                                          rotation_range=15, \n",
    "                                                          width_shift_range=0.1,\n",
    "                                                          height_shift_range=0.1,  \n",
    "                                                          horizontal_flip=True,  \n",
    "                                                          vertical_flip=False , zoom_range=0.2, shear_range=15)\n",
    "##We are fitting the data to Image data generator.\n",
    "#ImageGenerator = ImageFlow.flow(X_train,seed=10,batch_size=32)\n",
    "datagen.fit(X_train, augment=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dbi-OCG-OWOA"
   },
   "outputs": [],
   "source": [
    "#Saving Best Model and Representation of results\n",
    "filepath = \"/content/drive/MyDrive/27 CNN on CIFR/Model Output/model_depthwise_best.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath= filepath, save_weights_only=True,\n",
    "                              monitor='val_accuracy', verbose=1,\n",
    "                              save_best_only=True, mode='max') \n",
    "\n",
    "log_dir = \"logs/model_depthwise\"\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir = log_dir, histogram_freq=1)\n",
    "#callback_list = [checkpoint, tensorboard_callback, decay_lr]\n",
    "\n",
    "#Model Compilation\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.SGD(0.01, momentum = 0.7),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dNK4z_b7OWOA",
    "outputId": "65122cfb-94c7-4485-eeb9-1a484bad8e62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 32, 32, 36)   972         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 32, 32, 36)  144         ['conv2d[0][0]']                 \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 32, 32, 36)   0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " separable_conv2d (SeparableCon  (None, 32, 32, 18)  972         ['activation[0][0]']             \n",
      " v2D)                                                                                             \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 32, 32, 54)   0           ['conv2d[0][0]',                 \n",
      "                                                                  'separable_conv2d[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 32, 32, 54)  216         ['concatenate[0][0]']            \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 32, 32, 54)   0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " separable_conv2d_1 (SeparableC  (None, 32, 32, 18)  1458        ['activation_1[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 32, 32, 72)   0           ['concatenate[0][0]',            \n",
      "                                                                  'separable_conv2d_1[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 32, 32, 72)  288         ['concatenate_1[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 32, 32, 72)   0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " separable_conv2d_2 (SeparableC  (None, 32, 32, 18)  1944        ['activation_2[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 32, 32, 90)   0           ['concatenate_1[0][0]',          \n",
      "                                                                  'separable_conv2d_2[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 32, 32, 90)  360         ['concatenate_2[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 32, 32, 90)   0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " separable_conv2d_3 (SeparableC  (None, 32, 32, 18)  2430        ['activation_3[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 32, 32, 108)  0           ['concatenate_2[0][0]',          \n",
      "                                                                  'separable_conv2d_3[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 32, 32, 108)  432        ['concatenate_3[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 32, 32, 108)  0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " separable_conv2d_4 (SeparableC  (None, 32, 32, 18)  2916        ['activation_4[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate)    (None, 32, 32, 126)  0           ['concatenate_3[0][0]',          \n",
      "                                                                  'separable_conv2d_4[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 32, 32, 126)  504        ['concatenate_4[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 32, 32, 126)  0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " separable_conv2d_5 (SeparableC  (None, 32, 32, 18)  3402        ['activation_5[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " concatenate_5 (Concatenate)    (None, 32, 32, 144)  0           ['concatenate_4[0][0]',          \n",
      "                                                                  'separable_conv2d_5[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 32, 32, 144)  576        ['concatenate_5[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 32, 32, 144)  0           ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " separable_conv2d_6 (SeparableC  (None, 32, 32, 18)  3888        ['activation_6[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " concatenate_6 (Concatenate)    (None, 32, 32, 162)  0           ['concatenate_5[0][0]',          \n",
      "                                                                  'separable_conv2d_6[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 32, 32, 162)  648        ['concatenate_6[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 32, 32, 162)  0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " separable_conv2d_7 (SeparableC  (None, 32, 32, 18)  4374        ['activation_7[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " concatenate_7 (Concatenate)    (None, 32, 32, 180)  0           ['concatenate_6[0][0]',          \n",
      "                                                                  'separable_conv2d_7[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 32, 32, 180)  720        ['concatenate_7[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (None, 32, 32, 180)  0           ['batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " separable_conv2d_8 (SeparableC  (None, 32, 32, 18)  4860        ['activation_8[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " concatenate_8 (Concatenate)    (None, 32, 32, 198)  0           ['concatenate_7[0][0]',          \n",
      "                                                                  'separable_conv2d_8[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 32, 32, 198)  792        ['concatenate_8[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_9 (Activation)      (None, 32, 32, 198)  0           ['batch_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " separable_conv2d_9 (SeparableC  (None, 32, 32, 18)  5346        ['activation_9[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " concatenate_9 (Concatenate)    (None, 32, 32, 216)  0           ['concatenate_8[0][0]',          \n",
      "                                                                  'separable_conv2d_9[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 32, 32, 216)  864        ['concatenate_9[0][0]']          \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_10 (Activation)     (None, 32, 32, 216)  0           ['batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_10 (Separable  (None, 32, 32, 18)  5832        ['activation_10[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_10 (Concatenate)   (None, 32, 32, 234)  0           ['concatenate_9[0][0]',          \n",
      "                                                                  'separable_conv2d_10[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 32, 32, 234)  936        ['concatenate_10[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_11 (Activation)     (None, 32, 32, 234)  0           ['batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_11 (Separable  (None, 32, 32, 18)  6318        ['activation_11[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_11 (Concatenate)   (None, 32, 32, 252)  0           ['concatenate_10[0][0]',         \n",
      "                                                                  'separable_conv2d_11[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 32, 32, 252)  1008       ['concatenate_11[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_12 (Activation)     (None, 32, 32, 252)  0           ['batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_12 (Separable  (None, 32, 32, 18)  6804        ['activation_12[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_12 (Concatenate)   (None, 32, 32, 270)  0           ['concatenate_11[0][0]',         \n",
      "                                                                  'separable_conv2d_12[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 32, 32, 270)  1080       ['concatenate_12[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_13 (Activation)     (None, 32, 32, 270)  0           ['batch_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_13 (Separable  (None, 32, 32, 18)  7290        ['activation_13[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_13 (Concatenate)   (None, 32, 32, 288)  0           ['concatenate_12[0][0]',         \n",
      "                                                                  'separable_conv2d_13[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 32, 32, 288)  1152       ['concatenate_13[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_14 (Activation)     (None, 32, 32, 288)  0           ['batch_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_14 (Separable  (None, 32, 32, 18)  7776        ['activation_14[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_14 (Concatenate)   (None, 32, 32, 306)  0           ['concatenate_13[0][0]',         \n",
      "                                                                  'separable_conv2d_14[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 32, 32, 306)  1224       ['concatenate_14[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_15 (Activation)     (None, 32, 32, 306)  0           ['batch_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_15 (Separable  (None, 32, 32, 18)  8262        ['activation_15[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_15 (Concatenate)   (None, 32, 32, 324)  0           ['concatenate_14[0][0]',         \n",
      "                                                                  'separable_conv2d_15[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 32, 32, 324)  1296       ['concatenate_15[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_16 (Activation)     (None, 32, 32, 324)  0           ['batch_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_16 (Separable  (None, 32, 32, 18)  8748        ['activation_16[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_16 (Concatenate)   (None, 32, 32, 342)  0           ['concatenate_15[0][0]',         \n",
      "                                                                  'separable_conv2d_16[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 32, 32, 342)  1368       ['concatenate_16[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_17 (Activation)     (None, 32, 32, 342)  0           ['batch_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_17 (Separable  (None, 32, 32, 18)  9234        ['activation_17[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_17 (Concatenate)   (None, 32, 32, 360)  0           ['concatenate_16[0][0]',         \n",
      "                                                                  'separable_conv2d_17[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_18 (BatchN  (None, 32, 32, 360)  1440       ['concatenate_17[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_18 (Activation)     (None, 32, 32, 360)  0           ['batch_normalization_18[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_18 (Separable  (None, 32, 32, 18)  9720        ['activation_18[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_18 (Concatenate)   (None, 32, 32, 378)  0           ['concatenate_17[0][0]',         \n",
      "                                                                  'separable_conv2d_18[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_19 (BatchN  (None, 32, 32, 378)  1512       ['concatenate_18[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_19 (Activation)     (None, 32, 32, 378)  0           ['batch_normalization_19[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_19 (Separable  (None, 32, 32, 18)  10206       ['activation_19[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_19 (Concatenate)   (None, 32, 32, 396)  0           ['concatenate_18[0][0]',         \n",
      "                                                                  'separable_conv2d_19[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_20 (BatchN  (None, 32, 32, 396)  1584       ['concatenate_19[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_20 (Activation)     (None, 32, 32, 396)  0           ['batch_normalization_20[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_20 (Separable  (None, 32, 32, 18)  10692       ['activation_20[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_20 (Concatenate)   (None, 32, 32, 414)  0           ['concatenate_19[0][0]',         \n",
      "                                                                  'separable_conv2d_20[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_21 (BatchN  (None, 32, 32, 414)  1656       ['concatenate_20[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_21 (Activation)     (None, 32, 32, 414)  0           ['batch_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_21 (Separable  (None, 32, 32, 18)  11178       ['activation_21[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_21 (Concatenate)   (None, 32, 32, 432)  0           ['concatenate_20[0][0]',         \n",
      "                                                                  'separable_conv2d_21[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_22 (BatchN  (None, 32, 32, 432)  1728       ['concatenate_21[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_22 (Activation)     (None, 32, 32, 432)  0           ['batch_normalization_22[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_22 (Separable  (None, 32, 32, 18)  11664       ['activation_22[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_22 (Concatenate)   (None, 32, 32, 450)  0           ['concatenate_21[0][0]',         \n",
      "                                                                  'separable_conv2d_22[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_23 (BatchN  (None, 32, 32, 450)  1800       ['concatenate_22[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_23 (Activation)     (None, 32, 32, 450)  0           ['batch_normalization_23[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_23 (Separable  (None, 32, 32, 18)  12150       ['activation_23[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_23 (Concatenate)   (None, 32, 32, 468)  0           ['concatenate_22[0][0]',         \n",
      "                                                                  'separable_conv2d_23[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_24 (BatchN  (None, 32, 32, 468)  1872       ['concatenate_23[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_24 (Activation)     (None, 32, 32, 468)  0           ['batch_normalization_24[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_24 (Separable  (None, 32, 32, 18)  8892        ['activation_24[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " average_pooling2d (AveragePool  (None, 16, 16, 18)  0           ['separable_conv2d_24[0][0]']    \n",
      " ing2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_25 (BatchN  (None, 16, 16, 18)  72          ['average_pooling2d[0][0]']      \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_25 (Activation)     (None, 16, 16, 18)   0           ['batch_normalization_25[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_25 (Separable  (None, 16, 16, 18)  486         ['activation_25[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_24 (Concatenate)   (None, 16, 16, 36)   0           ['average_pooling2d[0][0]',      \n",
      "                                                                  'separable_conv2d_25[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_26 (BatchN  (None, 16, 16, 36)  144         ['concatenate_24[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_26 (Activation)     (None, 16, 16, 36)   0           ['batch_normalization_26[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_26 (Separable  (None, 16, 16, 18)  972         ['activation_26[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_25 (Concatenate)   (None, 16, 16, 54)   0           ['concatenate_24[0][0]',         \n",
      "                                                                  'separable_conv2d_26[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_27 (BatchN  (None, 16, 16, 54)  216         ['concatenate_25[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_27 (Activation)     (None, 16, 16, 54)   0           ['batch_normalization_27[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_27 (Separable  (None, 16, 16, 18)  1458        ['activation_27[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_26 (Concatenate)   (None, 16, 16, 72)   0           ['concatenate_25[0][0]',         \n",
      "                                                                  'separable_conv2d_27[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_28 (BatchN  (None, 16, 16, 72)  288         ['concatenate_26[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_28 (Activation)     (None, 16, 16, 72)   0           ['batch_normalization_28[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_28 (Separable  (None, 16, 16, 18)  1944        ['activation_28[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_27 (Concatenate)   (None, 16, 16, 90)   0           ['concatenate_26[0][0]',         \n",
      "                                                                  'separable_conv2d_28[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_29 (BatchN  (None, 16, 16, 90)  360         ['concatenate_27[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_29 (Activation)     (None, 16, 16, 90)   0           ['batch_normalization_29[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_29 (Separable  (None, 16, 16, 18)  2430        ['activation_29[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_28 (Concatenate)   (None, 16, 16, 108)  0           ['concatenate_27[0][0]',         \n",
      "                                                                  'separable_conv2d_29[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_30 (BatchN  (None, 16, 16, 108)  432        ['concatenate_28[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_30 (Activation)     (None, 16, 16, 108)  0           ['batch_normalization_30[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_30 (Separable  (None, 16, 16, 18)  2916        ['activation_30[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_29 (Concatenate)   (None, 16, 16, 126)  0           ['concatenate_28[0][0]',         \n",
      "                                                                  'separable_conv2d_30[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_31 (BatchN  (None, 16, 16, 126)  504        ['concatenate_29[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_31 (Activation)     (None, 16, 16, 126)  0           ['batch_normalization_31[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_31 (Separable  (None, 16, 16, 18)  3402        ['activation_31[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_30 (Concatenate)   (None, 16, 16, 144)  0           ['concatenate_29[0][0]',         \n",
      "                                                                  'separable_conv2d_31[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_32 (BatchN  (None, 16, 16, 144)  576        ['concatenate_30[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_32 (Activation)     (None, 16, 16, 144)  0           ['batch_normalization_32[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_32 (Separable  (None, 16, 16, 18)  3888        ['activation_32[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_31 (Concatenate)   (None, 16, 16, 162)  0           ['concatenate_30[0][0]',         \n",
      "                                                                  'separable_conv2d_32[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_33 (BatchN  (None, 16, 16, 162)  648        ['concatenate_31[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_33 (Activation)     (None, 16, 16, 162)  0           ['batch_normalization_33[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_33 (Separable  (None, 16, 16, 18)  4374        ['activation_33[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_32 (Concatenate)   (None, 16, 16, 180)  0           ['concatenate_31[0][0]',         \n",
      "                                                                  'separable_conv2d_33[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_34 (BatchN  (None, 16, 16, 180)  720        ['concatenate_32[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_34 (Activation)     (None, 16, 16, 180)  0           ['batch_normalization_34[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_34 (Separable  (None, 16, 16, 18)  4860        ['activation_34[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_33 (Concatenate)   (None, 16, 16, 198)  0           ['concatenate_32[0][0]',         \n",
      "                                                                  'separable_conv2d_34[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_35 (BatchN  (None, 16, 16, 198)  792        ['concatenate_33[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_35 (Activation)     (None, 16, 16, 198)  0           ['batch_normalization_35[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_35 (Separable  (None, 16, 16, 18)  5346        ['activation_35[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_34 (Concatenate)   (None, 16, 16, 216)  0           ['concatenate_33[0][0]',         \n",
      "                                                                  'separable_conv2d_35[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_36 (BatchN  (None, 16, 16, 216)  864        ['concatenate_34[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_36 (Activation)     (None, 16, 16, 216)  0           ['batch_normalization_36[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_36 (Separable  (None, 16, 16, 18)  5832        ['activation_36[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_35 (Concatenate)   (None, 16, 16, 234)  0           ['concatenate_34[0][0]',         \n",
      "                                                                  'separable_conv2d_36[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_37 (BatchN  (None, 16, 16, 234)  936        ['concatenate_35[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_37 (Activation)     (None, 16, 16, 234)  0           ['batch_normalization_37[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_37 (Separable  (None, 16, 16, 18)  6318        ['activation_37[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_36 (Concatenate)   (None, 16, 16, 252)  0           ['concatenate_35[0][0]',         \n",
      "                                                                  'separable_conv2d_37[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_38 (BatchN  (None, 16, 16, 252)  1008       ['concatenate_36[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_38 (Activation)     (None, 16, 16, 252)  0           ['batch_normalization_38[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_38 (Separable  (None, 16, 16, 18)  6804        ['activation_38[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_37 (Concatenate)   (None, 16, 16, 270)  0           ['concatenate_36[0][0]',         \n",
      "                                                                  'separable_conv2d_38[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_39 (BatchN  (None, 16, 16, 270)  1080       ['concatenate_37[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_39 (Activation)     (None, 16, 16, 270)  0           ['batch_normalization_39[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_39 (Separable  (None, 16, 16, 18)  7290        ['activation_39[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_38 (Concatenate)   (None, 16, 16, 288)  0           ['concatenate_37[0][0]',         \n",
      "                                                                  'separable_conv2d_39[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_40 (BatchN  (None, 16, 16, 288)  1152       ['concatenate_38[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_40 (Activation)     (None, 16, 16, 288)  0           ['batch_normalization_40[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_40 (Separable  (None, 16, 16, 18)  7776        ['activation_40[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_39 (Concatenate)   (None, 16, 16, 306)  0           ['concatenate_38[0][0]',         \n",
      "                                                                  'separable_conv2d_40[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_41 (BatchN  (None, 16, 16, 306)  1224       ['concatenate_39[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_41 (Activation)     (None, 16, 16, 306)  0           ['batch_normalization_41[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_41 (Separable  (None, 16, 16, 18)  8262        ['activation_41[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_40 (Concatenate)   (None, 16, 16, 324)  0           ['concatenate_39[0][0]',         \n",
      "                                                                  'separable_conv2d_41[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_42 (BatchN  (None, 16, 16, 324)  1296       ['concatenate_40[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_42 (Activation)     (None, 16, 16, 324)  0           ['batch_normalization_42[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_42 (Separable  (None, 16, 16, 18)  8748        ['activation_42[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_41 (Concatenate)   (None, 16, 16, 342)  0           ['concatenate_40[0][0]',         \n",
      "                                                                  'separable_conv2d_42[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_43 (BatchN  (None, 16, 16, 342)  1368       ['concatenate_41[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_43 (Activation)     (None, 16, 16, 342)  0           ['batch_normalization_43[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_43 (Separable  (None, 16, 16, 18)  9234        ['activation_43[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_42 (Concatenate)   (None, 16, 16, 360)  0           ['concatenate_41[0][0]',         \n",
      "                                                                  'separable_conv2d_43[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_44 (BatchN  (None, 16, 16, 360)  1440       ['concatenate_42[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_44 (Activation)     (None, 16, 16, 360)  0           ['batch_normalization_44[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_44 (Separable  (None, 16, 16, 18)  9720        ['activation_44[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_43 (Concatenate)   (None, 16, 16, 378)  0           ['concatenate_42[0][0]',         \n",
      "                                                                  'separable_conv2d_44[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_45 (BatchN  (None, 16, 16, 378)  1512       ['concatenate_43[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_45 (Activation)     (None, 16, 16, 378)  0           ['batch_normalization_45[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_45 (Separable  (None, 16, 16, 18)  10206       ['activation_45[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_44 (Concatenate)   (None, 16, 16, 396)  0           ['concatenate_43[0][0]',         \n",
      "                                                                  'separable_conv2d_45[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_46 (BatchN  (None, 16, 16, 396)  1584       ['concatenate_44[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_46 (Activation)     (None, 16, 16, 396)  0           ['batch_normalization_46[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_46 (Separable  (None, 16, 16, 18)  10692       ['activation_46[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_45 (Concatenate)   (None, 16, 16, 414)  0           ['concatenate_44[0][0]',         \n",
      "                                                                  'separable_conv2d_46[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_47 (BatchN  (None, 16, 16, 414)  1656       ['concatenate_45[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_47 (Activation)     (None, 16, 16, 414)  0           ['batch_normalization_47[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_47 (Separable  (None, 16, 16, 18)  11178       ['activation_47[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_46 (Concatenate)   (None, 16, 16, 432)  0           ['concatenate_45[0][0]',         \n",
      "                                                                  'separable_conv2d_47[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_48 (BatchN  (None, 16, 16, 432)  1728       ['concatenate_46[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_48 (Activation)     (None, 16, 16, 432)  0           ['batch_normalization_48[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_48 (Separable  (None, 16, 16, 18)  11664       ['activation_48[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_47 (Concatenate)   (None, 16, 16, 450)  0           ['concatenate_46[0][0]',         \n",
      "                                                                  'separable_conv2d_48[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_49 (BatchN  (None, 16, 16, 450)  1800       ['concatenate_47[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_49 (Activation)     (None, 16, 16, 450)  0           ['batch_normalization_49[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_49 (Separable  (None, 16, 16, 18)  8550        ['activation_49[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " average_pooling2d_1 (AveragePo  (None, 8, 8, 18)    0           ['separable_conv2d_49[0][0]']    \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " batch_normalization_50 (BatchN  (None, 8, 8, 18)    72          ['average_pooling2d_1[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_50 (Activation)     (None, 8, 8, 18)     0           ['batch_normalization_50[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_50 (Separable  (None, 8, 8, 18)    486         ['activation_50[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_48 (Concatenate)   (None, 8, 8, 36)     0           ['average_pooling2d_1[0][0]',    \n",
      "                                                                  'separable_conv2d_50[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_51 (BatchN  (None, 8, 8, 36)    144         ['concatenate_48[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_51 (Activation)     (None, 8, 8, 36)     0           ['batch_normalization_51[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_51 (Separable  (None, 8, 8, 18)    972         ['activation_51[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_49 (Concatenate)   (None, 8, 8, 54)     0           ['concatenate_48[0][0]',         \n",
      "                                                                  'separable_conv2d_51[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_52 (BatchN  (None, 8, 8, 54)    216         ['concatenate_49[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_52 (Activation)     (None, 8, 8, 54)     0           ['batch_normalization_52[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_52 (Separable  (None, 8, 8, 18)    1458        ['activation_52[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_50 (Concatenate)   (None, 8, 8, 72)     0           ['concatenate_49[0][0]',         \n",
      "                                                                  'separable_conv2d_52[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_53 (BatchN  (None, 8, 8, 72)    288         ['concatenate_50[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_53 (Activation)     (None, 8, 8, 72)     0           ['batch_normalization_53[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_53 (Separable  (None, 8, 8, 18)    1944        ['activation_53[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_51 (Concatenate)   (None, 8, 8, 90)     0           ['concatenate_50[0][0]',         \n",
      "                                                                  'separable_conv2d_53[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_54 (BatchN  (None, 8, 8, 90)    360         ['concatenate_51[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_54 (Activation)     (None, 8, 8, 90)     0           ['batch_normalization_54[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_54 (Separable  (None, 8, 8, 18)    2430        ['activation_54[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_52 (Concatenate)   (None, 8, 8, 108)    0           ['concatenate_51[0][0]',         \n",
      "                                                                  'separable_conv2d_54[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_55 (BatchN  (None, 8, 8, 108)   432         ['concatenate_52[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_55 (Activation)     (None, 8, 8, 108)    0           ['batch_normalization_55[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_55 (Separable  (None, 8, 8, 18)    2916        ['activation_55[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_53 (Concatenate)   (None, 8, 8, 126)    0           ['concatenate_52[0][0]',         \n",
      "                                                                  'separable_conv2d_55[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_56 (BatchN  (None, 8, 8, 126)   504         ['concatenate_53[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_56 (Activation)     (None, 8, 8, 126)    0           ['batch_normalization_56[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_56 (Separable  (None, 8, 8, 18)    3402        ['activation_56[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_54 (Concatenate)   (None, 8, 8, 144)    0           ['concatenate_53[0][0]',         \n",
      "                                                                  'separable_conv2d_56[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_57 (BatchN  (None, 8, 8, 144)   576         ['concatenate_54[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_57 (Activation)     (None, 8, 8, 144)    0           ['batch_normalization_57[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_57 (Separable  (None, 8, 8, 18)    3888        ['activation_57[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_55 (Concatenate)   (None, 8, 8, 162)    0           ['concatenate_54[0][0]',         \n",
      "                                                                  'separable_conv2d_57[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_58 (BatchN  (None, 8, 8, 162)   648         ['concatenate_55[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_58 (Activation)     (None, 8, 8, 162)    0           ['batch_normalization_58[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_58 (Separable  (None, 8, 8, 18)    4374        ['activation_58[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_56 (Concatenate)   (None, 8, 8, 180)    0           ['concatenate_55[0][0]',         \n",
      "                                                                  'separable_conv2d_58[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_59 (BatchN  (None, 8, 8, 180)   720         ['concatenate_56[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_59 (Activation)     (None, 8, 8, 180)    0           ['batch_normalization_59[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_59 (Separable  (None, 8, 8, 18)    4860        ['activation_59[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_57 (Concatenate)   (None, 8, 8, 198)    0           ['concatenate_56[0][0]',         \n",
      "                                                                  'separable_conv2d_59[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_60 (BatchN  (None, 8, 8, 198)   792         ['concatenate_57[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_60 (Activation)     (None, 8, 8, 198)    0           ['batch_normalization_60[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_60 (Separable  (None, 8, 8, 18)    5346        ['activation_60[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_58 (Concatenate)   (None, 8, 8, 216)    0           ['concatenate_57[0][0]',         \n",
      "                                                                  'separable_conv2d_60[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_61 (BatchN  (None, 8, 8, 216)   864         ['concatenate_58[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_61 (Activation)     (None, 8, 8, 216)    0           ['batch_normalization_61[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_61 (Separable  (None, 8, 8, 18)    5832        ['activation_61[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_59 (Concatenate)   (None, 8, 8, 234)    0           ['concatenate_58[0][0]',         \n",
      "                                                                  'separable_conv2d_61[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_62 (BatchN  (None, 8, 8, 234)   936         ['concatenate_59[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_62 (Activation)     (None, 8, 8, 234)    0           ['batch_normalization_62[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_62 (Separable  (None, 8, 8, 18)    6318        ['activation_62[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_60 (Concatenate)   (None, 8, 8, 252)    0           ['concatenate_59[0][0]',         \n",
      "                                                                  'separable_conv2d_62[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_63 (BatchN  (None, 8, 8, 252)   1008        ['concatenate_60[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_63 (Activation)     (None, 8, 8, 252)    0           ['batch_normalization_63[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_63 (Separable  (None, 8, 8, 18)    6804        ['activation_63[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_61 (Concatenate)   (None, 8, 8, 270)    0           ['concatenate_60[0][0]',         \n",
      "                                                                  'separable_conv2d_63[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_64 (BatchN  (None, 8, 8, 270)   1080        ['concatenate_61[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_64 (Activation)     (None, 8, 8, 270)    0           ['batch_normalization_64[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_64 (Separable  (None, 8, 8, 18)    7290        ['activation_64[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_62 (Concatenate)   (None, 8, 8, 288)    0           ['concatenate_61[0][0]',         \n",
      "                                                                  'separable_conv2d_64[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_65 (BatchN  (None, 8, 8, 288)   1152        ['concatenate_62[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_65 (Activation)     (None, 8, 8, 288)    0           ['batch_normalization_65[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_65 (Separable  (None, 8, 8, 18)    7776        ['activation_65[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_63 (Concatenate)   (None, 8, 8, 306)    0           ['concatenate_62[0][0]',         \n",
      "                                                                  'separable_conv2d_65[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_66 (BatchN  (None, 8, 8, 306)   1224        ['concatenate_63[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_66 (Activation)     (None, 8, 8, 306)    0           ['batch_normalization_66[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_66 (Separable  (None, 8, 8, 18)    8262        ['activation_66[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_64 (Concatenate)   (None, 8, 8, 324)    0           ['concatenate_63[0][0]',         \n",
      "                                                                  'separable_conv2d_66[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_67 (BatchN  (None, 8, 8, 324)   1296        ['concatenate_64[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_67 (Activation)     (None, 8, 8, 324)    0           ['batch_normalization_67[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_67 (Separable  (None, 8, 8, 18)    8748        ['activation_67[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_65 (Concatenate)   (None, 8, 8, 342)    0           ['concatenate_64[0][0]',         \n",
      "                                                                  'separable_conv2d_67[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_68 (BatchN  (None, 8, 8, 342)   1368        ['concatenate_65[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_68 (Activation)     (None, 8, 8, 342)    0           ['batch_normalization_68[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_68 (Separable  (None, 8, 8, 18)    9234        ['activation_68[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_66 (Concatenate)   (None, 8, 8, 360)    0           ['concatenate_65[0][0]',         \n",
      "                                                                  'separable_conv2d_68[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_69 (BatchN  (None, 8, 8, 360)   1440        ['concatenate_66[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_69 (Activation)     (None, 8, 8, 360)    0           ['batch_normalization_69[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_69 (Separable  (None, 8, 8, 18)    9720        ['activation_69[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_67 (Concatenate)   (None, 8, 8, 378)    0           ['concatenate_66[0][0]',         \n",
      "                                                                  'separable_conv2d_69[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_70 (BatchN  (None, 8, 8, 378)   1512        ['concatenate_67[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_70 (Activation)     (None, 8, 8, 378)    0           ['batch_normalization_70[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_70 (Separable  (None, 8, 8, 18)    10206       ['activation_70[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_68 (Concatenate)   (None, 8, 8, 396)    0           ['concatenate_67[0][0]',         \n",
      "                                                                  'separable_conv2d_70[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_71 (BatchN  (None, 8, 8, 396)   1584        ['concatenate_68[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_71 (Activation)     (None, 8, 8, 396)    0           ['batch_normalization_71[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_71 (Separable  (None, 8, 8, 18)    10692       ['activation_71[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_69 (Concatenate)   (None, 8, 8, 414)    0           ['concatenate_68[0][0]',         \n",
      "                                                                  'separable_conv2d_71[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_72 (BatchN  (None, 8, 8, 414)   1656        ['concatenate_69[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_72 (Activation)     (None, 8, 8, 414)    0           ['batch_normalization_72[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_72 (Separable  (None, 8, 8, 18)    11178       ['activation_72[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_70 (Concatenate)   (None, 8, 8, 432)    0           ['concatenate_69[0][0]',         \n",
      "                                                                  'separable_conv2d_72[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_73 (BatchN  (None, 8, 8, 432)   1728        ['concatenate_70[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_73 (Activation)     (None, 8, 8, 432)    0           ['batch_normalization_73[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_73 (Separable  (None, 8, 8, 18)    11664       ['activation_73[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_71 (Concatenate)   (None, 8, 8, 450)    0           ['concatenate_70[0][0]',         \n",
      "                                                                  'separable_conv2d_73[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_74 (BatchN  (None, 8, 8, 450)   1800        ['concatenate_71[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_74 (Activation)     (None, 8, 8, 450)    0           ['batch_normalization_74[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_74 (Separable  (None, 8, 8, 18)    8550        ['activation_74[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " average_pooling2d_2 (AveragePo  (None, 4, 4, 18)    0           ['separable_conv2d_74[0][0]']    \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " batch_normalization_75 (BatchN  (None, 4, 4, 18)    72          ['average_pooling2d_2[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_75 (Activation)     (None, 4, 4, 18)     0           ['batch_normalization_75[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_75 (Separable  (None, 4, 4, 18)    486         ['activation_75[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_72 (Concatenate)   (None, 4, 4, 36)     0           ['average_pooling2d_2[0][0]',    \n",
      "                                                                  'separable_conv2d_75[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_76 (BatchN  (None, 4, 4, 36)    144         ['concatenate_72[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_76 (Activation)     (None, 4, 4, 36)     0           ['batch_normalization_76[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_76 (Separable  (None, 4, 4, 18)    972         ['activation_76[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_73 (Concatenate)   (None, 4, 4, 54)     0           ['concatenate_72[0][0]',         \n",
      "                                                                  'separable_conv2d_76[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_77 (BatchN  (None, 4, 4, 54)    216         ['concatenate_73[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_77 (Activation)     (None, 4, 4, 54)     0           ['batch_normalization_77[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_77 (Separable  (None, 4, 4, 18)    1458        ['activation_77[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_74 (Concatenate)   (None, 4, 4, 72)     0           ['concatenate_73[0][0]',         \n",
      "                                                                  'separable_conv2d_77[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_78 (BatchN  (None, 4, 4, 72)    288         ['concatenate_74[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_78 (Activation)     (None, 4, 4, 72)     0           ['batch_normalization_78[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_78 (Separable  (None, 4, 4, 18)    1944        ['activation_78[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_75 (Concatenate)   (None, 4, 4, 90)     0           ['concatenate_74[0][0]',         \n",
      "                                                                  'separable_conv2d_78[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_79 (BatchN  (None, 4, 4, 90)    360         ['concatenate_75[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_79 (Activation)     (None, 4, 4, 90)     0           ['batch_normalization_79[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_79 (Separable  (None, 4, 4, 18)    2430        ['activation_79[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_76 (Concatenate)   (None, 4, 4, 108)    0           ['concatenate_75[0][0]',         \n",
      "                                                                  'separable_conv2d_79[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_80 (BatchN  (None, 4, 4, 108)   432         ['concatenate_76[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_80 (Activation)     (None, 4, 4, 108)    0           ['batch_normalization_80[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_80 (Separable  (None, 4, 4, 18)    2916        ['activation_80[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_77 (Concatenate)   (None, 4, 4, 126)    0           ['concatenate_76[0][0]',         \n",
      "                                                                  'separable_conv2d_80[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_81 (BatchN  (None, 4, 4, 126)   504         ['concatenate_77[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_81 (Activation)     (None, 4, 4, 126)    0           ['batch_normalization_81[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_81 (Separable  (None, 4, 4, 18)    3402        ['activation_81[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_78 (Concatenate)   (None, 4, 4, 144)    0           ['concatenate_77[0][0]',         \n",
      "                                                                  'separable_conv2d_81[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_82 (BatchN  (None, 4, 4, 144)   576         ['concatenate_78[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_82 (Activation)     (None, 4, 4, 144)    0           ['batch_normalization_82[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_82 (Separable  (None, 4, 4, 18)    3888        ['activation_82[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_79 (Concatenate)   (None, 4, 4, 162)    0           ['concatenate_78[0][0]',         \n",
      "                                                                  'separable_conv2d_82[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_83 (BatchN  (None, 4, 4, 162)   648         ['concatenate_79[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_83 (Activation)     (None, 4, 4, 162)    0           ['batch_normalization_83[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_83 (Separable  (None, 4, 4, 18)    4374        ['activation_83[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_80 (Concatenate)   (None, 4, 4, 180)    0           ['concatenate_79[0][0]',         \n",
      "                                                                  'separable_conv2d_83[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_84 (BatchN  (None, 4, 4, 180)   720         ['concatenate_80[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_84 (Activation)     (None, 4, 4, 180)    0           ['batch_normalization_84[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_84 (Separable  (None, 4, 4, 18)    4860        ['activation_84[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_81 (Concatenate)   (None, 4, 4, 198)    0           ['concatenate_80[0][0]',         \n",
      "                                                                  'separable_conv2d_84[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_85 (BatchN  (None, 4, 4, 198)   792         ['concatenate_81[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_85 (Activation)     (None, 4, 4, 198)    0           ['batch_normalization_85[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_85 (Separable  (None, 4, 4, 18)    5346        ['activation_85[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_82 (Concatenate)   (None, 4, 4, 216)    0           ['concatenate_81[0][0]',         \n",
      "                                                                  'separable_conv2d_85[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_86 (BatchN  (None, 4, 4, 216)   864         ['concatenate_82[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_86 (Activation)     (None, 4, 4, 216)    0           ['batch_normalization_86[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_86 (Separable  (None, 4, 4, 18)    5832        ['activation_86[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_83 (Concatenate)   (None, 4, 4, 234)    0           ['concatenate_82[0][0]',         \n",
      "                                                                  'separable_conv2d_86[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_87 (BatchN  (None, 4, 4, 234)   936         ['concatenate_83[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_87 (Activation)     (None, 4, 4, 234)    0           ['batch_normalization_87[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_87 (Separable  (None, 4, 4, 18)    6318        ['activation_87[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_84 (Concatenate)   (None, 4, 4, 252)    0           ['concatenate_83[0][0]',         \n",
      "                                                                  'separable_conv2d_87[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_88 (BatchN  (None, 4, 4, 252)   1008        ['concatenate_84[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_88 (Activation)     (None, 4, 4, 252)    0           ['batch_normalization_88[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_88 (Separable  (None, 4, 4, 18)    6804        ['activation_88[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_85 (Concatenate)   (None, 4, 4, 270)    0           ['concatenate_84[0][0]',         \n",
      "                                                                  'separable_conv2d_88[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_89 (BatchN  (None, 4, 4, 270)   1080        ['concatenate_85[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_89 (Activation)     (None, 4, 4, 270)    0           ['batch_normalization_89[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_89 (Separable  (None, 4, 4, 18)    7290        ['activation_89[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_86 (Concatenate)   (None, 4, 4, 288)    0           ['concatenate_85[0][0]',         \n",
      "                                                                  'separable_conv2d_89[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_90 (BatchN  (None, 4, 4, 288)   1152        ['concatenate_86[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_90 (Activation)     (None, 4, 4, 288)    0           ['batch_normalization_90[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_90 (Separable  (None, 4, 4, 18)    7776        ['activation_90[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_87 (Concatenate)   (None, 4, 4, 306)    0           ['concatenate_86[0][0]',         \n",
      "                                                                  'separable_conv2d_90[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_91 (BatchN  (None, 4, 4, 306)   1224        ['concatenate_87[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_91 (Activation)     (None, 4, 4, 306)    0           ['batch_normalization_91[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_91 (Separable  (None, 4, 4, 18)    8262        ['activation_91[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_88 (Concatenate)   (None, 4, 4, 324)    0           ['concatenate_87[0][0]',         \n",
      "                                                                  'separable_conv2d_91[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_92 (BatchN  (None, 4, 4, 324)   1296        ['concatenate_88[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_92 (Activation)     (None, 4, 4, 324)    0           ['batch_normalization_92[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_92 (Separable  (None, 4, 4, 18)    8748        ['activation_92[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_89 (Concatenate)   (None, 4, 4, 342)    0           ['concatenate_88[0][0]',         \n",
      "                                                                  'separable_conv2d_92[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_93 (BatchN  (None, 4, 4, 342)   1368        ['concatenate_89[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_93 (Activation)     (None, 4, 4, 342)    0           ['batch_normalization_93[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_93 (Separable  (None, 4, 4, 18)    9234        ['activation_93[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_90 (Concatenate)   (None, 4, 4, 360)    0           ['concatenate_89[0][0]',         \n",
      "                                                                  'separable_conv2d_93[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_94 (BatchN  (None, 4, 4, 360)   1440        ['concatenate_90[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_94 (Activation)     (None, 4, 4, 360)    0           ['batch_normalization_94[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_94 (Separable  (None, 4, 4, 18)    9720        ['activation_94[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_91 (Concatenate)   (None, 4, 4, 378)    0           ['concatenate_90[0][0]',         \n",
      "                                                                  'separable_conv2d_94[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_95 (BatchN  (None, 4, 4, 378)   1512        ['concatenate_91[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_95 (Activation)     (None, 4, 4, 378)    0           ['batch_normalization_95[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_95 (Separable  (None, 4, 4, 18)    10206       ['activation_95[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_92 (Concatenate)   (None, 4, 4, 396)    0           ['concatenate_91[0][0]',         \n",
      "                                                                  'separable_conv2d_95[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_96 (BatchN  (None, 4, 4, 396)   1584        ['concatenate_92[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_96 (Activation)     (None, 4, 4, 396)    0           ['batch_normalization_96[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_96 (Separable  (None, 4, 4, 18)    10692       ['activation_96[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_93 (Concatenate)   (None, 4, 4, 414)    0           ['concatenate_92[0][0]',         \n",
      "                                                                  'separable_conv2d_96[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_97 (BatchN  (None, 4, 4, 414)   1656        ['concatenate_93[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_97 (Activation)     (None, 4, 4, 414)    0           ['batch_normalization_97[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_97 (Separable  (None, 4, 4, 18)    11178       ['activation_97[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_94 (Concatenate)   (None, 4, 4, 432)    0           ['concatenate_93[0][0]',         \n",
      "                                                                  'separable_conv2d_97[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_98 (BatchN  (None, 4, 4, 432)   1728        ['concatenate_94[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_98 (Activation)     (None, 4, 4, 432)    0           ['batch_normalization_98[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_98 (Separable  (None, 4, 4, 18)    11664       ['activation_98[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_95 (Concatenate)   (None, 4, 4, 450)    0           ['concatenate_94[0][0]',         \n",
      "                                                                  'separable_conv2d_98[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_99 (BatchN  (None, 4, 4, 450)   1800        ['concatenate_95[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_99 (Activation)     (None, 4, 4, 450)    0           ['batch_normalization_99[0][0]'] \n",
      "                                                                                                  \n",
      " average_pooling2d_3 (AveragePo  (None, 2, 2, 450)   0           ['activation_99[0][0]']          \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 1, 1, 10)     18010       ['average_pooling2d_3[0][0]']    \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 10)           0           ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 735,238\n",
      "Trainable params: 687,538\n",
      "Non-trainable params: 47,700\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6yxzwWpsOWOB"
   },
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    lr = 0.01\n",
    "    if epoch > 180:\n",
    "        lr *= 0.00001\n",
    "    elif epoch > 180:\n",
    "        lr *= 0.0001\n",
    "    elif epoch > 120:\n",
    "        lr *= 0.001\n",
    "    print('\\nLearning rate: ', lr)\n",
    "    return lr\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oE9bhz5zOWOB",
    "outputId": "6ab98a53-21a5-46cb-d1f7-8f5d6602d820"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 1.6611 - accuracy: 0.3933\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.10000, saving model to /content/drive/MyDrive/27 CNN on CIFR/Model Output/model_depthwise_best.h5\n",
      "390/390 [==============================] - 213s 488ms/step - loss: 1.6611 - accuracy: 0.3933 - val_loss: 2.4854 - val_accuracy: 0.1000\n",
      "Epoch 2/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 1.3546 - accuracy: 0.5116\n",
      "Epoch 00002: val_accuracy improved from 0.10000 to 0.44030, saving model to /content/drive/MyDrive/27 CNN on CIFR/Model Output/model_depthwise_best.h5\n",
      "390/390 [==============================] - 186s 476ms/step - loss: 1.3546 - accuracy: 0.5116 - val_loss: 1.5931 - val_accuracy: 0.4403\n",
      "Epoch 3/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 1.1941 - accuracy: 0.5744\n",
      "Epoch 00003: val_accuracy improved from 0.44030 to 0.58190, saving model to /content/drive/MyDrive/27 CNN on CIFR/Model Output/model_depthwise_best.h5\n",
      "390/390 [==============================] - 187s 480ms/step - loss: 1.1941 - accuracy: 0.5744 - val_loss: 1.2061 - val_accuracy: 0.5819\n",
      "Epoch 4/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 1.0892 - accuracy: 0.6118\n",
      "Epoch 00004: val_accuracy did not improve from 0.58190\n",
      "390/390 [==============================] - 185s 473ms/step - loss: 1.0892 - accuracy: 0.6118 - val_loss: 1.2877 - val_accuracy: 0.5664\n",
      "Epoch 5/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.9996 - accuracy: 0.6483\n",
      "Epoch 00005: val_accuracy did not improve from 0.58190\n",
      "390/390 [==============================] - 185s 473ms/step - loss: 0.9996 - accuracy: 0.6483 - val_loss: 1.3923 - val_accuracy: 0.5632\n",
      "Epoch 6/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.9275 - accuracy: 0.6723\n",
      "Epoch 00006: val_accuracy improved from 0.58190 to 0.60360, saving model to /content/drive/MyDrive/27 CNN on CIFR/Model Output/model_depthwise_best.h5\n",
      "390/390 [==============================] - 186s 476ms/step - loss: 0.9275 - accuracy: 0.6723 - val_loss: 1.1891 - val_accuracy: 0.6036\n",
      "Epoch 7/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.8729 - accuracy: 0.6930\n",
      "Epoch 00007: val_accuracy improved from 0.60360 to 0.66520, saving model to /content/drive/MyDrive/27 CNN on CIFR/Model Output/model_depthwise_best.h5\n",
      "390/390 [==============================] - 186s 476ms/step - loss: 0.8729 - accuracy: 0.6930 - val_loss: 0.9680 - val_accuracy: 0.6652\n",
      "Epoch 8/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.8270 - accuracy: 0.7095\n",
      "Epoch 00008: val_accuracy improved from 0.66520 to 0.70740, saving model to /content/drive/MyDrive/27 CNN on CIFR/Model Output/model_depthwise_best.h5\n",
      "390/390 [==============================] - 186s 476ms/step - loss: 0.8270 - accuracy: 0.7095 - val_loss: 0.8591 - val_accuracy: 0.7074\n",
      "Epoch 9/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.7892 - accuracy: 0.7253\n",
      "Epoch 00009: val_accuracy did not improve from 0.70740\n",
      "390/390 [==============================] - 185s 473ms/step - loss: 0.7892 - accuracy: 0.7253 - val_loss: 0.8957 - val_accuracy: 0.6951\n",
      "Epoch 10/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.7582 - accuracy: 0.7359\n",
      "Epoch 00010: val_accuracy did not improve from 0.70740\n",
      "390/390 [==============================] - 185s 473ms/step - loss: 0.7582 - accuracy: 0.7359 - val_loss: 1.1198 - val_accuracy: 0.6504\n",
      "Epoch 11/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.7295 - accuracy: 0.7472\n",
      "Epoch 00011: val_accuracy did not improve from 0.70740\n",
      "390/390 [==============================] - 185s 473ms/step - loss: 0.7295 - accuracy: 0.7472 - val_loss: 1.1119 - val_accuracy: 0.6518\n",
      "Epoch 12/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.7007 - accuracy: 0.7563\n",
      "Epoch 00012: val_accuracy improved from 0.70740 to 0.72800, saving model to /content/drive/MyDrive/27 CNN on CIFR/Model Output/model_depthwise_best.h5\n",
      "390/390 [==============================] - 186s 476ms/step - loss: 0.7007 - accuracy: 0.7563 - val_loss: 0.8076 - val_accuracy: 0.7280\n",
      "Epoch 13/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.6736 - accuracy: 0.7665\n",
      "Epoch 00013: val_accuracy improved from 0.72800 to 0.73580, saving model to /content/drive/MyDrive/27 CNN on CIFR/Model Output/model_depthwise_best.h5\n",
      "390/390 [==============================] - 186s 476ms/step - loss: 0.6736 - accuracy: 0.7665 - val_loss: 0.7925 - val_accuracy: 0.7358\n",
      "Epoch 14/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.6562 - accuracy: 0.7725\n",
      "Epoch 00014: val_accuracy did not improve from 0.73580\n",
      "390/390 [==============================] - 185s 473ms/step - loss: 0.6562 - accuracy: 0.7725 - val_loss: 0.8319 - val_accuracy: 0.7329\n",
      "Epoch 15/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.6376 - accuracy: 0.7785\n",
      "Epoch 00015: val_accuracy improved from 0.73580 to 0.73740, saving model to /content/drive/MyDrive/27 CNN on CIFR/Model Output/model_depthwise_best.h5\n",
      "390/390 [==============================] - 186s 476ms/step - loss: 0.6376 - accuracy: 0.7785 - val_loss: 0.8016 - val_accuracy: 0.7374\n",
      "Epoch 16/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.6226 - accuracy: 0.7860\n",
      "Epoch 00016: val_accuracy did not improve from 0.73740\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.6226 - accuracy: 0.7860 - val_loss: 1.3224 - val_accuracy: 0.6449\n",
      "Epoch 17/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.6068 - accuracy: 0.7887\n",
      "Epoch 00017: val_accuracy improved from 0.73740 to 0.75590, saving model to /content/drive/MyDrive/27 CNN on CIFR/Model Output/model_depthwise_best.h5\n",
      "390/390 [==============================] - 186s 478ms/step - loss: 0.6068 - accuracy: 0.7887 - val_loss: 0.7673 - val_accuracy: 0.7559\n",
      "Epoch 18/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.5910 - accuracy: 0.7963\n",
      "Epoch 00018: val_accuracy did not improve from 0.75590\n",
      "390/390 [==============================] - 185s 475ms/step - loss: 0.5910 - accuracy: 0.7963 - val_loss: 0.9394 - val_accuracy: 0.7052\n",
      "Epoch 19/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.5725 - accuracy: 0.8025\n",
      "Epoch 00019: val_accuracy did not improve from 0.75590\n",
      "390/390 [==============================] - 185s 475ms/step - loss: 0.5725 - accuracy: 0.8025 - val_loss: 0.7819 - val_accuracy: 0.7477\n",
      "Epoch 20/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.5653 - accuracy: 0.8048\n",
      "Epoch 00020: val_accuracy improved from 0.75590 to 0.77590, saving model to /content/drive/MyDrive/27 CNN on CIFR/Model Output/model_depthwise_best.h5\n",
      "390/390 [==============================] - 186s 477ms/step - loss: 0.5653 - accuracy: 0.8048 - val_loss: 0.6724 - val_accuracy: 0.7759\n",
      "Epoch 21/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.5480 - accuracy: 0.8122\n",
      "Epoch 00021: val_accuracy improved from 0.77590 to 0.78470, saving model to /content/drive/MyDrive/27 CNN on CIFR/Model Output/model_depthwise_best.h5\n",
      "390/390 [==============================] - 186s 477ms/step - loss: 0.5480 - accuracy: 0.8122 - val_loss: 0.6766 - val_accuracy: 0.7847\n",
      "Epoch 22/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.5346 - accuracy: 0.8178\n",
      "Epoch 00022: val_accuracy did not improve from 0.78470\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.5346 - accuracy: 0.8178 - val_loss: 0.7045 - val_accuracy: 0.7700\n",
      "Epoch 23/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.5234 - accuracy: 0.8214\n",
      "Epoch 00023: val_accuracy did not improve from 0.78470\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.5234 - accuracy: 0.8214 - val_loss: 0.7636 - val_accuracy: 0.7567\n",
      "Epoch 24/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.5171 - accuracy: 0.8228\n",
      "Epoch 00024: val_accuracy improved from 0.78470 to 0.78860, saving model to /content/drive/MyDrive/27 CNN on CIFR/Model Output/model_depthwise_best.h5\n",
      "390/390 [==============================] - 186s 478ms/step - loss: 0.5171 - accuracy: 0.8228 - val_loss: 0.6385 - val_accuracy: 0.7886\n",
      "Epoch 25/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.5052 - accuracy: 0.8278\n",
      "Epoch 00025: val_accuracy did not improve from 0.78860\n",
      "390/390 [==============================] - 185s 475ms/step - loss: 0.5052 - accuracy: 0.8278 - val_loss: 0.6897 - val_accuracy: 0.7789\n",
      "Epoch 26/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4941 - accuracy: 0.8304\n",
      "Epoch 00026: val_accuracy improved from 0.78860 to 0.80130, saving model to /content/drive/MyDrive/27 CNN on CIFR/Model Output/model_depthwise_best.h5\n",
      "390/390 [==============================] - 186s 477ms/step - loss: 0.4941 - accuracy: 0.8304 - val_loss: 0.6022 - val_accuracy: 0.8013\n",
      "Epoch 27/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4864 - accuracy: 0.8342\n",
      "Epoch 00027: val_accuracy improved from 0.80130 to 0.81040, saving model to /content/drive/MyDrive/27 CNN on CIFR/Model Output/model_depthwise_best.h5\n",
      "390/390 [==============================] - 186s 477ms/step - loss: 0.4864 - accuracy: 0.8342 - val_loss: 0.5614 - val_accuracy: 0.8104\n",
      "Epoch 28/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4770 - accuracy: 0.8360\n",
      "Epoch 00028: val_accuracy improved from 0.81040 to 0.81170, saving model to /content/drive/MyDrive/27 CNN on CIFR/Model Output/model_depthwise_best.h5\n",
      "390/390 [==============================] - 187s 478ms/step - loss: 0.4770 - accuracy: 0.8360 - val_loss: 0.5695 - val_accuracy: 0.8117\n",
      "Epoch 29/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4723 - accuracy: 0.8387\n",
      "Epoch 00029: val_accuracy did not improve from 0.81170\n",
      "390/390 [==============================] - 186s 476ms/step - loss: 0.4723 - accuracy: 0.8387 - val_loss: 0.6357 - val_accuracy: 0.8012\n",
      "Epoch 30/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4575 - accuracy: 0.8439\n",
      "Epoch 00030: val_accuracy did not improve from 0.81170\n",
      "390/390 [==============================] - 185s 475ms/step - loss: 0.4575 - accuracy: 0.8439 - val_loss: 0.6166 - val_accuracy: 0.7988\n",
      "Epoch 31/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4536 - accuracy: 0.8462\n",
      "Epoch 00031: val_accuracy did not improve from 0.81170\n",
      "390/390 [==============================] - 185s 475ms/step - loss: 0.4536 - accuracy: 0.8462 - val_loss: 0.7165 - val_accuracy: 0.7768\n",
      "Epoch 32/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4502 - accuracy: 0.8441\n",
      "Epoch 00032: val_accuracy improved from 0.81170 to 0.81380, saving model to /content/drive/MyDrive/27 CNN on CIFR/Model Output/model_depthwise_best.h5\n",
      "390/390 [==============================] - 186s 477ms/step - loss: 0.4502 - accuracy: 0.8441 - val_loss: 0.5728 - val_accuracy: 0.8138\n",
      "Epoch 33/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4376 - accuracy: 0.8493\n",
      "Epoch 00033: val_accuracy did not improve from 0.81380\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.4376 - accuracy: 0.8493 - val_loss: 0.6174 - val_accuracy: 0.8000\n",
      "Epoch 34/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4328 - accuracy: 0.8529\n",
      "Epoch 00034: val_accuracy improved from 0.81380 to 0.83510, saving model to /content/drive/MyDrive/27 CNN on CIFR/Model Output/model_depthwise_best.h5\n",
      "390/390 [==============================] - 186s 477ms/step - loss: 0.4328 - accuracy: 0.8529 - val_loss: 0.5036 - val_accuracy: 0.8351\n",
      "Epoch 35/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4249 - accuracy: 0.8558\n",
      "Epoch 00035: val_accuracy did not improve from 0.83510\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.4249 - accuracy: 0.8558 - val_loss: 0.6888 - val_accuracy: 0.7942\n",
      "Epoch 36/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4203 - accuracy: 0.8568\n",
      "Epoch 00036: val_accuracy did not improve from 0.83510\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.4203 - accuracy: 0.8568 - val_loss: 0.6085 - val_accuracy: 0.8138\n",
      "Epoch 37/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4144 - accuracy: 0.8584\n",
      "Epoch 00037: val_accuracy did not improve from 0.83510\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.4144 - accuracy: 0.8584 - val_loss: 0.6019 - val_accuracy: 0.8113\n",
      "Epoch 38/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4064 - accuracy: 0.8619\n",
      "Epoch 00038: val_accuracy did not improve from 0.83510\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.4064 - accuracy: 0.8619 - val_loss: 0.7022 - val_accuracy: 0.7783\n",
      "Epoch 39/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.4014 - accuracy: 0.8621\n",
      "Epoch 00039: val_accuracy did not improve from 0.83510\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.4014 - accuracy: 0.8621 - val_loss: 0.5220 - val_accuracy: 0.8300\n",
      "Epoch 40/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3948 - accuracy: 0.8648\n",
      "Epoch 00040: val_accuracy did not improve from 0.83510\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.3948 - accuracy: 0.8648 - val_loss: 0.5753 - val_accuracy: 0.8204\n",
      "Epoch 41/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3955 - accuracy: 0.8652\n",
      "Epoch 00041: val_accuracy did not improve from 0.83510\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.3955 - accuracy: 0.8652 - val_loss: 0.5653 - val_accuracy: 0.8261\n",
      "Epoch 42/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3884 - accuracy: 0.8683\n",
      "Epoch 00042: val_accuracy improved from 0.83510 to 0.83780, saving model to /content/drive/MyDrive/27 CNN on CIFR/Model Output/model_depthwise_best.h5\n",
      "390/390 [==============================] - 186s 477ms/step - loss: 0.3884 - accuracy: 0.8683 - val_loss: 0.5248 - val_accuracy: 0.8378\n",
      "Epoch 43/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3811 - accuracy: 0.8712\n",
      "Epoch 00043: val_accuracy did not improve from 0.83780\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.3811 - accuracy: 0.8712 - val_loss: 0.6033 - val_accuracy: 0.8204\n",
      "Epoch 44/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3741 - accuracy: 0.8725\n",
      "Epoch 00044: val_accuracy did not improve from 0.83780\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.3741 - accuracy: 0.8725 - val_loss: 0.8690 - val_accuracy: 0.7456\n",
      "Epoch 45/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3683 - accuracy: 0.8747\n",
      "Epoch 00045: val_accuracy did not improve from 0.83780\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.3683 - accuracy: 0.8747 - val_loss: 0.5235 - val_accuracy: 0.8307\n",
      "Epoch 46/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3729 - accuracy: 0.8727\n",
      "Epoch 00046: val_accuracy improved from 0.83780 to 0.84870, saving model to /content/drive/MyDrive/27 CNN on CIFR/Model Output/model_depthwise_best.h5\n",
      "390/390 [==============================] - 186s 476ms/step - loss: 0.3729 - accuracy: 0.8727 - val_loss: 0.4859 - val_accuracy: 0.8487\n",
      "Epoch 47/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3596 - accuracy: 0.8780\n",
      "Epoch 00047: val_accuracy did not improve from 0.84870\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.3596 - accuracy: 0.8780 - val_loss: 0.7332 - val_accuracy: 0.7801\n",
      "Epoch 48/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3601 - accuracy: 0.8777\n",
      "Epoch 00048: val_accuracy did not improve from 0.84870\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.3601 - accuracy: 0.8777 - val_loss: 0.6883 - val_accuracy: 0.7981\n",
      "Epoch 49/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3506 - accuracy: 0.8823\n",
      "Epoch 00049: val_accuracy did not improve from 0.84870\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.3506 - accuracy: 0.8823 - val_loss: 0.6375 - val_accuracy: 0.8092\n",
      "Epoch 50/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3468 - accuracy: 0.8813\n",
      "Epoch 00050: val_accuracy improved from 0.84870 to 0.85650, saving model to /content/drive/MyDrive/27 CNN on CIFR/Model Output/model_depthwise_best.h5\n",
      "390/390 [==============================] - 186s 476ms/step - loss: 0.3468 - accuracy: 0.8813 - val_loss: 0.4561 - val_accuracy: 0.8565\n",
      "Epoch 51/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3484 - accuracy: 0.8803\n",
      "Epoch 00051: val_accuracy did not improve from 0.85650\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.3484 - accuracy: 0.8803 - val_loss: 0.4718 - val_accuracy: 0.8532\n",
      "Epoch 52/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3394 - accuracy: 0.8841\n",
      "Epoch 00052: val_accuracy did not improve from 0.85650\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.3394 - accuracy: 0.8841 - val_loss: 0.5196 - val_accuracy: 0.8380\n",
      "Epoch 53/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3427 - accuracy: 0.8835\n",
      "Epoch 00053: val_accuracy did not improve from 0.85650\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.3427 - accuracy: 0.8835 - val_loss: 0.4767 - val_accuracy: 0.8502\n",
      "Epoch 54/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3389 - accuracy: 0.8851\n",
      "Epoch 00054: val_accuracy did not improve from 0.85650\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.3389 - accuracy: 0.8851 - val_loss: 0.4907 - val_accuracy: 0.8464\n",
      "Epoch 55/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3310 - accuracy: 0.8873\n",
      "Epoch 00055: val_accuracy did not improve from 0.85650\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.3310 - accuracy: 0.8873 - val_loss: 0.5026 - val_accuracy: 0.8434\n",
      "Epoch 56/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3328 - accuracy: 0.8865\n",
      "Epoch 00056: val_accuracy did not improve from 0.85650\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.3328 - accuracy: 0.8865 - val_loss: 0.5509 - val_accuracy: 0.8372\n",
      "Epoch 57/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3220 - accuracy: 0.8903\n",
      "Epoch 00057: val_accuracy did not improve from 0.85650\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.3220 - accuracy: 0.8903 - val_loss: 0.5407 - val_accuracy: 0.8311\n",
      "Epoch 58/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3231 - accuracy: 0.8913\n",
      "Epoch 00058: val_accuracy did not improve from 0.85650\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.3231 - accuracy: 0.8913 - val_loss: 0.5350 - val_accuracy: 0.8309\n",
      "Epoch 59/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3213 - accuracy: 0.8894\n",
      "Epoch 00059: val_accuracy did not improve from 0.85650\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.3213 - accuracy: 0.8894 - val_loss: 0.4847 - val_accuracy: 0.8512\n",
      "Epoch 60/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3184 - accuracy: 0.8928\n",
      "Epoch 00060: val_accuracy did not improve from 0.85650\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.3184 - accuracy: 0.8928 - val_loss: 0.4903 - val_accuracy: 0.8479\n",
      "Epoch 61/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3123 - accuracy: 0.8939\n",
      "Epoch 00061: val_accuracy did not improve from 0.85650\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.3123 - accuracy: 0.8939 - val_loss: 0.4960 - val_accuracy: 0.8449\n",
      "Epoch 62/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3094 - accuracy: 0.8940\n",
      "Epoch 00062: val_accuracy did not improve from 0.85650\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.3094 - accuracy: 0.8940 - val_loss: 0.6458 - val_accuracy: 0.8063\n",
      "Epoch 63/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3052 - accuracy: 0.8964\n",
      "Epoch 00063: val_accuracy improved from 0.85650 to 0.86140, saving model to /content/drive/MyDrive/27 CNN on CIFR/Model Output/model_depthwise_best.h5\n",
      "390/390 [==============================] - 186s 476ms/step - loss: 0.3052 - accuracy: 0.8964 - val_loss: 0.4442 - val_accuracy: 0.8614\n",
      "Epoch 64/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.3030 - accuracy: 0.8974\n",
      "Epoch 00064: val_accuracy did not improve from 0.86140\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.3030 - accuracy: 0.8974 - val_loss: 0.5170 - val_accuracy: 0.8449\n",
      "Epoch 65/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2986 - accuracy: 0.8976\n",
      "Epoch 00065: val_accuracy did not improve from 0.86140\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.2986 - accuracy: 0.8976 - val_loss: 0.5547 - val_accuracy: 0.8332\n",
      "Epoch 66/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2990 - accuracy: 0.8982\n",
      "Epoch 00066: val_accuracy did not improve from 0.86140\n",
      "390/390 [==============================] - 185s 475ms/step - loss: 0.2990 - accuracy: 0.8982 - val_loss: 0.4758 - val_accuracy: 0.8509\n",
      "Epoch 67/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2962 - accuracy: 0.9004\n",
      "Epoch 00067: val_accuracy did not improve from 0.86140\n",
      "390/390 [==============================] - 185s 475ms/step - loss: 0.2962 - accuracy: 0.9004 - val_loss: 0.4557 - val_accuracy: 0.8557\n",
      "Epoch 68/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2916 - accuracy: 0.9011\n",
      "Epoch 00068: val_accuracy did not improve from 0.86140\n",
      "390/390 [==============================] - 185s 475ms/step - loss: 0.2916 - accuracy: 0.9011 - val_loss: 0.7715 - val_accuracy: 0.7927\n",
      "Epoch 69/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2885 - accuracy: 0.9021\n",
      "Epoch 00069: val_accuracy did not improve from 0.86140\n",
      "390/390 [==============================] - 185s 475ms/step - loss: 0.2885 - accuracy: 0.9021 - val_loss: 0.4993 - val_accuracy: 0.8471\n",
      "Epoch 70/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2864 - accuracy: 0.9027\n",
      "Epoch 00070: val_accuracy did not improve from 0.86140\n",
      "390/390 [==============================] - 185s 475ms/step - loss: 0.2864 - accuracy: 0.9027 - val_loss: 0.5505 - val_accuracy: 0.8313\n",
      "Epoch 71/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2836 - accuracy: 0.9045\n",
      "Epoch 00071: val_accuracy did not improve from 0.86140\n",
      "390/390 [==============================] - 185s 475ms/step - loss: 0.2836 - accuracy: 0.9045 - val_loss: 0.6850 - val_accuracy: 0.8117\n",
      "Epoch 72/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2823 - accuracy: 0.9050\n",
      "Epoch 00072: val_accuracy did not improve from 0.86140\n",
      "390/390 [==============================] - 185s 475ms/step - loss: 0.2823 - accuracy: 0.9050 - val_loss: 0.5027 - val_accuracy: 0.8506\n",
      "Epoch 73/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2736 - accuracy: 0.9064\n",
      "Epoch 00073: val_accuracy improved from 0.86140 to 0.86750, saving model to /content/drive/MyDrive/27 CNN on CIFR/Model Output/model_depthwise_best.h5\n",
      "390/390 [==============================] - 186s 477ms/step - loss: 0.2736 - accuracy: 0.9064 - val_loss: 0.4331 - val_accuracy: 0.8675\n",
      "Epoch 74/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2746 - accuracy: 0.9066\n",
      "Epoch 00074: val_accuracy did not improve from 0.86750\n",
      "390/390 [==============================] - 185s 475ms/step - loss: 0.2746 - accuracy: 0.9066 - val_loss: 0.4294 - val_accuracy: 0.8674\n",
      "Epoch 75/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2756 - accuracy: 0.9063\n",
      "Epoch 00075: val_accuracy did not improve from 0.86750\n",
      "390/390 [==============================] - 185s 475ms/step - loss: 0.2756 - accuracy: 0.9063 - val_loss: 0.8485 - val_accuracy: 0.7797\n",
      "Epoch 76/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2726 - accuracy: 0.9079\n",
      "Epoch 00076: val_accuracy improved from 0.86750 to 0.87120, saving model to /content/drive/MyDrive/27 CNN on CIFR/Model Output/model_depthwise_best.h5\n",
      "390/390 [==============================] - 186s 478ms/step - loss: 0.2726 - accuracy: 0.9079 - val_loss: 0.4230 - val_accuracy: 0.8712\n",
      "Epoch 77/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2687 - accuracy: 0.9097\n",
      "Epoch 00077: val_accuracy did not improve from 0.87120\n",
      "390/390 [==============================] - 185s 475ms/step - loss: 0.2687 - accuracy: 0.9097 - val_loss: 0.5496 - val_accuracy: 0.8366\n",
      "Epoch 78/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2659 - accuracy: 0.9096\n",
      "Epoch 00078: val_accuracy did not improve from 0.87120\n",
      "390/390 [==============================] - 185s 475ms/step - loss: 0.2659 - accuracy: 0.9096 - val_loss: 0.4533 - val_accuracy: 0.8633\n",
      "Epoch 79/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2608 - accuracy: 0.9130\n",
      "Epoch 00079: val_accuracy did not improve from 0.87120\n",
      "390/390 [==============================] - 185s 475ms/step - loss: 0.2608 - accuracy: 0.9130 - val_loss: 0.6002 - val_accuracy: 0.8314\n",
      "Epoch 80/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2655 - accuracy: 0.9103\n",
      "Epoch 00080: val_accuracy did not improve from 0.87120\n",
      "390/390 [==============================] - 185s 475ms/step - loss: 0.2655 - accuracy: 0.9103 - val_loss: 0.5377 - val_accuracy: 0.8468\n",
      "Epoch 81/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2565 - accuracy: 0.9142\n",
      "Epoch 00081: val_accuracy did not improve from 0.87120\n",
      "390/390 [==============================] - 185s 475ms/step - loss: 0.2565 - accuracy: 0.9142 - val_loss: 0.4915 - val_accuracy: 0.8548\n",
      "Epoch 82/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2594 - accuracy: 0.9130\n",
      "Epoch 00082: val_accuracy did not improve from 0.87120\n",
      "390/390 [==============================] - 185s 475ms/step - loss: 0.2594 - accuracy: 0.9130 - val_loss: 0.4888 - val_accuracy: 0.8499\n",
      "Epoch 83/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2582 - accuracy: 0.9135\n",
      "Epoch 00083: val_accuracy did not improve from 0.87120\n",
      "390/390 [==============================] - 185s 475ms/step - loss: 0.2582 - accuracy: 0.9135 - val_loss: 0.4257 - val_accuracy: 0.8664\n",
      "Epoch 84/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2534 - accuracy: 0.9142\n",
      "Epoch 00084: val_accuracy did not improve from 0.87120\n",
      "390/390 [==============================] - 185s 475ms/step - loss: 0.2534 - accuracy: 0.9142 - val_loss: 0.4979 - val_accuracy: 0.8544\n",
      "Epoch 85/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2513 - accuracy: 0.9148\n",
      "Epoch 00085: val_accuracy did not improve from 0.87120\n",
      "390/390 [==============================] - 185s 475ms/step - loss: 0.2513 - accuracy: 0.9148 - val_loss: 0.4249 - val_accuracy: 0.8708\n",
      "Epoch 86/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2527 - accuracy: 0.9138\n",
      "Epoch 00086: val_accuracy did not improve from 0.87120\n",
      "390/390 [==============================] - 185s 475ms/step - loss: 0.2527 - accuracy: 0.9138 - val_loss: 0.4355 - val_accuracy: 0.8683\n",
      "Epoch 87/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2445 - accuracy: 0.9181\n",
      "Epoch 00087: val_accuracy improved from 0.87120 to 0.87600, saving model to /content/drive/MyDrive/27 CNN on CIFR/Model Output/model_depthwise_best.h5\n",
      "390/390 [==============================] - 187s 478ms/step - loss: 0.2445 - accuracy: 0.9181 - val_loss: 0.4017 - val_accuracy: 0.8760\n",
      "Epoch 88/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2493 - accuracy: 0.9163\n",
      "Epoch 00088: val_accuracy did not improve from 0.87600\n",
      "390/390 [==============================] - 185s 475ms/step - loss: 0.2493 - accuracy: 0.9163 - val_loss: 0.4362 - val_accuracy: 0.8642\n",
      "Epoch 89/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2470 - accuracy: 0.9151\n",
      "Epoch 00089: val_accuracy did not improve from 0.87600\n",
      "390/390 [==============================] - 185s 475ms/step - loss: 0.2470 - accuracy: 0.9151 - val_loss: 0.5800 - val_accuracy: 0.8386\n",
      "Epoch 90/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2393 - accuracy: 0.9189\n",
      "Epoch 00090: val_accuracy did not improve from 0.87600\n",
      "390/390 [==============================] - 185s 475ms/step - loss: 0.2393 - accuracy: 0.9189 - val_loss: 0.5216 - val_accuracy: 0.8492\n",
      "Epoch 91/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2383 - accuracy: 0.9192\n",
      "Epoch 00091: val_accuracy did not improve from 0.87600\n",
      "390/390 [==============================] - 185s 475ms/step - loss: 0.2383 - accuracy: 0.9192 - val_loss: 0.4995 - val_accuracy: 0.8551\n",
      "Epoch 92/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2369 - accuracy: 0.9211\n",
      "Epoch 00092: val_accuracy did not improve from 0.87600\n",
      "390/390 [==============================] - 185s 475ms/step - loss: 0.2369 - accuracy: 0.9211 - val_loss: 0.4381 - val_accuracy: 0.8721\n",
      "Epoch 93/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2386 - accuracy: 0.9191\n",
      "Epoch 00093: val_accuracy did not improve from 0.87600\n",
      "390/390 [==============================] - 185s 475ms/step - loss: 0.2386 - accuracy: 0.9191 - val_loss: 0.5462 - val_accuracy: 0.8428\n",
      "Epoch 94/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2325 - accuracy: 0.9223\n",
      "Epoch 00094: val_accuracy did not improve from 0.87600\n",
      "390/390 [==============================] - 185s 475ms/step - loss: 0.2325 - accuracy: 0.9223 - val_loss: 0.4295 - val_accuracy: 0.8696\n",
      "Epoch 95/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2300 - accuracy: 0.9218\n",
      "Epoch 00095: val_accuracy did not improve from 0.87600\n",
      "390/390 [==============================] - 185s 475ms/step - loss: 0.2300 - accuracy: 0.9218 - val_loss: 0.4453 - val_accuracy: 0.8694\n",
      "Epoch 96/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2321 - accuracy: 0.9201\n",
      "Epoch 00096: val_accuracy did not improve from 0.87600\n",
      "390/390 [==============================] - 185s 475ms/step - loss: 0.2321 - accuracy: 0.9201 - val_loss: 0.4882 - val_accuracy: 0.8553\n",
      "Epoch 97/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2275 - accuracy: 0.9240\n",
      "Epoch 00097: val_accuracy did not improve from 0.87600\n",
      "390/390 [==============================] - 185s 475ms/step - loss: 0.2275 - accuracy: 0.9240 - val_loss: 0.4246 - val_accuracy: 0.8742\n",
      "Epoch 98/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2256 - accuracy: 0.9240\n",
      "Epoch 00098: val_accuracy improved from 0.87600 to 0.88100, saving model to /content/drive/MyDrive/27 CNN on CIFR/Model Output/model_depthwise_best.h5\n",
      "390/390 [==============================] - 187s 478ms/step - loss: 0.2256 - accuracy: 0.9240 - val_loss: 0.3941 - val_accuracy: 0.8810\n",
      "Epoch 99/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2284 - accuracy: 0.9227\n",
      "Epoch 00099: val_accuracy did not improve from 0.88100\n",
      "390/390 [==============================] - 185s 475ms/step - loss: 0.2284 - accuracy: 0.9227 - val_loss: 0.4796 - val_accuracy: 0.8569\n",
      "Epoch 100/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2228 - accuracy: 0.9240\n",
      "Epoch 00100: val_accuracy did not improve from 0.88100\n",
      "390/390 [==============================] - 185s 475ms/step - loss: 0.2228 - accuracy: 0.9240 - val_loss: 0.5142 - val_accuracy: 0.8510\n",
      "Epoch 101/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2239 - accuracy: 0.9239\n",
      "Epoch 00101: val_accuracy did not improve from 0.88100\n",
      "390/390 [==============================] - 185s 475ms/step - loss: 0.2239 - accuracy: 0.9239 - val_loss: 0.4337 - val_accuracy: 0.8683\n",
      "Epoch 102/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2221 - accuracy: 0.9250\n",
      "Epoch 00102: val_accuracy did not improve from 0.88100\n",
      "390/390 [==============================] - 185s 475ms/step - loss: 0.2221 - accuracy: 0.9250 - val_loss: 0.4211 - val_accuracy: 0.8762\n",
      "Epoch 103/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2224 - accuracy: 0.9254\n",
      "Epoch 00103: val_accuracy did not improve from 0.88100\n",
      "390/390 [==============================] - 185s 475ms/step - loss: 0.2224 - accuracy: 0.9254 - val_loss: 0.6233 - val_accuracy: 0.8363\n",
      "Epoch 104/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2206 - accuracy: 0.9262\n",
      "Epoch 00104: val_accuracy did not improve from 0.88100\n",
      "390/390 [==============================] - 185s 475ms/step - loss: 0.2206 - accuracy: 0.9262 - val_loss: 0.3982 - val_accuracy: 0.8796\n",
      "Epoch 105/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2201 - accuracy: 0.9248\n",
      "Epoch 00105: val_accuracy did not improve from 0.88100\n",
      "390/390 [==============================] - 185s 475ms/step - loss: 0.2201 - accuracy: 0.9248 - val_loss: 0.4496 - val_accuracy: 0.8704\n",
      "Epoch 106/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2161 - accuracy: 0.9271\n",
      "Epoch 00106: val_accuracy did not improve from 0.88100\n",
      "390/390 [==============================] - 185s 475ms/step - loss: 0.2161 - accuracy: 0.9271 - val_loss: 0.4074 - val_accuracy: 0.8775\n",
      "Epoch 107/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2133 - accuracy: 0.9283\n",
      "Epoch 00107: val_accuracy did not improve from 0.88100\n",
      "390/390 [==============================] - 185s 475ms/step - loss: 0.2133 - accuracy: 0.9283 - val_loss: 0.4887 - val_accuracy: 0.8607\n",
      "Epoch 108/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2105 - accuracy: 0.9295\n",
      "Epoch 00108: val_accuracy did not improve from 0.88100\n",
      "390/390 [==============================] - 185s 475ms/step - loss: 0.2105 - accuracy: 0.9295 - val_loss: 0.5088 - val_accuracy: 0.8609\n",
      "Epoch 109/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2123 - accuracy: 0.9286\n",
      "Epoch 00109: val_accuracy did not improve from 0.88100\n",
      "390/390 [==============================] - 185s 475ms/step - loss: 0.2123 - accuracy: 0.9286 - val_loss: 0.4558 - val_accuracy: 0.8659\n",
      "Epoch 110/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2109 - accuracy: 0.9299\n",
      "Epoch 00110: val_accuracy did not improve from 0.88100\n",
      "390/390 [==============================] - 185s 475ms/step - loss: 0.2109 - accuracy: 0.9299 - val_loss: 0.4864 - val_accuracy: 0.8626\n",
      "Epoch 111/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2069 - accuracy: 0.9289\n",
      "Epoch 00111: val_accuracy did not improve from 0.88100\n",
      "390/390 [==============================] - 185s 475ms/step - loss: 0.2069 - accuracy: 0.9289 - val_loss: 0.4284 - val_accuracy: 0.8766\n",
      "Epoch 112/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2030 - accuracy: 0.9318\n",
      "Epoch 00112: val_accuracy improved from 0.88100 to 0.88420, saving model to /content/drive/MyDrive/27 CNN on CIFR/Model Output/model_depthwise_best.h5\n",
      "390/390 [==============================] - 186s 478ms/step - loss: 0.2030 - accuracy: 0.9318 - val_loss: 0.3927 - val_accuracy: 0.8842\n",
      "Epoch 113/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2090 - accuracy: 0.9291\n",
      "Epoch 00113: val_accuracy did not improve from 0.88420\n",
      "390/390 [==============================] - 185s 475ms/step - loss: 0.2090 - accuracy: 0.9291 - val_loss: 0.6420 - val_accuracy: 0.8298\n",
      "Epoch 114/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2019 - accuracy: 0.9319\n",
      "Epoch 00114: val_accuracy did not improve from 0.88420\n",
      "390/390 [==============================] - 185s 475ms/step - loss: 0.2019 - accuracy: 0.9319 - val_loss: 0.4881 - val_accuracy: 0.8620\n",
      "Epoch 115/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2011 - accuracy: 0.9328\n",
      "Epoch 00115: val_accuracy did not improve from 0.88420\n",
      "390/390 [==============================] - 185s 475ms/step - loss: 0.2011 - accuracy: 0.9328 - val_loss: 0.4231 - val_accuracy: 0.8748\n",
      "Epoch 116/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2009 - accuracy: 0.9318\n",
      "Epoch 00116: val_accuracy did not improve from 0.88420\n",
      "390/390 [==============================] - 185s 475ms/step - loss: 0.2009 - accuracy: 0.9318 - val_loss: 0.4611 - val_accuracy: 0.8682\n",
      "Epoch 117/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1992 - accuracy: 0.9324\n",
      "Epoch 00117: val_accuracy did not improve from 0.88420\n",
      "390/390 [==============================] - 185s 475ms/step - loss: 0.1992 - accuracy: 0.9324 - val_loss: 0.5630 - val_accuracy: 0.8456\n",
      "Epoch 118/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1966 - accuracy: 0.9339\n",
      "Epoch 00118: val_accuracy did not improve from 0.88420\n",
      "390/390 [==============================] - 185s 475ms/step - loss: 0.1966 - accuracy: 0.9339 - val_loss: 0.4118 - val_accuracy: 0.8805\n",
      "Epoch 119/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.2016 - accuracy: 0.9324\n",
      "Epoch 00119: val_accuracy did not improve from 0.88420\n",
      "390/390 [==============================] - 185s 475ms/step - loss: 0.2016 - accuracy: 0.9324 - val_loss: 0.4062 - val_accuracy: 0.8792\n",
      "Epoch 120/120\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1941 - accuracy: 0.9339\n",
      "Epoch 00120: val_accuracy did not improve from 0.88420\n",
      "390/390 [==============================] - 185s 475ms/step - loss: 0.1941 - accuracy: 0.9339 - val_loss: 0.4500 - val_accuracy: 0.8703\n"
     ]
    }
   ],
   "source": [
    "epochs = 120\n",
    "\n",
    "decay_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.95, patience=5, \n",
    "                                                verbose=1, mode='auto', min_delta=0.001, \n",
    "                                                cooldown=0, min_lr=0.000001)\n",
    "\n",
    "callback_list = [checkpoint]\n",
    "\n",
    "history = model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),\\\n",
    "                    steps_per_epoch=X_train.shape[0] // batch_size, epochs=epochs,\\\n",
    "                    verbose=1,validation_data=(X_test,y_test),callbacks=callback_list)\n",
    "\n",
    "model.save_weights('/content/drive/MyDrive/27 CNN on CIFR/Model Output/model_depthwise_120Epoch.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 694
    },
    "id": "aA_JHSeTOWOC",
    "outputId": "f4d27993-4ed3-40b6-ca1f-801ae74c84f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 10s 31ms/step - loss: 0.3927 - accuracy: 0.8842\n",
      "Test loss: 0.39271241426467896\n",
      "Test accuracy: 0.8841999769210815\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Testing the model- Best Model\n",
    "model.load_weights('/content/drive/MyDrive/27 CNN on CIFR/Model Output/model_depthwise_best.h5')\n",
    "score = model.evaluate(X_test, y_test, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "print(\"--\"*25)\n",
    "\n",
    "\n",
    "summarize_diagnostics(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OVl4tNL8OWOC"
   },
   "source": [
    "▶ **Loading Model from 120th Epoch for further Training:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6L-gO_xHOWOC",
    "outputId": "49b49df9-2977-40e2-e989-ee6f4412398d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1614 - accuracy: 0.9471\n",
      "Epoch 00001: val_accuracy improved from 0.88420 to 0.89430, saving model to /content/drive/MyDrive/27 CNN on CIFR/Model Output/model_depthwise_best.h5\n",
      "390/390 [==============================] - 186s 477ms/step - loss: 0.1614 - accuracy: 0.9471 - val_loss: 0.3525 - val_accuracy: 0.8943\n",
      "Epoch 2/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1478 - accuracy: 0.9517\n",
      "Epoch 00002: val_accuracy improved from 0.89430 to 0.89690, saving model to /content/drive/MyDrive/27 CNN on CIFR/Model Output/model_depthwise_best.h5\n",
      "390/390 [==============================] - 186s 477ms/step - loss: 0.1478 - accuracy: 0.9517 - val_loss: 0.3574 - val_accuracy: 0.8969\n",
      "Epoch 3/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1426 - accuracy: 0.9534\n",
      "Epoch 00003: val_accuracy improved from 0.89690 to 0.89970, saving model to /content/drive/MyDrive/27 CNN on CIFR/Model Output/model_depthwise_best.h5\n",
      "390/390 [==============================] - 186s 477ms/step - loss: 0.1426 - accuracy: 0.9534 - val_loss: 0.3429 - val_accuracy: 0.8997\n",
      "Epoch 4/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1411 - accuracy: 0.9544\n",
      "Epoch 00004: val_accuracy did not improve from 0.89970\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.1411 - accuracy: 0.9544 - val_loss: 0.3505 - val_accuracy: 0.8978\n",
      "Epoch 5/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1412 - accuracy: 0.9541\n",
      "Epoch 00005: val_accuracy did not improve from 0.89970\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.1412 - accuracy: 0.9541 - val_loss: 0.3423 - val_accuracy: 0.8994\n",
      "Epoch 6/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1351 - accuracy: 0.9567\n",
      "Epoch 00006: val_accuracy improved from 0.89970 to 0.90020, saving model to /content/drive/MyDrive/27 CNN on CIFR/Model Output/model_depthwise_best.h5\n",
      "390/390 [==============================] - 187s 479ms/step - loss: 0.1351 - accuracy: 0.9567 - val_loss: 0.3438 - val_accuracy: 0.9002\n",
      "Epoch 7/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1367 - accuracy: 0.9551\n",
      "Epoch 00007: val_accuracy improved from 0.90020 to 0.90080, saving model to /content/drive/MyDrive/27 CNN on CIFR/Model Output/model_depthwise_best.h5\n",
      "390/390 [==============================] - 186s 477ms/step - loss: 0.1367 - accuracy: 0.9551 - val_loss: 0.3395 - val_accuracy: 0.9008\n",
      "Epoch 8/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1341 - accuracy: 0.9570\n",
      "Epoch 00008: val_accuracy did not improve from 0.90080\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.1341 - accuracy: 0.9570 - val_loss: 0.3465 - val_accuracy: 0.8992\n",
      "Epoch 9/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1357 - accuracy: 0.9566\n",
      "Epoch 00009: val_accuracy improved from 0.90080 to 0.90090, saving model to /content/drive/MyDrive/27 CNN on CIFR/Model Output/model_depthwise_best.h5\n",
      "390/390 [==============================] - 186s 477ms/step - loss: 0.1357 - accuracy: 0.9566 - val_loss: 0.3422 - val_accuracy: 0.9009\n",
      "Epoch 10/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1311 - accuracy: 0.9578\n",
      "Epoch 00010: val_accuracy did not improve from 0.90090\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.1311 - accuracy: 0.9578 - val_loss: 0.3462 - val_accuracy: 0.9005\n",
      "Epoch 11/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1326 - accuracy: 0.9575\n",
      "Epoch 00011: val_accuracy did not improve from 0.90090\n",
      "390/390 [==============================] - 185s 475ms/step - loss: 0.1326 - accuracy: 0.9575 - val_loss: 0.3407 - val_accuracy: 0.9004\n",
      "Epoch 12/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1270 - accuracy: 0.9594\n",
      "Epoch 00012: val_accuracy improved from 0.90090 to 0.90120, saving model to /content/drive/MyDrive/27 CNN on CIFR/Model Output/model_depthwise_best.h5\n",
      "390/390 [==============================] - 186s 477ms/step - loss: 0.1270 - accuracy: 0.9594 - val_loss: 0.3391 - val_accuracy: 0.9012\n",
      "Epoch 13/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1318 - accuracy: 0.9564\n",
      "Epoch 00013: val_accuracy did not improve from 0.90120\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.1318 - accuracy: 0.9564 - val_loss: 0.3484 - val_accuracy: 0.8998\n",
      "Epoch 14/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1307 - accuracy: 0.9576\n",
      "Epoch 00014: val_accuracy did not improve from 0.90120\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.1307 - accuracy: 0.9576 - val_loss: 0.3444 - val_accuracy: 0.9010\n",
      "Epoch 15/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1291 - accuracy: 0.9584\n",
      "Epoch 00015: val_accuracy improved from 0.90120 to 0.90300, saving model to /content/drive/MyDrive/27 CNN on CIFR/Model Output/model_depthwise_best.h5\n",
      "390/390 [==============================] - 186s 477ms/step - loss: 0.1291 - accuracy: 0.9584 - val_loss: 0.3345 - val_accuracy: 0.9030\n",
      "Epoch 16/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1308 - accuracy: 0.9575\n",
      "Epoch 00016: val_accuracy did not improve from 0.90300\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.1308 - accuracy: 0.9575 - val_loss: 0.3390 - val_accuracy: 0.9018\n",
      "Epoch 17/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1278 - accuracy: 0.9581\n",
      "Epoch 00017: val_accuracy did not improve from 0.90300\n",
      "390/390 [==============================] - 185s 475ms/step - loss: 0.1278 - accuracy: 0.9581 - val_loss: 0.3462 - val_accuracy: 0.9004\n",
      "Epoch 18/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1277 - accuracy: 0.9585\n",
      "Epoch 00018: val_accuracy did not improve from 0.90300\n",
      "390/390 [==============================] - 185s 475ms/step - loss: 0.1277 - accuracy: 0.9585 - val_loss: 0.3532 - val_accuracy: 0.8990\n",
      "Epoch 19/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1281 - accuracy: 0.9590\n",
      "Epoch 00019: val_accuracy did not improve from 0.90300\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.1281 - accuracy: 0.9590 - val_loss: 0.3538 - val_accuracy: 0.8999\n",
      "Epoch 20/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1258 - accuracy: 0.9588\n",
      "Epoch 00020: val_accuracy did not improve from 0.90300\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.1258 - accuracy: 0.9588 - val_loss: 0.3436 - val_accuracy: 0.9009\n",
      "Epoch 21/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1265 - accuracy: 0.9598\n",
      "Epoch 00021: val_accuracy did not improve from 0.90300\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.1265 - accuracy: 0.9598 - val_loss: 0.3548 - val_accuracy: 0.8988\n",
      "Epoch 22/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1251 - accuracy: 0.9602\n",
      "Epoch 00022: val_accuracy improved from 0.90300 to 0.90420, saving model to /content/drive/MyDrive/27 CNN on CIFR/Model Output/model_depthwise_best.h5\n",
      "390/390 [==============================] - 186s 477ms/step - loss: 0.1251 - accuracy: 0.9602 - val_loss: 0.3327 - val_accuracy: 0.9042\n",
      "Epoch 23/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1233 - accuracy: 0.9611\n",
      "Epoch 00023: val_accuracy did not improve from 0.90420\n",
      "390/390 [==============================] - 185s 475ms/step - loss: 0.1233 - accuracy: 0.9611 - val_loss: 0.3501 - val_accuracy: 0.9000\n",
      "Epoch 24/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1201 - accuracy: 0.9623\n",
      "Epoch 00024: val_accuracy did not improve from 0.90420\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.1201 - accuracy: 0.9623 - val_loss: 0.3341 - val_accuracy: 0.9037\n",
      "Epoch 25/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1201 - accuracy: 0.9617\n",
      "Epoch 00025: val_accuracy did not improve from 0.90420\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.1201 - accuracy: 0.9617 - val_loss: 0.3472 - val_accuracy: 0.9018\n",
      "Epoch 26/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1242 - accuracy: 0.9599\n",
      "Epoch 00026: val_accuracy did not improve from 0.90420\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.1242 - accuracy: 0.9599 - val_loss: 0.3363 - val_accuracy: 0.9031\n",
      "Epoch 27/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1213 - accuracy: 0.9617\n",
      "Epoch 00027: val_accuracy did not improve from 0.90420\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.1213 - accuracy: 0.9617 - val_loss: 0.3518 - val_accuracy: 0.9006\n",
      "Epoch 28/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1210 - accuracy: 0.9608\n",
      "Epoch 00028: val_accuracy did not improve from 0.90420\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.1210 - accuracy: 0.9608 - val_loss: 0.3456 - val_accuracy: 0.9014\n",
      "Epoch 29/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1210 - accuracy: 0.9610\n",
      "Epoch 00029: val_accuracy did not improve from 0.90420\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.1210 - accuracy: 0.9610 - val_loss: 0.3410 - val_accuracy: 0.9024\n",
      "Epoch 30/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1209 - accuracy: 0.9615\n",
      "Epoch 00030: val_accuracy did not improve from 0.90420\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.1209 - accuracy: 0.9615 - val_loss: 0.3570 - val_accuracy: 0.9001\n",
      "Epoch 31/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1176 - accuracy: 0.9616\n",
      "Epoch 00031: val_accuracy did not improve from 0.90420\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.1176 - accuracy: 0.9616 - val_loss: 0.3540 - val_accuracy: 0.9015\n",
      "Epoch 32/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1189 - accuracy: 0.9613\n",
      "Epoch 00032: val_accuracy did not improve from 0.90420\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.1189 - accuracy: 0.9613 - val_loss: 0.3469 - val_accuracy: 0.9010\n",
      "Epoch 33/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1173 - accuracy: 0.9617\n",
      "Epoch 00033: val_accuracy did not improve from 0.90420\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.1173 - accuracy: 0.9617 - val_loss: 0.3454 - val_accuracy: 0.9022\n",
      "Epoch 34/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1214 - accuracy: 0.9597\n",
      "Epoch 00034: val_accuracy did not improve from 0.90420\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.1214 - accuracy: 0.9597 - val_loss: 0.3557 - val_accuracy: 0.8989\n",
      "Epoch 35/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1183 - accuracy: 0.9619\n",
      "Epoch 00035: val_accuracy did not improve from 0.90420\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.1183 - accuracy: 0.9619 - val_loss: 0.3544 - val_accuracy: 0.9000\n",
      "Epoch 36/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1174 - accuracy: 0.9619\n",
      "Epoch 00036: val_accuracy did not improve from 0.90420\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.1174 - accuracy: 0.9619 - val_loss: 0.3545 - val_accuracy: 0.9004\n",
      "Epoch 37/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1193 - accuracy: 0.9614\n",
      "Epoch 00037: val_accuracy did not improve from 0.90420\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.1193 - accuracy: 0.9614 - val_loss: 0.3419 - val_accuracy: 0.9007\n",
      "Epoch 38/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1152 - accuracy: 0.9609\n",
      "Epoch 00038: val_accuracy did not improve from 0.90420\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.1152 - accuracy: 0.9609 - val_loss: 0.3489 - val_accuracy: 0.9021\n",
      "Epoch 39/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1196 - accuracy: 0.9612\n",
      "Epoch 00039: val_accuracy did not improve from 0.90420\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.1196 - accuracy: 0.9612 - val_loss: 0.3407 - val_accuracy: 0.9003\n",
      "Epoch 40/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1159 - accuracy: 0.9634\n",
      "Epoch 00040: val_accuracy did not improve from 0.90420\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.1159 - accuracy: 0.9634 - val_loss: 0.3635 - val_accuracy: 0.8987\n",
      "Epoch 41/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1166 - accuracy: 0.9629\n",
      "Epoch 00041: val_accuracy did not improve from 0.90420\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.1166 - accuracy: 0.9629 - val_loss: 0.3792 - val_accuracy: 0.8963\n",
      "Epoch 42/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1152 - accuracy: 0.9633\n",
      "Epoch 00042: val_accuracy did not improve from 0.90420\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.1152 - accuracy: 0.9633 - val_loss: 0.3533 - val_accuracy: 0.9005\n",
      "Epoch 43/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1149 - accuracy: 0.9628\n",
      "Epoch 00043: val_accuracy did not improve from 0.90420\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.1149 - accuracy: 0.9628 - val_loss: 0.3467 - val_accuracy: 0.9014\n",
      "Epoch 44/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1144 - accuracy: 0.9634\n",
      "Epoch 00044: val_accuracy did not improve from 0.90420\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.1144 - accuracy: 0.9634 - val_loss: 0.3564 - val_accuracy: 0.8985\n",
      "Epoch 45/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1131 - accuracy: 0.9641\n",
      "Epoch 00045: val_accuracy did not improve from 0.90420\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.1131 - accuracy: 0.9641 - val_loss: 0.3602 - val_accuracy: 0.9000\n",
      "Epoch 46/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1173 - accuracy: 0.9613\n",
      "Epoch 00046: val_accuracy did not improve from 0.90420\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.1173 - accuracy: 0.9613 - val_loss: 0.3511 - val_accuracy: 0.9023\n",
      "Epoch 47/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1142 - accuracy: 0.9634\n",
      "Epoch 00047: val_accuracy did not improve from 0.90420\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.1142 - accuracy: 0.9634 - val_loss: 0.3531 - val_accuracy: 0.8988\n",
      "Epoch 48/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1174 - accuracy: 0.9611\n",
      "Epoch 00048: val_accuracy did not improve from 0.90420\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.1174 - accuracy: 0.9611 - val_loss: 0.3611 - val_accuracy: 0.8980\n",
      "Epoch 49/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1145 - accuracy: 0.9634\n",
      "Epoch 00049: val_accuracy did not improve from 0.90420\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.1145 - accuracy: 0.9634 - val_loss: 0.3612 - val_accuracy: 0.8994\n",
      "Epoch 50/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1120 - accuracy: 0.9643\n",
      "Epoch 00050: val_accuracy did not improve from 0.90420\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.1120 - accuracy: 0.9643 - val_loss: 0.3650 - val_accuracy: 0.8998\n",
      "Epoch 51/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1140 - accuracy: 0.9641\n",
      "Epoch 00051: val_accuracy did not improve from 0.90420\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.1140 - accuracy: 0.9641 - val_loss: 0.3624 - val_accuracy: 0.8975\n",
      "Epoch 52/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1119 - accuracy: 0.9644\n",
      "Epoch 00052: val_accuracy did not improve from 0.90420\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.1119 - accuracy: 0.9644 - val_loss: 0.3586 - val_accuracy: 0.8998\n",
      "Epoch 53/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1118 - accuracy: 0.9637\n",
      "Epoch 00053: val_accuracy did not improve from 0.90420\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.1118 - accuracy: 0.9637 - val_loss: 0.3441 - val_accuracy: 0.9028\n",
      "Epoch 54/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1112 - accuracy: 0.9652\n",
      "Epoch 00054: val_accuracy did not improve from 0.90420\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.1112 - accuracy: 0.9652 - val_loss: 0.3623 - val_accuracy: 0.9005\n",
      "Epoch 55/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1106 - accuracy: 0.9643\n",
      "Epoch 00055: val_accuracy did not improve from 0.90420\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.1106 - accuracy: 0.9643 - val_loss: 0.3746 - val_accuracy: 0.8959\n",
      "Epoch 56/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1145 - accuracy: 0.9630\n",
      "Epoch 00056: val_accuracy did not improve from 0.90420\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.1145 - accuracy: 0.9630 - val_loss: 0.3562 - val_accuracy: 0.9014\n",
      "Epoch 57/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1134 - accuracy: 0.9637\n",
      "Epoch 00057: val_accuracy did not improve from 0.90420\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.1134 - accuracy: 0.9637 - val_loss: 0.3528 - val_accuracy: 0.9035\n",
      "Epoch 58/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1133 - accuracy: 0.9627\n",
      "Epoch 00058: val_accuracy did not improve from 0.90420\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.1133 - accuracy: 0.9627 - val_loss: 0.3682 - val_accuracy: 0.8988\n",
      "Epoch 59/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1117 - accuracy: 0.9637\n",
      "Epoch 00059: val_accuracy did not improve from 0.90420\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.1117 - accuracy: 0.9637 - val_loss: 0.3598 - val_accuracy: 0.9006\n",
      "Epoch 60/60\n",
      "390/390 [==============================] - ETA: 0s - loss: 0.1133 - accuracy: 0.9630\n",
      "Epoch 00060: val_accuracy did not improve from 0.90420\n",
      "390/390 [==============================] - 185s 474ms/step - loss: 0.1133 - accuracy: 0.9630 - val_loss: 0.3538 - val_accuracy: 0.9013\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.backend as k\n",
    "k.set_value(model.optimizer.lr, 0.001)\n",
    "\n",
    "callback_list = [checkpoint]\n",
    "model.load_weights('/content/drive/MyDrive/27 CNN on CIFR/Model Output/model_depthwise_120Epoch.h5')\n",
    "epochs = 60\n",
    "history = model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),\\\n",
    "                    steps_per_epoch=X_train.shape[0] // batch_size,epochs=epochs,\\\n",
    "                    verbose=1,validation_data=(X_test,y_test),callbacks=callback_list)\n",
    "\n",
    "model.save_weights('/content/drive/MyDrive/27 CNN on CIFR/Model Output/model_depthwise_180Epoch.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eil0p1K54KWT"
   },
   "source": [
    "**Loading Best Model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ILI-233y39ev",
    "outputId": "54a88b75-c7f9-4761-d9ab-9ceaaccc21f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 9s 30ms/step - loss: 0.3327 - accuracy: 0.9042\n",
      "Test loss: 0.33\n",
      "Test accuracy: 0.9\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Testing the model- Rev01 Model\n",
    "model.load_weights('/content/drive/MyDrive/27 CNN on CIFR/Model Output/model_depthwise_best.h5')\n",
    "score = model.evaluate(X_test, y_test, verbose=1)\n",
    "print('Test loss:', np.round(score[0],2))\n",
    "print('Test accuracy:', np.round(score[1],2))\n",
    "print(\"--\"*25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gm_EREtykRTO"
   },
   "source": [
    "# **Representation of Results:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aNfjNu6t4bXm",
    "outputId": "c1a9ced0-cb67-4793-9703-be01e3a0d496"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+------------------+--------------+---------------------------+--------------------------+\n",
      "|      Model Description      | Total Parameters | Hyperparams  | Best Model Accuracy-Train | Best Model Accuracy-Test |\n",
      "+-----------------------------+------------------+--------------+---------------------------+--------------------------+\n",
      "| Depthwise Seperable Conv2D  |     208,630      | Epoch   :300 |                           |                          |\n",
      "|         + Elastic Net       |                  |  l      :12  |           0.9387          |          0.8925          |\n",
      "|                             |                  |  filters:36  |                           |                          |\n",
      "|                             |                  |              |                           |                          |\n",
      "| Depthwise Seperable Conv2D  |     735,238      | Epoch   :144 |                           |                          |\n",
      "|         + Elastic Net       |                  |  l      :24  |           0.9602          |          0.9042          |\n",
      "|                             |                  |  filters:36  |                           |                          |\n",
      "|                             |                  |              |                           |                          |\n",
      "+-----------------------------+------------------+--------------+---------------------------+--------------------------+\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "results = PrettyTable()\n",
    "results.field_names = [\"Model Description\", \"Total Parameters\",\"Hyperparams\",\"Best Model Accuracy-Train\", \"Best Model Accuracy-Test\"]\n",
    "results.add_row([ \"Depthwise Seperable Conv2D \\n + Elastic Net\\n\" , \"208,630\", \"Epoch   :300\\n l      :12 \\nfilters:36\\n\",\"\\n 0.9387\", \"\\n 0.8925\" ])\n",
    "results.add_row([ \"Depthwise Seperable Conv2D \\n + Elastic Net\" , \"735,238\",\"Epoch   :144\\n l      :24 \\nfilters:36\\n\", \"\\n 0.9602\", \"\\n 0.9042\" ])\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9cJC6-rmQNkX"
   },
   "source": [
    "`End`"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "N0qCY60zPh65",
    "a1RnV1qvPorf"
   ],
   "machine_shape": "hm",
   "name": "DenseNet - cifar10- Solution.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
